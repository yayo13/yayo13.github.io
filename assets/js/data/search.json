[
  
  {
    "title": "【PLM】（7）XLNet",
    "url": "/posts/PLM-7-XLNet/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2024-06-11 00:00:00 +0000",
    





    
    "snippet": "XLNet是对以Bert为代表的自编码模型的改进如《【PLM】（3）自编码自回归 &amp; GPT-1、2、3模型》所述，无监督语言建模可以分为  以Bert为代表的自编码（AE）方式  以GPT为代表的自回归（AR）方式其中自编码方式拥有双向注意力的优势，但其Masked LM的预训练方式存在两个问题  预训练阶段使用[MASK]符号做掩码，但fine-tune阶段该掩码不存在，会导致一...",
    "content": "XLNet是对以Bert为代表的自编码模型的改进如《【PLM】（3）自编码自回归 &amp; GPT-1、2、3模型》所述，无监督语言建模可以分为  以Bert为代表的自编码（AE）方式  以GPT为代表的自回归（AR）方式其中自编码方式拥有双向注意力的优势，但其Masked LM的预训练方式存在两个问题  预训练阶段使用[MASK]符号做掩码，但fine-tune阶段该掩码不存在，会导致一定偏差  Bert每次会对句子的15%个token做掩码，每次训练仅考虑未被掩码的内容，会丢失被掩码内容间的相关性知识自回归方式由于采用生成移位方式进行训练，规避了自编码上述两个问题，但单向注意力的方式以及去噪自编码（DAE）的舍弃（MLM本质上有DAE的效果，对数据做了破坏）限制了其在NLU任务上的表现因此谷歌联合卡内基在2019年提出XLNet【1】，在自回归训练方式的基础上做自编码改进，在20个NLU任务上超越了Bert推荐阅读  XLNet 详解  官方代码XLNet解决方案XLNet的核心思想是：将训练序列乱序，让模型生成在Bert中被MASK的token以New York is a city为例，假设New和York分别是需要模型预测的内容1）传统Masked LM（Bert）在Bert中首先处理为[MASK] [MASK] is a city，然后让模型预测被掩盖的内容\\[J_{BERT}=\\log{p(New|is\\ a\\ city)}+\\log{p(York|is\\ a\\ city)}\\tag{1}\\]2）XLNetXLNet将原序列处理为is a city (New) (York)（一种处理方式，其中的is a city可以打乱），括号内的是希望模型生成的内容\\[J_{XLNet}=\\log{p(New|is\\ a\\ city)}+\\log{p(York|New,is\\ a\\ city)}\\tag{2}\\]也就是说XLNet将待生成的内容强制放在序列后面，与自回归的训练方式保持一致，同时下一个待生成的内容依赖上一个生成的内容，解决了Bert中掩码内容间相关性知识丢失的问题XLNet的想法很好，但如此也带来了2个问题与现有框架的适配问题 - 注意力掩码的修改为了与现有框架训练数据保持一致，XLNet并没有对输入序列直接做乱序（同样，输入的位置编码也保持不变），而是通过注意力掩码间接实现图1. 双流自注意力如图1右侧的Attention Masks所示，这里先只关注对角线为白色的那个，元素(i,j)如果为红色则表示原始序列中第$i$个token能看到第$j$个token假设输入序列长度为4，乱序后为3、2、4、1，那么显然原始序列中第3个token无法看到任何内容（第3行全为白色），原始序列中第1个token能看到其他3个（第1行后3个为红色）位置信息的丢失问题 - 双流自注意力还是以图1的3、2、4、1为例，如果沿用传统自回归的训练方式，显然会导致$p(4\\lvert 3,2)=p(4\\lvert 2,3)$的问题因为如式2所示的优化，只关注了待预测token能看到的上下文内容，但忽视了该token在原始输入中与上下文的位置关系为此，XLNet提出了双流自注意力机制，使用$h$（内容流）和$g$（查询流）两组隐含变量确保信息不丢失  内容流$h_{z_t}$：与标准的transformer隐藏变量类似，是$x_{z_t}$本身及其可视上下文的内容编码表示。注意网络输入向量由word embedding+position embedding组成，因此$h_i^{(0)}$由$x_i$的word embedding初始化  查询流$g_{z_t}$：包含$x_{z_t}$可视上下文和$x_{z_t}$的原始位置编码。使用$w$做随机初始化图2. 双流自注意力从m-1层到m层的更新公式图1(a)(b)是图2公式的示例，$g_{z_t}^{(m)}$的更新以$g_{z_t}^{(m-1)}$为query，而$h_{z_t}^{(m)}$的更新以$h_{z_t}^{(m-1)}$为query，其中$m$表示其所在网络的层数如式3所示，最终使用顶层的查询流预测下一个token\\[p_{\\theta}(X_{z_t}=x|x_{z_{&lt;t}})=\\frac{exp(e(x)^Tg_{\\theta}(x_{z_{&lt;t}},z_t))}{\\sum_{x'}exp(e(x')^Tg_{\\theta}(x_{z_{&lt;t}},z_t))}\\tag{4}\\]在finetuning阶段，可以丢弃（drop，我的理解应该是冻结）查询流$g_{z_t}$，回归通用的transformer流程其他优化XLNet另外做了2项优化  借鉴了Transformer-XL，引入了其中的“记忆机制”和“相对位置编码”，改善模型的长文本能力  对多语句任务（如NSP），不再像Bert一样使用绝对位置编码（即第1条语句的位置编码为0，第2条为1），而是使用可学习的$s_+$和$s_-$，前者表示两条语句属于同一片段，后者表示不属于同一片段图3. 消融实验图3中的$K$为一条语句中用于预测的token量（$\\frac{1}{K}$对应Bert中被MASK的比例），文章采用的值为[6,7]，与Bert采用的15%相近此外文章发现NSP（next sentence predict，判断两条语句是否属于同一片段）任务对大一点的模型没什么正向作用（图3第5行）小结XLNet是对以Bert为代表的自编码模型的改进，通过后置待预测的token实现“生成式MASK”其改进的前提是：自编码结构的双向机制比自回归方式更优但后续GPT的发展却让这个前提变得不那么可信，事实上只要模型足够大，训练数据足够多，自回归结构的模型也可以高质量完成NLU因此本文更多的是记录XLNet所做的尝试，时至今日复现应用的意义不大文献【1】Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in neural information processing systems, 2019, 32."
  },
  
  {
    "title": "【长文本】（2）Ernie-Doc",
    "url": "/posts/%E9%95%BF%E6%96%87%E6%9C%AC-2-Ernie-Doc/",
    "categories": "LLM, 长文本",
    "tags": "长文本",
    "date": "2024-05-30 00:00:00 +0000",
    





    
    "snippet": "Ernie-Doc【1】是百度2020年提出的面向篇章级长文本建模的预训练-微调框架Ernie-Doc基于Transformer-XL和XLNet【2】做改进，主要有以下3点创新  回顾式建模机制（Retrospective Feed Mechanism）  增强记忆机制（Enhanced Recurrence Mechanism）  段落重排目标训练（Segment-reordering ...",
    "content": "Ernie-Doc【1】是百度2020年提出的面向篇章级长文本建模的预训练-微调框架Ernie-Doc基于Transformer-XL和XLNet【2】做改进，主要有以下3点创新  回顾式建模机制（Retrospective Feed Mechanism）  增强记忆机制（Enhanced Recurrence Mechanism）  段落重排目标训练（Segment-reordering Objective）Ernie-Doc在业界首次实现了全篇章无限长文本的双向建模，在阅读理解、信息抽取等13个权威中英文长文本语言理解任务上取得SOTA图1. 左：WikiText-103数据集（长文本语言建模）；右：IMDB&amp;HYP数据集（长文本分类）长文本模型技术现状图2. 长文本技术现状如图2所示，目前长文本模型可分为3个技术方向  稀疏注意力（sparse attention transformer）  记忆机制（recurrence transformer）  层次机制（hierarchical transformer）其中Ernie-Doc与Transformer-XL和XLNet一样，同属于记忆机制的技术方向Ernie-Doc核心改进Ernie-Doc认为Transformer-XL仅考虑了上一片段的信息（式3），不能真正做到全文建模但实际上后者在论文中其实已有考虑图3. Transformer-XL对cache片段长度的解释在实践中，Transformer-XL考虑前M个片段（M与片段长度一致）图4. 右上：Transformer-XL；下：Ernie-Doc回顾式建模机制该机制模仿人类第一遍略读第二遍精读的过程，首先将全文通读一遍，然后结合全文通读信息对每个片段做精读\\[\\begin{aligned}\\hat{H}&amp;=[\\hat{H}_{1:T}^1\\circ\\hat{H}_{1:T}^2\\cdots\\circ\\hat{H}_{1:T}^N],&amp;略读\\\\\\tilde{h}_{\\tau+1}^{n-1}&amp;=[SG(\\hat{H}\\circ h_{\\tau}^{n-1})\\circ h_{\\tau+1}^{n-1}],&amp;精读\\end{aligned}\\tag{1}\\]其中  $\\hat{H}\\in R^{(L\\times T\\times N)\\times d}$为略读阶段全文所有层的隐藏状态矩阵，其中$T$是切分后的片段数，$L$是每个片段的长度，$N$是网络的总层数，$d$是隐藏状态向量的维度  $\\hat{H}_{1:T}^i=[\\hat{h_1^i}\\circ \\hat{h_2^i}\\cdots\\circ \\hat{h_T^i}]\\in R^{(L*T)\\times d}$是第$i$层沿所有片段拼接的隐藏状态矩阵  $\\tilde{h}_{\\tau+1}^{n-1}$是扩展后的上下文（extended context）按式1的方式很显然需要消耗大量内存和计算量，因此文章对略读阶段做了式2的效率改进\\[\\hat{H}_r=\\left[\\hat{h}_N^N\\circ\\hat{h}_{2*N}^N\\cdots\\circ\\hat{h}_{\\lfloor T/N\\rfloor}^N\\right]\\tag{2}\\]如式2所示，略读阶段对每个片段只保留最后一个token的最后一层的隐藏状态，因为该状态已经包含了整个片段的信息增强记忆机制回顾式建模引入了通读和精读的概念，在精读阶段除了附加通读的内容$\\hat{H}$，剩下的和Transformer-XL（式3）一致，都是取上一片段同层的信息做拼接\\[\\tilde{h}_{\\tau+1}^{n-1}=[SG(h_{\\tau}^{n-1})\\circ h_{\\tau+1}^{n-1}]\\tag{3}\\]Ernie-Doc希望其中的$h_{\\tau}^{n-1}$已经包含了全部上下文信息，因此将式3修改为\\[\\tilde{h}_{\\tau+1}^{n-1}=[SG(h_{\\tau}^n)\\circ h_{\\tau+1}^{n-1}]\\tag{4}\\]即取上一片段上一层的信息做拼接，并将该机制应用在通读和精读（比较图1右上和下的信息传递方向）对于式4，个人的理解是这样的：$SG(h_{\\tau}^{n-1})$应该包含了通读的部分，即精读部分的$\\tau$段其实应该对应$SG(h_{T+\\tau}^{n-1})$段落重排训练前面的方法是从模型结构上确保篇章信息获取的可能性，段落重排是从训练任务上让模型建立段落间关系的理解能力对长文本$D$  依次切分为1、2，…，m个chunk  将所有chunk随机打乱顺序  将打乱顺序的所有chunk切分为$T$个片段（每个片段最长$L$）\\[\\begin{aligned}&amp;D=\\{C_1,C_2,C_3\\}\\\\\\Rightarrow&amp;\\hat{D}=\\{C_2,C_3,C_1\\}\\\\&amp;=\\{S_1,S_2,\\cdots,S_T\\}\\end{aligned}\\tag{5}\\]式5是将$D$切分为3个chunk后打乱顺序再切分$T$个片段的示意这样重排任务就变成了一个$K$分类问题\\[K=\\sum_{i=1}^mi!\\tag{6}\\]对比式5和式6，任务目标是对m个chunk重排，而不是$T$个片段重排后者理论上当然也可以，但类别数$K=T!$太大了，实际操作会有难度MLM预训练目标函数文章介绍了MLM预训练目标函数，在此之前应该会先做生成式的预训练（交叉熵），估计是因为没啥创新就没写了\\[\\max_{\\theta}\\log{p_{\\theta}(S_{\\tau}|\\hat{S_{\\tau}}))}+\\mathbb{I}_{\\tau=T}\\log{p_{\\theta}(D|\\hat{D})}\\tag{7}\\]  式7的前半部分是常规的Mask LM训练，$S_{\\tau}$对应式5中的片段，将其随机置[MASK]就是$\\hat{S_{\\tau}}$，需要模型推测出被mask的token  式7的后半部分是段落重排训练，$D$对应式5中的长文本，将其切分并乱序就是$\\hat{D}$，$\\mathbb{I}_{\\tau=T}$表示仅在第$T$片段做优化。这也很好理解，需要将所有片段输入才能执行重排图5. 消融实验（w/o表示without）文献【1】Ding S, Shang J, Wang S, et al. ERNIE-Doc: A retrospective long-document modeling transformer[J]. arXiv preprint arXiv:2012.15688, 2020.【2】Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in neural information processing systems, 2019, 32."
  },
  
  {
    "title": "【长文本】（1）Transformer-XL",
    "url": "/posts/%E9%95%BF%E6%96%87%E6%9C%AC-1-Transformer-XL/",
    "categories": "LLM, 长文本",
    "tags": "长文本",
    "date": "2024-05-22 00:00:00 +0000",
    





    
    "snippet": "推荐阅读：  Transformer-XL官方源代码  《【核心代码解读】Transformer-XL》  《Transformer-XL解读（论文 + PyTorch源码）》回顾注意力机制的核心计算公式1，可以看到，虽然理论上transformer结构可以接受无限长度的输入，但由此带来的计算量是无法接受的\\[Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqr...",
    "content": "推荐阅读：  Transformer-XL官方源代码  《【核心代码解读】Transformer-XL》  《Transformer-XL解读（论文 + PyTorch源码）》回顾注意力机制的核心计算公式1，可以看到，虽然理论上transformer结构可以接受无限长度的输入，但由此带来的计算量是无法接受的\\[Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\tag{1}\\]因此包括bert在内的模型会限定输入长度（512）Vanilla Transformers【1】将长文本输入截断为多个片段，每个片段分别做训练，推理阶段则采用“滑动窗口”的思路，每次往右移动一个token确保不超过最大长度。但这样会带来几个问题  上下文长度受限：无法编码长文本  上下文碎片：片段间信息无法共享，影响模型性能  推理速度慢：每预测一个词，都需要重新构建一次上下文因此MCU联合Google在2019年的Transformer-XL【2】中提出一个朴素的思路：空间换计算量，记录上一个片段的每层状态，用于下一个片段的计算基于上述思路，Transformer-XL对现有的transformer提出2点改进  记忆机制  相对位置编码图1. 左：困惑度PPL降低；右：推理速度提升1800+倍从图1可看到，与方法Vanilla Transformers相比，Transformer-XL的模型精度和推理速度均取得很大提升记忆机制令相邻的两个片段分别为$s_{\\tau}=[x_{\\tau,1}, …, x_{\\tau,L}]$和$s_{\\tau+1}=[x_{\\tau+1,1}, …, x_{\\tau+1,L}]$，片段长度均为$L$令输入片段$s_{\\tau}$在第$n$层隐含层的向量为$h_{\\tau}^n\\in R^{L\\times d}$，其中$d$为向量维度那么对于输入片段$s_{\\tau+1}$在第$n$层的隐含层向量$h_{\\tau+1}^n$，计算如下\\[\\begin{aligned}\\tilde{h}_{\\tau+1}^{n-1}&amp;=[SG(h_{\\tau}^{n-1})\\circ h_{\\tau+1}^{n-1}]\\\\q_{\\tau+1}^n, k_{\\tau+1}^n,v_{\\tau+1}^n&amp;=h_{\\tau+1}^{n-1}W_q^T,\\tilde{h}_{\\tau+1}^{n-1}W_k^T,\\tilde{h}_{\\tau+1}^{n-1}W_v^T\\\\h_{\\tau+1}^n&amp;=TransformerLayer(q_{\\tau+1}^n,k_{\\tau+1}^n,v_{\\tau+1}^n)\\end{aligned}\\tag{2}\\]其中$SG()$表示去掉梯度，$[A\\circ B]$表示按长度方向拼接两个向量因此$\\tilde{h}_{\\tau+1}^{n-1}\\in R^{2L\\times d}$是由第$\\tau$片段在$n-1$层的向量去掉梯度信息后与第$\\tau+1$片段在$n-1$层的向量组合而成注意$q_{\\tau+1}^n, k_{\\tau+1}^n,v_{\\tau+1}^n$，其中$q_{\\tau+1}^n$（即第$\\tau+1$片段在$n$层的query）仅考虑当前片段，而key和value考虑了$\\tau$片段图2. 记忆机制需要注意的是，这里仅考虑对前一个片段做记忆（cache），输入长度扩展为$2L$，实际中可以根据显存对多个前置片段做记忆，扩展为$nL$相对位置编码传统的transformer采用绝对位置编码：$\\tau$片段的第$k$个token与$\\tau+1$片段的第$k$个token的位置编码是完全相同的这对于采用记忆机制的Transformer-XL来说显然不合适，因此提出了相对位置编码的方法首先考虑传统tranformer中绝对位置编码的attention score的计算\\[\\begin{aligned}A_{i,j}^{abs}&amp;=Q^TK\\\\&amp;=(E_{x_i}+U_i)^TW_q^TW_k(E_{x_j}+U_j)\\\\&amp;=\\underbrace{E_{x_i}^TW_q^TW_kE_{x_j}}_{a}+\\underbrace{E_{x_i}^TW_q^TW_kU_j}_{b}+\\underbrace{U_i^TW_q^TW_kE_{x_j}}_{c}+\\underbrace{U_i^TW_q^TW_kU_j}_{d}\\end{aligned}\\tag{3}\\]式中  $W_q$、$W_k$分别为query和key的权重矩阵  $E_{x_i}$为第$i$个token的编码向量  $U_i$为第$i$个token的绝对位置编码向量Transformer-XL将式3修改为如式4所示的相对位置编码方式\\[A_{i,j}^{rel}=\\underbrace{E_{x_i}^TW_q^TW_{k,E}E_{x_j}}_{a}+\\underbrace{E_{x_i}^TW_q^TW_{k,R}R_{i-j}}_{b}+\\underbrace{u^TW_{k,E}E_{x_j}}_{c}+\\underbrace{v^TW_{k,R}R_{i-j}}_{d}\\tag{4}\\]比较式3和式4，有以下3个方面的差别1）替换key的位置编码式3中（b，d部分）与key相关的绝对位置编码$U_j$被替换为相对位置编码$R_{i-j}$其中$R$采用正弦位置编码，该编码方式仍为绝对位置编码，但$R_{i-j}$是以query和key的位置差为自变量，仅考虑两者的相互位置关系2）删除query的位置编码考虑式1，QK计算中每条query向量会与所有位置的key向量做乘积，因此这里每条query的位置是无关的因此方法将c部分的$U_i^TW_q^T$替换为1个可学习参数$u$，将d部分的$U_i^TW_q^T$替换为1个可学习参数$v$3）对key权重做进一步区分原始方法中key的权重用$W_k$表示，方法将其区分为与内容相关的$W_{k,E}$和与位置相关的$W_{k,R}$如此一来，式4中的a、b、c、d4个部分就有了明确的含义  a部分衡量query与key内容相关的信息  b部分捕捉基于当前query内容的位置偏差  c部分为全局内容偏差，衡量key内容的重要性  d部分为全局位置偏差，衡量key相对位置的重要性最后，结合式2和式4，文章总结了基于Transformer-XL方法，一个前向transformer层的计算公式（以单头为例）\\[\\begin{aligned}\\tilde{h}_{\\tau}^{n-1}&amp;=[SG(m_{\\tau}^{n-1})\\circ h_{\\tau}^{n-1}]\\\\q_{\\tau}^n, k_{\\tau}^n,v_{\\tau}^n&amp;=h_{\\tau}^{n-1}{W_q^n}^T,\\tilde{h}_{\\tau}^{n-1}{W_{k,E}^n}^T,\\tilde{h}_{\\tau}^{n-1}{W_v^n}^T\\\\A_{\\tau,i,j}^n&amp;={q_{\\tau,i}^n}^Tk_{\\tau,j}^n+{q_{\\tau,i}^n}^TW_{k,R}^nR_{i-j}+u^Tk_{\\tau,j}+v^TW_{k,R}^nR_{i-j}\\\\a_{\\tau}^n&amp;=MaskedSoftmax(A_{\\tau}^n)v_{\\tau}^n\\\\o_{\\tau}^n&amp;=LayerNorm(Linear(a_{\\tau}^n)+h_{\\tau}^{n-1})\\\\h_{\\tau}^n&amp;=PositionwiseFeedForward(o_{\\tau}^n)\\end{aligned}\\tag{5}\\]效率优化方法对式4中的$W_{k,R}R_{i-j}$做了加速，计算复杂度从$O(n^2)$降为$O(n)$令$M$为片段$\\tau-1$的长度，$L$为当前段$\\tau$的长度，那么$i-j$的可能取值为[0, M+L-1]，即\\[\\begin{aligned}Q&amp;=\\left[\\begin{matrix}R_{M+L-1}^T\\\\R_{M+L-2}^T\\\\\\vdots\\\\R_1\\\\R_0^T\\end{matrix}\\right]W_{k,R}^T\\\\&amp;=\\left[\\begin{matrix}[W_{k,R}R_{M+L-1}]^T\\\\ [W_{k,R}R_{M+L-2}]^T\\\\\\vdots\\\\ [W_{k,R}R_1]^T\\\\ [W_{k,R}R_0]^T\\end{matrix}\\right]\\in R^{(M+L)\\times d}\\end{aligned}\\tag{6}\\]用$B$表示式4中的b部分（$E_{x_i}^TW_q^TW_{k,R}R_{i-j}$）对所有$i-j$的取值矩阵\\[\\begin{aligned}B&amp;=\\left[\\begin{matrix}q_0^TW_{k,R}R_M&amp;\\cdots&amp;q_0^TW_{k,R}R_0&amp;0&amp;\\cdots&amp;0\\\\q_1^TW_{k,R}R_{M+1}&amp;\\cdots&amp;q_1^TW_{k,R}R_1&amp;q_1^TW_{k,R}R_0&amp;\\cdots&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\q_{L-1}^TW_{k,R}R_{M+L-1}&amp;\\cdots&amp;q_{L-1}^TW_{k,R}R_{M+L-1}&amp;q_{L-1}^TW_{k,R}R_{L-1}&amp;\\cdots&amp;q_{L-1}W_{k,R}R_0\\end{matrix}\\right]\\\\&amp;=\\left[\\begin{matrix}q_0^TQ_{L-1}&amp;\\cdots&amp;q_0^TQ_{M+L-1}&amp;0&amp;\\cdots&amp;0\\\\q_1^TQ_{L-2}&amp;\\cdots&amp;q_1^TQ_{M+L-2}&amp;q_1^TQ_{M+L-1}&amp;\\cdots&amp;0\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\q_{L-1}^TQ_0&amp;\\cdots&amp;q_{L-1}^TQ_M&amp;q_{L-1}^TQ_{M+1}&amp;\\cdots&amp;q_{L-1}^TQ_{M+L-1}\\end{matrix}\\right]\\end{aligned}\\tag{7}\\]注意式7的下三角形式，生成式的模型query的每个token只能“看到”该位置之前的key因此对于$q_0$，只能看到$\\tau-1$段的$M$个key进一步的，定义\\[\\tilde{B}=qQ^T=\\left[\\begin{matrix}q_0^TQ_0&amp;\\cdots&amp;q_0^TQ_M&amp;q_0^TQ_{M+1}&amp;\\cdots&amp;q_0^TQ_{M+L-1}\\\\q_1^TQ_0&amp;\\cdots&amp;q_1^TQ_M&amp;q_1^TQ_{M+1}&amp;\\cdots&amp;q_1^TQ_{M+L-1}\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\q_{L-1}^TQ_0&amp;\\cdots&amp;q_{L-1}Q_M&amp;q_{L-1}Q_{M+1}&amp;\\cdots&amp;q_{L-1}^TQ_{M+L-1}\\end{matrix}\\right]\\tag{8}\\]对比式7和式8，可以看到$B$的第$i$行就是$\\tilde{B}$第$i$行左移$L-1-i$位，因此在计算中只需首先计算$\\tilde{B}$，然后按行左移即可文献【1】Al-Rfou R, Choe D, Constant N, et al. Character-level language modeling with deeper self-attention[C]//Proceedings of the AAAI conference on artificial intelligence. 2019, 33(01): 3159-3166.【2】Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019."
  },
  
  {
    "title": "【PLM】（6）ERNIE系列",
    "url": "/posts/PLM-6-ERNIE%E7%B3%BB%E5%88%97/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2024-05-15 00:00:00 +0000",
    





    
    "snippet": "Erne是百度公开的系列预模型，原本只是在预训练层面对Bert的改进，之后不断发展为一个满足各种需求的预训练模型系列图0. Ernie系列模型里程碑            时间      模型名称      特点                  2019.3      ERNIE1.0      改进预训练方法，超越bert（模型结构与bert一致）              2019.7...",
    "content": "Erne是百度公开的系列预模型，原本只是在预训练层面对Bert的改进，之后不断发展为一个满足各种需求的预训练模型系列图0. Ernie系列模型里程碑            时间      模型名称      特点                  2019.3      ERNIE1.0      改进预训练方法，超越bert（模型结构与bert一致）              2019.7      ERNIE2.0      提出多任务持续学习框架，进一步提升模型迁移学习能力（模型结构与bert一致）              2020.5      ERNIE-Gen      将ERNIE预训练技术扩展至文本生成领域              2020.9      ERNIE-ViL      面向视觉-语言知识增强的预训练框架，视觉常识推理榜单取得第一              2021.5      ERNIE-Gram      多粒度语言知识模型              2021.5      ERNIE-Doc      超长文本双向建模预训练模型              2021.5      ERNIE-UNIMO      语言与视觉一体的预训练模型              2021.12      ERNIE-M      多语言预训练模型              2022.5      ERNIE-3.0      同时支持自回归和自编码的模型（模型结构基于Transformer-XL)      官方基于PaddlePaddle的实现目前已被Transformers支持ERNIE_PRETRAINED_MODEL_ARCHIVE_LIST = [    \"nghuyong/ernie-1.0-base-zh\",    \"nghuyong/ernie-2.0-base-en\",    \"nghuyong/ernie-2.0-large-en\",    \"nghuyong/ernie-3.0-base-zh\",    \"nghuyong/ernie-3.0-medium-zh\",    \"nghuyong/ernie-3.0-mini-zh\",    \"nghuyong/ernie-3.0-micro-zh\",    \"nghuyong/ernie-3.0-nano-zh\",    \"nghuyong/ernie-gram-zh\",    \"nghuyong/ernie-health-zh\",    # See all ERNIE models at https://huggingface.co/models?filter=ernie]推荐阅读：  详解百度ERNIE进化史及典型应用场景  官方开源地址  第三方对模型做huggingface格式转换后的地址ERNIE1.0Bert的预训练使用了Masked LM和NSP两种子任务百度2019年发表的Ernie1.0【1】对Masked LM任务做了改进，在5项NLP任务中均领先了Bert图1. Ernie1.0在5项任务中均取得SOTA其中  XNLI：多语种（14种）的文本蕴含任务，包括“蕴含”、“冲突”、“无法推断”3种输出  LCQMC：判断给定的句子对是否意图一致  MSRA-NER：微软公开的序列标注任务，模型根据制定的实体类型做NER  ChnSentiCorp：情感分析任务，判断输入的情感是积极还是消极  npcc-dbqa：根据问题选择答案三阶段Masked LM预训练Bert的Masked LM以15%的概率掩盖句子中的词，在训练阶段让模型预测被掩盖的部分Ernie认为这种方式并没有考虑句子中的先验知识，因此提出3层次的Masked LM：  Basic-Level Masking：与Bert一样，随机掩盖句子中的词  Phrase-Level Masking：随机掩盖句子中的短语  Entity-Level Masking：随机掩盖句子中的实体图2. Ernie1.0的3层次MLM方案文章认为通过短语和实体的MLM，可以让模型隐性的学习到句子中蕴含的知识如图2所示，模型可以学习到J.K.Rowling是一个人名，其写了一本叫做HarryPotter的书图3. 三阶段MLM的消融实验如图3所示，论文使用预训练数据的10%做了消融实验，比较各模型在XNLI上的精度，可以看到使用了短语和实体的方式取得了更好的效果其他细节Ernie采用了多种来源的数据做预训练，文章认为这可以提高预训练模型的迁移学习能力            数据集      句子数量      数据特点                  中文维基      2.1千万      -              百度百科      5.1千万      百科全书式的文章              百度新闻      4.7千万      提供了电影名称、演员名称、足球队等实体知识              百度贴吧      5.4千万      论坛数据，可用于做对话任务训练      对话数据训练DLM的训练也是Masked LM的方式，比较特别的是  文章将多轮对话组织成一条语句，通过[SEP]区分  使用Dialogue Embedding替换了Bert中句子对的Segment Embedding，其中Q代表question，R代表response图4. DLM对话数据训练此外论文还通过随机抽掉question或response构建数据集，训练模型判断是否为多轮对话的能力图5. DLM对话数据训练的消融实验ERNIE2.0越来越多的证据（包括Ernie1.0）表明，预训练的任务越多模型的能力越强由此Ernie2.0【2】提出了一个持续学习框架，可以不断建立新的预训练任务，同时避免模型遗忘老任务的知识图6. ERNIE2.0在各项任务上均超过BERTErnie2.0多任务持续学习框架图7. 左：Ernie2.0多任务持续学习；中：多任务学习；右：持续学习如图7所示  多任务学习：在训练中使用多种学习任务，这些任务彼此没有先后顺序  持续学习：先后以不同的任务对模型做训练，确保其不会遗忘之前任务中学到的知识  Ernie2.0持续学习：每次训练新任务时，都会同时训练之前的任务，避免知识遗忘具体来说Ernie2.0针对持续学习的两个难点提出了解决方案：1）如何避免知识遗忘ERNIE在训练新任务时，首先使用之前训练好的模型做初始化，然后将新任务和所有老任务一起训练2）如何有效训练不同的任务在训练的不同任务阶段，为每种任务指派N次训练迭代图8. 3种学习方式的消融实验从图8的消融实验可以看到上述3种学习方式的差异：  多任务学习：没有区分训练阶段，所有的任务均训练50k步  持续学习：区分了训练阶段，每个阶段训练1种任务，每种任务训练50k步后结束该阶段  多任务持续学习（Ernie2.0）：区分了训练阶段，每个阶段均包含当前任务和所有历史任务，每种任务总的训练步数仍为50k其他细节输入的任务类型编码图9. Task embedding任务类型编码如图9，在预训练阶段对不同的任务做编号0-&gt;N，将其作为Task embedding与其他输入相加在针对具体应用的fine-tuning时，该部分指定任意值都行（相当于通过fine-tuning调整模型对任务类型的认知）预训练任务Ernie2.0采用了7种预训练任务，将其分为Token级别任务和Sentence级别任务1）Token级别任务  Knowledge Masking：Ernie1.0的任务，包含词、短语和实体的MLM  Capitalizattion Prediction：预测一个字是不是大小写，该任务对英文NER比较有用  Token-Document Relation：预测一个词是不是会多次出现在文章中，即判断其是不是高频词2）Sentence级别任务  Sentence Recordering：将文章拆诶$m$份，然后随机组合（共$k=m!$种），然后让模型选择正确的排序组合（$k$分类问题）  Sentence Distance：句子距离预测（0-句子相邻；1-句子不相邻；2-句子在不同文章中）  Discourse Relation：判断语义关系（0-提问与标题强相关；1-提问与标题弱相关；2-提问与标题不相关）  IR Relevance：信息检索关系任务，预测query和网页标题的关系（0-强相关；1-弱相关；2-不相关）预训练数据图10. 上：Ernie2.0预训练数据类型&amp;规模；下：各数据类型对应构造的与训练任务情况所有训练数据均为无监督或弱监督数据，其中弱监督数据采用更大的模型做标注得到（模型蒸馏）训练超参Ernie2.0一共训练了两个大小的模型  base模型：48张V100  large模型：64张V100使用Adam优化器，参数为$\\beta_1=0.9$，$\\beta_2=0.98$在英文模型上的学习率为$lr=5e-5$，中文模型为$lr=1.28e-4$（因为基于BERT的原因？）ERNIE3.0Ernie3.0【3】推出的时候GPT这种自回归的模型已经流行开来，增加模型大小成为主流研究方向Ernie3.0认为当时大多数大模型采用自回归方式直接对文本做训练，没有引入语法和世界知识，这会导致面向NLU的fine-tuning性能不佳因此文章以Transformer-XL【4】（提升长文本输入能力）为基础架构（Ernie1.0和Ernie2.0以Bert为基础架构），使用4TB的文本语料和1个知识图谱训练了10B参数量的模型具体来说，Ernie3.0将模型分为通用表示模块和任务相关表示模块，两个模块都是基于Transformer-XL构建，让底层的网络学习基础的共性的知识，高层的网络学习具体任务相关的知识总结Ernie1.0、Ernie2.0和Ernie3.0总体上看都是通过丰富预训练任务提升预训练模型的能力从实现层面上看，transformers较新的版本已经融合了Transformer-XL等论文的方法，因此从Ernie1,2,3、Ernie-gram和Ernie-health均与Bert保持一致，仅在embedding层加了task_type_id# add `task_type_id` for ERNIE modelif self.use_task_id:    if task_type_ids is None:        task_type_ids = torch.zeros(input_shape, dtype=torch.long, elf.position_ids.device)    task_type_embeddings = self.task_type_embeddings(task_type_ids)    embeddings += task_type_embeddings这也是huggingface中的示例直接使用BertTokenizer的原因from transformers import BertTokenizer, ErnieForMaskedLMtokenizer = BertTokenizer.from_pretrained(\"nghuyong/ernie-3.0-base-zh\")model = ErnieForMaskedLM.from_pretrained(\"nghuyong/ernie-3.0-base-zh\")文献【1】Sun Y, Wang S, Li Y, et al. Ernie: Enhanced representation through knowledge integration[J]. arXiv preprint arXiv:1904.09223, 2019.【2】Sun Y, Wang S, Li Y, et al. Ernie 2.0: A continual pre-training framework for language understanding[C]//Proceedings of the AAAI conference on artificial intelligence. 2020, 34(05): 8968-8975.【3】Sun Y, Wang S, Feng S, et al. Ernie 3.0: Large-scale knowledge enhanced pre-training for language understanding and generation[J]. arXiv preprint arXiv:2107.02137, 2021.【4】Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019."
  },
  
  {
    "title": "【RAG】检索优化 - FliCo基于模型蒸馏",
    "url": "/posts/RAG-%E6%A3%80%E7%B4%A2%E4%BC%98%E5%8C%96-FliCo%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/",
    "categories": "LLM, RAG",
    "tags": "RAG",
    "date": "2024-04-25 00:00:00 +0000",
    





    
    "snippet": "RAG在检索方面的优化一般有以下几种思路  提高检索效率：比如使用多向量检索器  提高检索质量：比如对知识库做优化、对检索的内容做进一步优化  改善用户query：将用户query改写为更适合检索的表达FliCo【1】通过训练一个模型$M_{ctx}$，对检索得到的上下文做蒸馏，从而提高检索质量图1. FliCo通过蒸馏上下文改善RAG输出如图1所示，FliCo认为检索的上下文可能从两个方面...",
    "content": "RAG在检索方面的优化一般有以下几种思路  提高检索效率：比如使用多向量检索器  提高检索质量：比如对知识库做优化、对检索的内容做进一步优化  改善用户query：将用户query改写为更适合检索的表达FliCo【1】通过训练一个模型$M_{ctx}$，对检索得到的上下文做蒸馏，从而提高检索质量图1. FliCo通过蒸馏上下文改善RAG输出如图1所示，FliCo认为检索的上下文可能从两个方面误导模型  检索到的上下文虽然是相关的，但表述过于复杂，容易误导（生成）模型  检索到的上下文是不相关的，导致（生成）模型产生错误认知FliCo在github上公开了工程代码问题描述给定知识库段落集合$P={p_i},i\\in K$，每个$p_i$包含$n_i$个spans（论文中一个span是一个句子），即$p_i=[t_i^1,…,t_i^{n_i}]$给定一对数据$e={q,o}$，其中$q$为用户query，$o$为理想的模型输出对于$q$，假设检索得到的上下文是$T={t_i^j}$，则理想情况模型输出的表示为$M_{gen}(o\\lvert q,T)$训练数据构建为了建立模型对输入上下文做“蒸馏”的能力，文章提出了3种方法构建训练数据图2. 3种方法构建蒸馏后数据文章将使用下述3种方法对上下文蒸馏得到$t_{silver}$，以其为期望输出，训练蒸馏模型$M_{ctx}(t_{silver}\\lvert q\\oplus P)$StrInc对排序后（一般根据相似度）的检索上下文${p_1,p_2,…}$，依次检查$p_i$是否包含理想输出$o$，只要找到1个就返回$p_i$这种方式简单直接，缺点是硬匹配可能失败，或者找到的仍然是噪声甚至错误内容（比如$o$为\"是对的\"而$p_i$为\"不是对的\"，$p_i$显然包含$o$，但含义相反）Lexical是对StrInc的优化，将检索到的每段上下文$p_m$按字符拆分，同样对理想输出$o$按字符拆分，计算两者字符的重叠比例，选取重叠率最高的作为输出$p_i$def calc_unigram_f1(text: str, answers: list[str], field: str = \"f1\") -&gt; float:    \"\"\"Calculate unigram f1 score between the text and reference answers.\"\"\"    norm_pred = normalize_text(text)    norm_answers = [normalize_text(ans) for ans in answers]    # 按字符切分，按位统计重叠数量    common_tokens = [        Counter(norm_pred) &amp; Counter(norm_ans) for norm_ans in norm_answers    ]    num_same = [sum(common.values()) for common in common_tokens]    score_list = []    for i, num in enumerate(num_same):        if num == 0:            score_list.append(0.0)        else:            p = 1.0 * num / len(norm_pred)            r = 1.0 * num / len(norm_answers[i])            f1 = 2 * p * r / (p + r)            if field == \"precision\":                score_list.append(p)            elif field == \"recall\":                score_list.append(r)            elif field == \"f1\":                score_list.append(f1)            else:                raise ValueError(f\"Unknown field: {field}\")    return max(score_list)CXMI条件交叉互信息（Conditional Cross-Mutual Information, CXMI）如式1所示\\[f_{cxmi}=\\frac{M_{gen}(o|t\\oplus q)}{M_{gen}(o|q)} \\tag{1}\\]$M_{gen}(o\\lvert t\\oplus q)$是给定query和上下文，模型输出$o$的概率$M_{gen}(o\\lvert q)$是给定query，模型输出$o$的概率显然如果  $f_{cxmi}=1$，则上下文$t$可有可无  $f_{cxmi}&gt;1$，则上下文$t$有利于模型生成  $f_{cxmi}&lt;1$，则上下文$t$带来了噪声，不利于模型生成这里最终选择$f_{cxmi}$最高的$p_i$作为输出def calc_cxmi_score(    model: AutoModelForSeq2SeqLM,    tokenizer: AutoTokenizer,    answer: str,    base_input: str,    ctx_input: str,    apply_sigmoid: bool = False,) -&gt; float:    \"\"\"Compute the CXMI score.\"\"\"    base_probs = get_output_probs(model, tokenizer, base_input, answer)    ctx_probs = get_output_probs(model, tokenizer, ctx_input, answer)    diff = sent_wise_diff(base_scores=base_probs, ctx_scores=ctx_probs)    if apply_sigmoid:        diff = sigmoid(diff)    return diff3种方法的选择文章基于6种数据集对上述3种方法做实验，认为3种方法各有适合的场景      数据集类型    数据集名称    评估指标    适用的方法        开放域QA    NQ【2】    EM    StrInc        TriviaQA(TQA)【3】    EM    StrInc        多跳QA    HotpotQA【4】    F1    CXMI        长文本QA    ELI5【5】    F1    CXMI        事实验证    FEVER【6】    Acc    CXMI        基于知识的对话生成    WoW【7】    F1    Lexical  图3. 使用T5和Llama2在6种数据集上的精度比较文章在”Limiitations”中也表示，上述3种方法也不一定具有普适性，还是要根据数据特点做选择和设计模型训练及对比如图2所示  文章基于提出的3种方法构建label数据$t_{silver}$  分别以Flan-T5和Llama2为基础模型训练蒸馏模型$M_{ctx}$  同样分别使用Flan-T5和Llama2作为生成模型$M_{gen}$图4. 消融实验  FULL是使用全部检索的上下文做生成  PSG是参考【8】提出的段落级筛选方法，对检索内容做过滤的生成  FilCo是使用文章方法训练的$M_{ctx}$模型蒸馏得到的内容做生成  Silver是使用文章提出的3种训练数据构建方法得到的内容做生成（证明这3种方法的合理性，在实际应用中没有理想输出，只能通过模型$M_{ctx}$做蒸馏）个人思考文章的方法在常规RAG中又引入了一个内容蒸馏模型，可以提高原始检索内容的质量（文章认为一是降低上下文长度，一是提高上下文的精确性），但有以下几个问题  很显然会降低系统效率  蒸馏模型的训练需要人工标注（理想的输出$o$）  蒸馏模型与输出模型是耦合的（考虑CXMI指标），这意味着不同的输出模型要训练一个配套的蒸馏模型  理论上输出模型足够强（推理能力、长文本能力），可以同时完成蒸馏模型的工作文献【1】Wang Z, Araki J, Jiang Z, et al. Learning to filter context for retrieval-augmented generation[J]. arXiv preprint arXiv:2311.08377, 2023.【2】Kwiatkowski T, Palomaki J, Redfield O, et al. Natural questions: a benchmark for question answering research[J]. Transactions of the Association for Computational Linguistics, 2019, 7: 453-466.【3】Joshi M, Choi E, Weld D S, et al. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension[J]. arXiv preprint arXiv:1705.03551, 2017.【4】Yang Z, Qi P, Zhang S, et al. HotpotQA: A dataset for diverse, explainable multi-hop question answering[J]. arXiv preprint arXiv:1809.09600, 2018.【5】Fan A, Jernite Y, Perez E, et al. ELI5: Long form question answering[J]. arXiv preprint arXiv:1907.09190, 2019.Fan A, Jernite Y, Perez E, et al. ELI5: Long form question answering[J]. arXiv preprint arXiv:1907.09190, 2019.【6】Thorne J, Vlachos A, Christodoulopoulos C, et al. FEVER: a large-scale dataset for fact extraction and VERification[J]. arXiv preprint arXiv:1803.05355, 2018.【7】Dinan E, Roller S, Shuster K, et al. Wizard of wikipedia: Knowledge-powered conversational agents[J]. arXiv preprint arXiv:1811.01241, 2018.【8】Asai A, Gardner M, Hajishirzi H. Evidentiality-guided generation for knowledge-intensive NLP tasks[J]. arXiv preprint arXiv:2112.08688, 2021."
  },
  
  {
    "title": "【信息抽取】UIE算法 - 实践篇",
    "url": "/posts/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-UIE%E7%AE%97%E6%B3%95-%E5%AE%9E%E8%B7%B5%E7%AF%87/",
    "categories": "LLM, 信息抽取",
    "tags": "信息抽取",
    "date": "2024-04-16 00:00:00 +0000",
    





    
    "snippet": "UIE的实现有3个版本可参考  随论文公开的版本  百度PaddlePaddle官方实现的版本  第三方将百度的版本用PyTorch实现的版本随论文公开的版本该版本基本遵照了论文的路线，代码质量还不错  基座模型使用的T5  实现了实体抽取、关系抽取和事件抽取3种功能下面是代码中使用的schema## record.schema# 实体类型列表type_list[\"人物\", \"自然地点\", ...",
    "content": "UIE的实现有3个版本可参考  随论文公开的版本  百度PaddlePaddle官方实现的版本  第三方将百度的版本用PyTorch实现的版本随论文公开的版本该版本基本遵照了论文的路线，代码质量还不错  基座模型使用的T5  实现了实体抽取、关系抽取和事件抽取3种功能下面是代码中使用的schema## record.schema# 实体类型列表type_list[\"人物\", \"自然地点\", \"地理区域\", \"组织机构\"]# 关系名称列表（代码中称为角色列表）role_list[]# 实体与关系字典{\"人物\": [], \"自然地点\": [], \"地理区域\": [], \"组织机构\": []}代码中仅使用了实体类型列表和关系列表，将其通过&lt;spot&gt;和&lt;asoc&gt;做标记，与正文text拼接在一起def schema_to_ssi(schema: RecordSchema):    ssi = \"&lt;spot&gt; \" + \"&lt;spot&gt; \".join(sorted(schema.type_list))    ssi += \"&lt;asoc&gt; \" + \"&lt;asoc&gt; \".join(sorted(schema.role_list))    ssi += \"&lt;extra_id_2&gt; \"    return ssiself._ssi = schema_to_ssi(self._schema)text = [self._ssi + x for x in text]第三方PyTorch实现的版本该版本实现迁移自百度PaddleNLP  实现了标注、微调、推理全过程  提供了paddlepaddle模型转PyTorch模型的工具  支持onnx推理与论文版本相比，PaddleNLP最大的不同有  基座模型不是encoder-decoder架构的T5，而是百度自研的Ernie  prompt不是SSI设计的由[spot]、[asso]指示的结构，而是自然语言描述的指令  模型本质上执行的不是一个生成任务，而是一个分类任务，即从输入语句中找到所需答案的起始与结束位置x = \"2022语言与智能技术竞赛由中国中文信息学会和中国计算机学会联合主办\"## 论文版本的prompt + 分隔符 + 文本\"&lt;spot&gt; 竞赛名称 \" + \"&lt;extra_id_2&gt; \" + x                  # 实体抽取\"&lt;spot&gt; 竞赛名称 &lt;asoc&gt; 主办方 \" + \"&lt;extra_id_2&gt; \" + x    # 关系抽取## paddlenlp版本的prompt + 分隔符 + 文本\"竞赛名称\" + \"&lt;eos&gt;\" + x                                  # 实体抽取\"2022语言与智能技术竞赛的主办方\" + \"&lt;eos&gt;\" + x             # 关系抽取推理图1. 推理流程建立schema tree接受的schema格式为列表（[]，实体抽取）、字典（{k:[],}，关系抽取）# schema示例schema = ['航母', '国家']schema = {'竞赛名称': ['主办方', '承办方', '已举办次数']}图2. schema tree示例如图2右所示，关系抽取对应其中非root的父节点，此时组织的prompt为[竞赛名称]的[主办方]# 使用\"的\"或\"of\"连接父子节点for k, v in input_map.items():    for idx in v:        for i in range(len(result_list[idx])):            if self._is_en:                prefix[k].append(\" of \" +                                 result_list[idx][i][\"text\"])            else:                prefix[k].append(result_list[idx][i][\"text\"] + \"的\")初始化推理器对于多语言的场景（对应uie-m-base, uie-m-large），使用BertTokenizerFast，其他场景使用ErnieMTokenizerFastif self._multilingual:    from tokenizer import ErnieMTokenizerFast    self._tokenizer = ErnieMTokenizerFast.from_pretrained(        self._task_path)else:    from transformers import BertTokenizerFast    self._tokenizer = BertTokenizerFast.from_pretrained(        self._task_path)但实际上两者用的基模型都是百度自家的Ernie而非T5执行推理推理的整体逻辑为  对文本输入依次考虑每个schema node得到结果（如图3所示，schema中有k个实体/关系，则需前向推理k次）  如果该节点有非root父节点，则改造为关系抽取  模型的输出就是输入语句中的起始结束位置  汇总所有结果并做格式校正，得到最终输出图3. 一条语句需推理k次对输入做encode的细节encoded_inputs = self._tokenizer(    text=short_texts_prompts,        # batch = (short_texts_prompts, put_texts)    text_pair=short_input_texts,    stride=2,                        # 当输入长度超过max_length做截断时，前后两个en数                                     # 如`abcdefg`, max_length=4, stride=2, 则d`、`cdef`、`efg`    truncation=True,    max_length=self._max_seq_len,    padding=padding_type,            # longest - 将当前batch内的序列pad到其                                     # 在这里1个batch为(prompt, input)    add_special_tokens=True,    return_offsets_mapping=True,     # 返回每个token和char位置的对应关系                                     # 如\"中文切分\"，假设tokenizer后为[\"[CLS]\", \", \"分\"]                                     # 则\"offset_mapping\"为`1x4x2`：[[[0,0],[0,,[3,4]]]                                     # special_token为[0,0]，其余表示f-char, end-of-char]    return_tensors=\"np\")UIE输出示例x = \"2022语言与智能技术竞赛由中国中文信息学会和中国计算机学会联合主办，百度公司、中国中文信息学会评测工作委员会和中国计算机学会自然语言处理专委会承办，已连续举办4届，成为全球最热门的中文NLP赛事之一。\"schema = {'竞赛名称': ['主办方', '承办方', '已举办次数']}output = [{\t\"竞赛名称\": [{\t\t\"text\": \"2022语言与智能技术竞赛\",\t\t\"start\": 0,\t\t\"end\": 13,\t\t\"probability\": 0.7825407,\t\t\"relations\": {\t\t\t\"主办方\": [{\t\t\t\t\"text\": \"中国中文信息学会\",\t\t\t\t\"start\": 14,\t\t\t\t\"end\": 22,\t\t\t\t\"probability\": 0.84217143\t\t\t}, {\t\t\t\t\"text\": \"中国计算机学会\",\t\t\t\t\"start\": 23,\t\t\t\t\"end\": 30,\t\t\t\t\"probability\": 0.75808024\t\t\t}],\t\t\t\"承办方\": [{\t\t\t\t\"text\": \"百度公司\",\t\t\t\t\"start\": 35,\t\t\t\t\"end\": 39,\t\t\t\t\"probability\": 0.8292705\t\t\t}, {\t\t\t\t\"text\": \"中国中文信息学会评测工作委员会\",\t\t\t\t\"start\": 40,\t\t\t\t\"end\": 55,\t\t\t\t\"probability\": 0.7000498\t\t\t}, {\t\t\t\t\"text\": \"中国计算机学会自然语言处理专委会\",\t\t\t\t\"start\": 56,\t\t\t\t\"end\": 72,\t\t\t\t\"probability\": 0.61934865\t\t\t}],\t\t\t\"已举办次数\": [{\t\t\t\t\"text\": \"4届\",\t\t\t\t\"start\": 80,\t\t\t\t\"end\": 82,\t\t\t\t\"probability\": 0.46712998\t\t\t}]\t\t}\t}]}]标注 &amp; 微调该版本代码使用doccanno标注，并提供了到训练数据的转换工具下面是标注结果导出内容的示例{    \"id\": 38,    \"text\": \"百科名片你知道我要什么，是歌手高明骏演唱的一首歌曲，1989年发行，收录于个人专辑《丛林男孩》中\",    \"relations\": [        {            \"id\": 20,            \"from_id\": 51,            \"to_id\": 53,            \"type\": \"歌手\"        },        {            \"id\": 21,            \"from_id\": 51,            \"to_id\": 55,            \"type\": \"发行时间\"        },        {            \"id\": 22,            \"from_id\": 51,            \"to_id\": 54,            \"type\": \"所属专辑\"        }    ],    \"entities\": [        {            \"id\": 51,            \"start_offset\": 4,            \"end_offset\": 11,            \"label\": \"作品名\"        },        {            \"id\": 53,            \"start_offset\": 15,            \"end_offset\": 18,            \"label\": \"人物名\"        },        {            \"id\": 54,            \"start_offset\": 42,            \"end_offset\": 46,            \"label\": \"作品名\"        },        {            \"id\": 55,            \"start_offset\": 26,            \"end_offset\": 31,            \"label\": \"时间\"        }    ]}由于模型是输出句子中起始与结束位置，因此微调时计算每个位置分类的交叉熵criterion = torch.nn.functional.binary_cross_entropyoutputs = model(input_ids=input_ids,                token_type_ids=token_type_ids,                attention_mask=att_mask)start_prob, end_prob = outputs[0], outputs[1]start_ids = start_ids.type(torch.float32)end_ids = end_ids.type(torch.float32)loss_start = criterion(start_prob, start_ids)loss_end = criterion(end_prob, end_ids)loss = (loss_start + loss_end) / 2.0这里有一个细节，代码中loss计算使用的是binary_cross_entropy，这是因为模型输出已经做了softmax关于binary_cross_entropy和cross_entroy的区别可参考《神经网络的改进》"
  },
  
  {
    "title": "【信息抽取】UIE算法 - 理论篇",
    "url": "/posts/%E4%BF%A1%E6%81%AF%E6%8A%BD%E5%8F%96-UIE%E7%AE%97%E6%B3%95-%E7%90%86%E8%AE%BA%E7%AF%87/",
    "categories": "LLM, 信息抽取",
    "tags": "信息抽取",
    "date": "2024-04-02 00:00:00 +0000",
    





    
    "snippet": "信息抽取概念信息抽取（Information Extraction, IE）旨在从非结构化的文本中提取用户指定的信息，并将其结构化通常来说IE可以分为如下几个任务：实体抽取、关系抽取、事件抽取、情感抽取实体抽取也称命名实体识别（Named Entity Recognition, NER），该任务是识别文本中指定类别的实体# 输入文本text = \"2月8日上午北京冬奥会自由式滑雪女子大跳台决...",
    "content": "信息抽取概念信息抽取（Information Extraction, IE）旨在从非结构化的文本中提取用户指定的信息，并将其结构化通常来说IE可以分为如下几个任务：实体抽取、关系抽取、事件抽取、情感抽取实体抽取也称命名实体识别（Named Entity Recognition, NER），该任务是识别文本中指定类别的实体# 输入文本text = \"2月8日上午北京冬奥会自由式滑雪女子大跳台决赛中中国选手谷爱凌以188.25分获得金牌！\"# 指定的实体类别type_ner = \"提取其中类型为'时间','选手','赛事名称'的实体\"# NER期望的输出{    \"时间\": \"2月8日上午\",    \"选手\": \"谷爱凌\"，    \"赛事名称\": \"北京冬奥会自由式滑雪女子大跳台决赛\"}关系抽取关系抽取（Relation Extraction, RE），该任务从文本中识别实体，同时抽取实体间的语义关系，最终获得三元组信息，即（主体，谓语，客体）# 输入文本text = \"2022语言与智能技术竞赛由中国中文信息学会和中国计算机学会联合主办，百度公司、中国中文信息学会评测工作委员会和中国计算机学会自然语言处理专委会承办，已连续举办4届，成为全球最热门的中文NLP赛事之一。\"# 指定的抽取类别type_re = \"以'竞赛名称'为主体，提取'主办方','承办方','已举办的次数'\"# RE期望的输出{    \"竞赛名称\": \"2022语言与智能技术竞赛\",    \"主办方\": [\"中国中文信息学会\", \"中国计算机学会\"],    \"承办方\": [\"百度公司\", \"中国计算机学会自然语言处理专委会\", \"中国中文信息学会评测工作委员会\"],    \"已举办的次数\": \"4届\"}事件抽取事件抽取（Event Extraction, EE），该任务从文本中抽取指定的事件触发词(Trigger)和事件论元(Argument)，组合为相应的事件结构化信息# 输入文本text = \"中国地震台网正式测定：5月16日06时08分在云南临沧市凤庆县(北纬24.34度，东经99.98度)发生3.5级地震，震源深度10千米。\"# 指定的抽取事件type_ee = \"以'地震'为触发词，提取'时间','震中位置','震源深度'\"# EE期望的输出{    \"地震触发词\": \"地震\",    \"地震强度\": \"3.5级\",    \"震中位置\": \"云南临沧市凤庆县(北纬24.34度，东经99.98度)\",    \"震源深度\": \"10千米\"}情感抽取情感抽取从文本中抽取指定对象的情感倾向# 输入文本text = \"店面干净，很清静，服务员服务热情，性价比很高，发现收银台有排队\"# 指定的抽取对象type_se = \"抽取其中包含的'评价维度'，并给出各个维度的情感倾向\"# SE期望的输出{    \"评价维度\": [\"性价比\", \"店面\"],    \"性价比\": {        \"情感倾向\": \"正向\"        \"观点词\": \"高\"    },    \"店面\": {        \"情感倾向\": \"正向\"        \"观点词\": \"干净\"    }}UIE算法通用信息抽取统一框架（Unified Structure Generation for Universal Information Extraction, UIE）【1】是百度2022年提出的IE方法  一个框架解决实体抽取、关系抽取、事件抽取、情感抽取  在上述4个任务的共13个测试集中均取得了SOTA图1. UIE在4个任务共13个测试集取得SOTAUIE的总体思路如图2所示图2. UIE的总体框架  SSI：统一不同任务的指令，以prompt的方式引导模型输出  UIE：生成式模型  SEL：统一不同任务的输出SSIStructural schema instructor（SSI）设计了专门的schema结构，用以指示生成式模型输出特定IE任务的特定内容SSI包含3类元素：  spotname: 指定IE任务中的spotting name  assoname: 指定IE任务中关系类  special symbols: 标记词，如[spot][asso][text]将SSI与text组合在一起作为UIE的输入，如图2所示，[spot] person [asso] work for [text] Steve became CEO of Apple in 1997.为最终输入生成式模型的内容SELStructured extraction language（SEL）“规定”了生成式模型的输出格式，覆盖了不同IE任务的需求图3. 左：SSI格式；右：输出示例图3右是句子Steve became CEO of Apple in 1997.的输出示例其中蓝色部分是关系抽取的结果，红色部分是事件抽取的结果，黑色是实体抽取的结果1）Spot Name需要定位的信息点的类型，如实体、事件触发词等示例中输出了person、start-position、organization和time类型实体的值2）Asso Name与上一级信息点类型的关系名称示例中输出了person -&gt; work for -&gt; 的关系的值3）Info Span抽取的信息点的值或关系对应的值UIE论文中选取encoder-decoder结构的模型，基于T5-v1.1-base和T5-v1.1-large训练得到UIE-base和UIE-large预训练UIE在预训练阶段构建了3种训练语料，用于训练模型3方面的能力1）文本2结构 能力（训练 编码器+解码器）对Wikidata做对齐，构造对称数据集$D_{pair}$，每对数据包含  token序列$x$  结构化（SSI）记录$y$文章实验发现，如果schema中的指示全为正样本，模型可能只会记住三元组关系，无法建立通用映射能力，因此在训练数据的schema中混入负样本\\[S_{meta}=S_{+}\\cup S_{s-}\\cup S_{a-} \\tag{1}\\]其中$S_{+}=S_{s+}\\cup S_{a+}$是正样本，$S_{s+}$是信息点类型spot，$S_{a+}$是关系名称asso对应的$S_{s-}$和$S_{a-}$分别为$x$中不存在的信息点类型和关系名称# 举个例子假设x为:                 \"Steve became CEO of Apple in 1997.\"那么正样本的schema可以为: \"[spot] person [asso] work for [text] Steve became CEO of Apple in 1997.\"混入负样本的schema可以为: \"[spot] person [asso] work for [spot] vehicle [asso] located in [text] Steve became CEO of Apple in 1997.\"其中\"vehicle\"和\"located in\"在x中是不存在的，我们希望模型对应的输出为空用于$D_{pair}$数据集的训练目标函数如式2所示\\[L_{pair}=\\sum_{(x,y)\\in D_{pair}}-log\\ p(y|x,s_{meta};\\theta_e,\\theta_d) \\tag{2}\\]其中$\\theta_e$和$\\theta_d$分别代表模型编码器和解码器的参数2）结构化生成 能力（训练 解码器）从ConceptNet【2】和Wikidata中整理构造数据集$D_{record}$该数据集均为结构化（SSI）的$y$，用于训练模型结构化解码的能力用于$D_{record}$数据集的训练目标函数如式3所示\\[L_{record}=\\sum_{y\\in D_{record}}-log\\ p(y_i|y_{&lt;i};\\theta_d) \\tag{3}\\]3）语义编码 能力（训练 编码器）从Wikipedia中获取非结构化的文本数据，整理得到$D_{text}$  与$D_{pair}$一起训练，因此也同时更新编码器与解码器参数  训练方式与bert一样，挖出一些span，让模型预测被mask的内容  文章发现该部分训练可以有效缓解模型对指令语义（spotname、assoname）的遗忘\\[L_{text}=\\sum_{x\\in D_{text}}-log\\ p(x''|x';\\theta_e,\\theta_d) \\tag{4}\\]最终，整体的训练误差如式5所示\\[L=L_{pair}+L_{record}+L_{text} \\tag{5}\\]图4. 针对不同数据集的消融实验图4中  UIE-base是使用了$D_{pair}$、$D_{record}$和$D_{text}$训练后在4类任务上的精度  下面3行是分别剔除$D_{pair}$、$D_{record}$和$D_{text}$训练后的精度对比微调以预训练的模型为基础做微调，可以迅速建立针对特定任务的IE能力用于微调的数据集$D_{task}$与$D_{pair}$类似，包含schema、文本x、结构化输出y\\[L_{FT}=\\sum_{(s,x,y)\\in D_{task}}-log\\ p(y|x,s;\\theta_e,\\theta_d) \\tag{6}\\]在微调过程中文章设计了名为拒答机制的方法：  在schema中随机插入负样本（参考式1）  以一定概率在y中插入负样本对应的NULL结果图5. 微调过程中的“拒答机制”如图5所示，spot类型facility在输入的例句中不存在，作为负样本插入schema中，相应的，输出的y有一定概率被插入facility: [NULL]显然该机制本质上是数据增强，让模型学会对指令中不合理的部分拒绝回答图6. 拒答机制的消融实验以图6的UIE-base为例，使用了拒答机制训练的精度为79.54%，没使用时为68.13%图7为UIE在预训练和微调阶段的超参数设置情况图7. 预训练和微调阶段的超参数文献【1】Lu Y, Liu Q, Dai D, et al. Unified structure generation for universal information extraction[J]. arXiv preprint arXiv:2203.12277, 2022.【2】Speer R, Chin J, Havasi C. Conceptnet 5.5: An open multilingual graph of general knowledge[C]//Proceedings of the AAAI conference on artificial intelligence. 2017, 31(1)."
  },
  
  {
    "title": "【文本相似性】（2）BGE系列",
    "url": "/posts/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E6%80%A7-2-BGE%E7%B3%BB%E5%88%97/",
    "categories": "LLM, 文本相似性",
    "tags": "文本相似性",
    "date": "2024-03-07 00:00:00 +0000",
    





    
    "snippet": "BGE（BAAI General Embedding）【1】是智源研究院2023年公开的embedding模型（包含small、base、large三种大小，embedding维度分别为512、768和1024）同时智源还提出了自己的训练和评估测试集，下图是BGE与其他模型在评估测试集上的表现图1. BGE系列与其他模型的精度对比BGE在github上开放了模型和数据的下载数据集如图2所示，...",
    "content": "BGE（BAAI General Embedding）【1】是智源研究院2023年公开的embedding模型（包含small、base、large三种大小，embedding维度分别为512、768和1024）同时智源还提出了自己的训练和评估测试集，下图是BGE与其他模型在评估测试集上的表现图1. BGE系列与其他模型的精度对比BGE在github上开放了模型和数据的下载数据集如图2所示，论文将所有的东西打包称为C-PACK，里面包含3个资源  C-MTEB：中文文本embedding的评估数据集，包含6种任务35个数据集  C-MTP：embedding模型的中文训练数据集，包含标注数据和无标注数据  C-TEM：智源推出了BGE系列embedding模型，包含BGE-small、BGE-base、BGE-large图2. C-PACK训练数据集C-MTPC-MTP(Chinese Massive Text Pairs)的主要数据来源是中文预训练语料库悟道【2】，最终清洗得到1亿中文语料、2亿英文语料C-MTP分成labeled和unlabeled两类，前者数据量少（838465对）但质量高，后者根据原始数据集本身的格式推断得到（如标题与正文认为是相关的），数据量大图3. C-MTP中unlabeled和labeled的来源与数据量对比为了进一步保证unlabeled部分数据的质量，C-MTP使用第三方工具Text2Vec-Chinese为数据对的相关性打分，以0.43为阈值进行过滤智源在其官网开放了C-MTP的下载，以下是其中英文语料的来源图4. C-MTP数据来源从图4可以看出，这里的pair是指主题一致的两段文本，可能是question-answer，也可能是title-passage如下所示，C-MTP的每条数据由query查询语句、pos正样本列表和neg负样本列表组成{\"query\": str, \"pos\": List[str], \"neg\":List[str]}评估数据集C-MTEBC-MTEB是英文embedding评估集MTEB的中文对标版本C-MTEB对35个公开数据集的数据做清洗，整理为以下6类任务  检索任务(retrieval)：对每个query，从给定语料库中找到top-k个相关的文档。该任务数据采自BEIR【3】，使用NDCG@10作为评价指标  重排任务(re-ranking)：对每个query，基于语义相似度对给定候选文档列表（通常包含1个正样本和n个负样本）进行排序，使用MAP做评价指标  文本相似任务(STS)：判断给定的两条语句语义上是否相似，使用Spearman作为评价指标  分类任务(classification): 对给定query做出分类，使用平均精度AP作为评价指标  句子对分类任务(pair-classification)：给定一个句子对，根据语义对两者的关系做出分类，使用平均精度AP作为评价指标  聚类任务(clustering)：给定一组句子，根据语义对其做聚类，使用V-measure作为评价指标BGE在MTEB的chinese分支上公开了评估榜单图5. C-MTEB榜单模型训练BGE使用bert的架构，将最后一层的[CLS]作为最终的embedding输出            模型名称      BGE-small      BGE-base      BGE-large                  参数量      24M      102M      326M              向量维度      512      768      1024      其训练流程如图6所示图6. BGE训练pipeline预训练使用悟道数据做预训练使用RetroMAE【4】【5】提出的方法做训练，可以提升预训练模型的语义表征能力未标注数据微调未标注微调数据比标注数据大很多，该阶段可用于训练模型的通用能力：即区分文本对中的正负样本该阶段使用对比学习，引入正负样本进行学习，同时采用【6】提出的in-batch negatives策略，然后将其放到一个大的batch（19200，评估部分会对比不同batchsize对精度的影响）中进行训练，以增强embedding的可区分性\\[min\\sum_{(p,q)}-log\\frac{e^{&lt;e_p,e_q&gt;/\\tau}}{e^{&lt;e_p,e_q&gt;/\\tau}+\\sum_{Q'}e^{&lt;e_p,e_{q'}&gt;/\\tau}} \\tag{1}\\]式1中$p$和$q$是文本对，$q’\\in Q’$是负样本，$\\tau$是温度系数具体来说有以下细节：1）三元组结构训练数据：(query, positive, negative)，其中negative可能包含多个负样本（参考C-MTP的数据格式）2）in-batch negatives策略：在同一个batch中，使用其他数据作为额外的负样本（可以大大增加负样本数量）3）cross-device sharing策略：使用【7】提出的方法，可以在不同GPU间共享反例，这样可以进一步增加负样本数量（使得19200的batchsize成为可能）标注数据微调标注数据的质量很高、任务类型全，可用于训练模型对特定任务的执行能力这里采用了两种策略进行训练1）指令微调通过指令微调可让模型快速获得任务意图# 微调指令示例search relevant passages for the query由于该机制的存在，在实际使用BGE时，需要在query前增加任务相关的指令...elif 'bge-' in model:    from langchain.embeddings import HuggingFaceBgeEmbeddings    if 'zh' in model:        # for chinese model        query_instruction = \"为这个句子生成表示以用于检索相关文章：\"    elif 'en' in model:        # for english model        query_instruction = \"Represent this sentence for searching relevant passages:\"    else:        # maybe ReRanker or else, just use empty string instead        query_instruction = \"\"    embeddings = HuggingFaceBgeEmbeddings(model_name=get_model_path(model),                                          model_kwargs={'device': device},                                          query_instruction=query_instruction)    if model == \"bge-large-zh-noinstruct\":  # bge large -noinstruct embedding        embeddings.query_instruction = \"\"2）难负样例微调在in-batch negatives策略的基础上，额外增加一个从原始语料中挖掘的难负样例$q’$模型评估如图1，论文对比了BGE、Text2Vec、Luotuo、M3E、Multi.E5、OpenAI-Ada-002在C-MTEB上的表现  BGE系列精度领先，特别是在检索任务、句子对分类任务、和重排任务有显著优势  随着模型尺寸的增加，精度有明显增长消融实验图7. 消融对比实验如图7所示，文章在模型训练的不同阶段做了消融对比其中BGE-i是使用未标注样本微调后的模型，BGE-f是进一步使用标注样本微调后的模型1）预训练阶段使用其他预训练的模型（文章选用Chinese-RoBERTa）做未标注样本的微调，评估微调模型（图7中用BGE-i w.o. pre-train表示）在C-MTEB上的表现对比BGE-i和BGE-i w.o. pre-train可以看到，二者总体上精度持平2）未标注样本微调阶段对比BGE-i和BGE-f可以推断：使用高质量、多样化的数据（哪怕数量不大）是可以带来性能显著提升的此外文章还对比了在对比学习的in-batch训练过程中使用不同batchsize对精度的影响（图8）：增大batchsize可以获得持续的性能提升图8. batchsize的消融对比实验3）标注样本微调阶段对比了不使用指令微调的模型（图7中用w.o. Instruct表示）在C-MTEB上的表现，发现使用了指令微调的模型（BGE-f）精度更高：指令微调可以潜在的为领域微调做出贡献文献【1】Xiao S, Liu Z, Zhang P, et al. C-pack: Packaged resources to advance general chinese embedding[J]. arXiv preprint arXiv:2309.07597, 2023.【2】Yuan S, Zhao H, Du Z, et al. Wudaocorpora: A super large-scale chinese corpora for pre-training language models[J]. AI Open, 2021, 2: 65-68.【3】Thakur N, Reimers N, Rücklé A, et al. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models[J]. arXiv preprint arXiv:2104.08663, 2021.【4】Liu Z, Shao Y. Retromae: Pre-training retrieval-oriented transformers via masked auto-encoder[J]. arXiv preprint arXiv:2205.12035, 2022.【5】Xiao S, Liu Z, Shao Y, et al. RetroMAE-2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models[J]. arXiv preprint arXiv:2305.02564, 2023.【6】Karpukhin V, Oğuz B, Min S, et al. Dense passage retrieval for open-domain question answering[J]. arXiv preprint arXiv:2004.04906, 2020.【7】Gao L, Zhang Y, Han J, et al. Scaling deep contrastive learning batch size under memory limited setup[J]. arXiv preprint arXiv:2101.06983, 2021."
  },
  
  {
    "title": "transformers文本生成的解码策略",
    "url": "/posts/transformers%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90%E7%9A%84%E8%A7%A3%E7%A0%81%E7%AD%96%E7%95%A5/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-11-27 00:00:00 +0000",
    





    
    "snippet": "解码策略决定模型按什么方式确定生成的tokenstransformers中支持了以下8种文本生成的解码策略# transformers.generation.utils.GenerationModeclass GenerationMode(ExplicitEnum):    \"\"\"    Possible generation modes, downstream of the [`~gene...",
    "content": "解码策略决定模型按什么方式确定生成的tokenstransformers中支持了以下8种文本生成的解码策略# transformers.generation.utils.GenerationModeclass GenerationMode(ExplicitEnum):    \"\"\"    Possible generation modes, downstream of the [`~generation.GenerationMixin.generate`] method.    \"\"\"    # Non-beam methods    CONTRASTIVE_SEARCH = \"contrastive_search\"    GREEDY_SEARCH = \"greedy_search\"    SAMPLE = \"sample\"    ASSISTED_GENERATION = \"assisted_generation\"    # Beam methods    BEAM_SEARCH = \"beam_search\"    BEAM_SAMPLE = \"beam_sample\"    CONSTRAINED_BEAM_SEARCH = \"constrained_beam_search\"    GROUP_BEAM_SEARCH = \"group_beam_search\"博文介绍了当前部分主流模型使用的解码方案            模型      解码策略      备注                  GPT-2(OpenAI)      greedy decoding      阅读理解任务和翻译任务              GPT-3(OpenAI)      top-p sampling      temperature=1, top_p=0.9              Meena(Google)      sample-and-rank      N=20, temperature=0.88, random sampling              LaMDA(Google)      sample-and-rank      N=16, temperature=1, top_k sampling, k=40              Llama(Meta)      greedy decoding      QA任务      参考文章1）《【自然语言处理】【文本生成】Transformers中用于语言生成的不同解码方法》2）《鹅厂专家讲透AI文本生成解码策略与代码实现》3）《一种简单有效的解码策略：Contrastive Search》4）《辅助生成: 低延迟文本生成的新方向》背景说明对于文本生成的大模型，标准的pipeline是  输入$t$个tokens，模型输出字典中每个token的概率，根据不同的策略选择其中的一个token补充在输入中  然后将这$t+1$个tokens再次输入模型，获得第$t+2$个输出  如此重复直到输出的token为结束标志[EOS]或输出的tokens数达到指定阈值transformers的文本生成函数为GenerationMixin.generate其中GenerationMixin被PreTrainedModel继承，后者是所有模型类的基类，因此可通过llama.generate生成文本temperature参数temperature参数用于调整模型输出的每个token的概率分布\\[p'_t=\\frac{e^{log(p_t)/temperature}}{\\sum_i^te^{log(p_{i})/temperature}} \\tag{1}\\]可以看到，temperature越大，新的概率分布$p’_t$越均匀，意味着模型的输出越随机解码策略      策略名称    子策略    思路    特点        greedy_search    每次选择概率最高的作为下一个token    简单，但无法确保最优路径，无法控制模型重复输出        beam_search    每一步保留概率最大的前num_beams个分支，下一步时对每条路径计算概率乘积，选择总体概率最高的路径    是对greedy_search的改进，更可能找到最优路径，但有多样性不足的问题，无法控制模型重复输出        group_beam_search    将beam_search中的候选路径进行分组，在各组内寻找最优解    是对beam_search在输出多样性方面的改进        constrained_beam_search    允许提前指定解码输出中包含特定的token    是对beam_search带约束的改进        sampling    sampling    以模型对所有tokens输出的概率为权重做随机选择    通过引入随机处理，可以在一定程度增加输出的多样性        sampling and rank    首先采用sampling策略保留多条结果，然后从这些结果中选择最佳的    可以提高sampling的精度        top-k sampling    首先选择前k个概率最高的tokens，然后在这个范围内根据其概率做随机选择    可以提高sampling的精度        top-p sampling    首先将所有tokens按概率从高到低排序，然后从第1个开始做概率累加，当累加值大于阈值p时停止，针对参与累加的tokens，根据其概率做随机选择    与top-k相比，自适应能力更强（前者每一步的k是固定的）        beam sample    在每个t步保留最佳的n个序列，最后对多条结果做选择    是对sample and rank的改进        contrastive search    根据模型预测token与之前token的相似度对概率做衰减    主要考虑优化重复输出的问题        assisted_generation    引入一个小模型做辅助，用以加速大模型的输出    主要考虑优化模型输出效率的问题  transformers解码策略判断及参数transformers是基于输入的参数组合确定解码策略的，图1是解码策略判断函数transformers.generation.utils.GenerationMixin._get_generation_mode的逻辑图1. _get_generation_mode判断逻辑transformers的生成策略参数类为trannsformers.generation.configuration_utils.GenerationConfig，包含的参数如下表所示其中常用的参数有：max_length、min_length、do_sample、top_k、top_p、repetition_penalty、temperature      参数名称    参数含义    值类型    默认值        max_length    输入token+输出token的总长度上限    int    20        max_new_tokens    输出token的总长度上限（推荐使用该参数取代max_length）    int    None        min_length    输入token+输出token的总长度下限    int    0        min_new_tokens    输出token的总长度下限（推荐使用该参数取代min_length）    int    None        early_stopping    仅对beam类策略有效，可选值为\"True\"、\"False\"、\"never\"，对应策略停止搜索的时机    bool、string    False        max_time    限定生成的用时（秒）    float    None        do_sample    是否使用sample策略    bool    False        num_beams    beam search类的策略使用的beams数，1表示不用beam策略    int    1        num_beam_groups    针对group_beam_search，确定分组的数量，1表示不分组，参考论文【2】    int    1        penalty_alpha    针对contrastive search，参考式2，是平衡惩罚项的系数    float    None        use_cache    模型前向推理是否使用cache，可以加速解码    bool    True        temperature    参考式1，调整模型输出概率分布，值越大越随机    float    1.0        top_k    针对top-k sampling和contrastive search    int    50        top_p    针对top-p sampling，小于1时有效    float    1.0        typical_p    不总是从分布高概率区域中选词，而是从信息含量接近预期值（即typical_p）的区域选，小于1时有效，参考论文【3】    float    1.0        epsilon_cutoff    取(0,1)时有效，高于该条件概率的token才会被采样，论文推荐取值为[3e-4,9e-4]，参考论文【4】    float    0.0        eta_cutoff    取(0,1)时有效，基于条件概率满足其公式的token才会被采样，论文推荐取值为[3e-4,2e-3]，参考论文【4】    float    0.0        diversity_penalty    仅对group_beam_search有效    float    0.0        repetition_penalty    对重复token做惩罚，取1.0时不做惩罚，参考论文【5】    float    1.0        encoder_repetition_penalty    对不在原始输入中的序列做指数惩罚，取1.0时不做惩罚    float    1.0        length_penalty    针对beam类的策略，影响生成文本的长度：取值&gt;0鼓励生成长文本，取值&lt;0鼓励生成短文本，取值=0不做惩罚    float    1.0        exponential_decay_length_penalty    用于控制生成文本的长度，当生成一定数量的token后，施加一个指数增长的长度惩罚因子，格式为(start_index, decay_factor)，前者表示开始惩罚的token位置，后者为指数衰减因子    tuple(int, float)    None        no_repeat_ngram_size    控制重复词生成，取值&gt;0时有效，此时连续n-gram个token在生成的语句中只出现1次    int    0        encoder_no_repeat_ngram_size    用于控制重复生成，针对encoder-decoder模型，大于0时有效，此时encoder_input_ids中连续n-gram的tokens不会出现在decoder_input_ids里    int    0        decoder_start_token_id    针对encoder-decoder模型，其解码起始符bos可能跟编码器不一样，可以在这里指定    int    None        bad_words_ids    不允许生成的token    list[list[int]]    None        suppress_tokens    生成内容中会被屏蔽的token    List[int]    None        begin_suppress_tokens    在生成开始时被屏蔽的tokens    List[int]    None        force_words_ids    针对constrained_beam_search策略，强制在生成内容中必须包含的token    list[list[int]]    None        constraints    针对constrained_beam_search策略，基于Constraint类实现的自定义的token限制方法    List[Constraint]    None        renormalize_logits    是否在最后再次将输出做一次规范化，建议设为True以防万一    bool    False        forced_bos_token_id    解码器在生成decoder_start_token_id对应的token后指定生成的token，mBART这种多语言模型会用到，用于区分target语种    int    None        forced_eos_token_id    解码器在生成结束后指定生成的token，mBART这种多语言模型会用到    int    None        remove_invalid_values    是否移除类似nan、inf这种无效输出，可能会降低生成速度    bool    False        forced_decoder_ids    限制生成的前后token的对应，如[[1,123]]表示当生成的token为1时，后续的token一定为123    List[List[int]]    None        sequence_bias    影响标记序列的生成概率，Tuple[int]为标记的token序列，float为正时其生成概率增加，为负时降低    Dict[Tuple[int], float]    None         guidance_scale    大于1时有效，值越大生成的内容与输入越接近，通常会降低生成的质量    float    None         low_memory    仅对contrastive search有效，使用top-k的方式降低内存消耗    bool    None         num_return_sequences    对每条输入生成的语句数量    int    1         output_attentions    是否返回所有attention层的输出    bool    False        output_hidden_states    是否返回所有层的hidden_states    bool    False        output_scores    是否返回模型预测概率    bool    False        return_dict_in_generate    是否返回模型的字典型输出    bool    False        pad_token_id    指定padding token    int    None        bos_token_id    指定bos token    int    None        eos_token_id    指定eos token    int    None        num_assistant_tokens    仅对assisted_generation策略有效，指定小模型（辅助模型）在每个回合中生成的tokens数量（之后交由大模型做check），该值越大速度越快，但生成的质量可能会降低    int    5        num_assistant_tokens_schedule    仅对assisted_generation策略有效，确定num_assistant_tokens的变化策略：    \"heuristic\"：如果辅助模型本轮生成的所有tokens都被大模型判断为正确，则下一轮的num_assistant_tokens+2，否则num_assistant_tokens-1；\"constant\"：始终保持num_assistant_tokens不变    string    heuristic  beam_search策略详述图2. num_beams取2时每一步考虑2个分支如图2所示，当num_beams取2时，$t=1$会同时保留nice(0.5)和dog(0.4)，$t=2$时通过计算dog has(0.4*0.9)和nice woman(0.5*0.4)，确定输出为The dog hasconstrained_beam_search策略详述constrained_beam_search是beam_search带约束的改进，其允许提前指定解码输出中包含特定的tokenencoder_input_str = \"translate English to German: How old are you?\"# 输出中强制包含force_words# 输出中强制包含force_flexible中的某一个force_words = [\"Sie\"]force_flexible = [\"scream\", \"screams\", \"screaming\", \"screamed\"]input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\").input_idsforce_words_ids = [    tokenizer(force_words, add_special_tokens=False).input_ids,    tokenizer(force_flexible, add_special_tokens=False).input_ids,]outputs = model.generate(    input_ids,    force_words_ids=force_words_ids,    num_beams=5,    num_return_sequences=1,    no_repeat_ngram_size=1,    remove_invalid_values=True,)contrastive search策略详述方法出自论文《A contrastive framework for neural text generation》【1】contrastive search主要考虑优化重复输出的问题\\[x_t=\\arg\\mathop{\\max}\\limits_{v\\in V^k}\\{(1-\\alpha)p_{\\theta}(v|x_{&lt;t})-\\alpha(max\\{s(h_v,h_{x_j}):1\\le j\\le t-1\\})\\} \\tag{2}\\]如式2所示  $p_{\\theta}(v\\lvert x_{&lt;t})$是模型首先根据前$t-1$个tokens输出的第$t$个位置所有tokens的概率，然后对其按从大到小排序，选择前$k$个得到的输出  $V^k$就是上述$k$个输出组成的集合  后半部分是取token $v$与前$t-1$个token最高的相似度。该部分作为“惩罚”项，相似度越高对模型赋予的概率衰减越大  $\\alpha$是调节参数assisted_generation策略详述该方法引入一个小模型做辅助，用以加速大模型的输出参考Huggingface的博客文章，其基本思路是  使用小模型通过$n$次迭代在输入的基础上补充$n$个token  然后将原始输入加上述的$n$个token输入大模型，因为模型每个位置的输出是对上一个token的预测，因此可以用来评价小模型的每个输出是否“正确”  截取连续正确的前$m(m&lt;=n)$个token补充在原始输入中重复上述过程这种方式可以加速模型输出，当小模型的质量极差时退化到greedy_search图3. assisted generation策略文献【1】Su Y, Lan T, Wang Y, et al. A contrastive framework for neural text generation[J]. Advances in Neural Information Processing Systems, 2022, 35: 21548-21561.【2】Vijayakumar A K, Cogswell M, Selvaraju R R, et al. Diverse beam search: Decoding diverse solutions from neural sequence models[J]. arXiv preprint arXiv:1610.02424, 2016.【3】Meister C, Pimentel T, Wiher G, et al. Locally typical sampling[J]. Transactions of the Association for Computational Linguistics, 2023, 11: 102-121.【4】Hewitt J, Manning C D, Liang P. Truncation sampling as language model desmoothing[J]. arXiv preprint arXiv:2210.15191, 2022.【5】Keskar N S, McCann B, Varshney L R, et al. Ctrl: A conditional transformer language model for controllable generation[J]. arXiv preprint arXiv:1909.05858, 2019."
  },
  
  {
    "title": "【文本相似性】（1）SBert",
    "url": "/posts/%E6%96%87%E6%9C%AC%E7%9B%B8%E4%BC%BC%E6%80%A7-1-SBert/",
    "categories": "LLM, 文本相似性",
    "tags": "文本相似性",
    "date": "2023-10-30 00:00:00 +0000",
    





    
    "snippet": "基于BERT做文本相似性度量有2种传统思路  将两个句子输入BERT，直接输出两者是否相似  分别将两个句子输入BERT，取输出层的embedding，或者输出层中[CLS]token的向量表示，通过计算量两组向量的距离给出相似度第1种方法每对比一次需要encode两条语句，计算量太大；第2种方法精度表现比不上GloVeSentence-BERT(SBert)【1】是2019年提出的计算文本...",
    "content": "基于BERT做文本相似性度量有2种传统思路  将两个句子输入BERT，直接输出两者是否相似  分别将两个句子输入BERT，取输出层的embedding，或者输出层中[CLS]token的向量表示，通过计算量两组向量的距离给出相似度第1种方法每对比一次需要encode两条语句，计算量太大；第2种方法精度表现比不上GloVeSentence-BERT(SBert)【1】是2019年提出的计算文本相似度的方法，方法基于BERT设计了孪生网络结构，在STS任务上优于InferSent和USE图1. 基于NLI数据做SFT的Sbert在STS数据集上取得了最好的效果官方代码、官方主页模型设计SBert分别从BERT输出的池化策略和训练损失函数两方面做了研究池化策略对BERT的输出做池化可以确保生成长度一致的特征向量，SBert对比了以下3种方式  使用输出中的[CLS]  对输出做平均池化  对输出做最大池化实验发现平均池化效果最好损失函数SBert尝试了3种损失函数图2. (a)分类目标函数网络结构；(b)回归目标函数网络结构  训练过程使用Adam优化器，lr取2e-5，仅训练1个epoch  以上3种结构只在训练过程中使用，在推理过程中仅分别计算embedding，通过余弦值衡量相似度分类目标函数如图2(a)所示，两条语句分别经过权重一致的BERT网络（孪生网络），将得到的embedding做组合后输入分类头得到是否相似的分类结果文章通过消融实验发现，基于分类目标函数的网络  池化策略的选择影响不大  输入分类头的组合方式影响更大图3. 分类头目标函数的消融实验如图3所示，基于分类头目标函数的方法，采用平均池化 + (u,v,|u-v|)的组合效果最好回归目标函数如图2(b)所示，两条语句分别经过权重一致的BERT网络（孪生网络），将得到的embedding计算余弦相似度作为相似度得分，使用均方根误差MSE计算损失文章通过消融实验发现，基于回归目标函数的网络  池化策略影响最大  MAX策略明显比MEAN和CLS策略差Triplet目标函数对于待训练的语句$a$，给定一个与之相似的语句$p$和与之不相似的语句$n$Triplet的训练目标是使得embedding表示$s_a$与$s_p$的距离远小于$s_a$与$s_n$的距离（与人脸识别的训练方法类似）\\[max(||s_a-s_p||-||s_a-s_n||+\\epsilon, 0) \\tag{1}\\]这里采用欧式距离计算，默认设置$\\epsilon=1$Spearman评价指标图1和图3所涉的精度都是Spearman相关性，即模型输出所有样本对的相似度列表$A$，将其与GT相似度列表$B$计算相关性\\[r_s=1-\\frac{6\\sum_{i=1}^n(R_i-Q_i)^2}{n(n^2-1)} \\tag{2}\\]其中$R_i$是$A$的秩次，$Q_i$是$B$的秩次，$n$是列表的长度秩次假设$A$组和$B$组的值为            A组      4.7      6.4      2.6      3.2      6.2                  B组      1.7      2.6      3.6      2.3      3.7      将其按从小到大统一排序，标记出现的次序，即为秩次            秩次      1      2      (3+4)/2=3.5      5      6      7      8      9      10                  A组                    2.6      3.2                    4.7      5.2      6.4              B组      1.7      2.3      2.6             3.6      3.7                           文献【1】Reimers N, Gurevych I. Sentence-bert: Sentence embeddings using siamese bert-networks[J]. arXiv preprint arXiv:1908.10084, 2019."
  },
  
  {
    "title": "【大模型安全性评估】（1）诚实性(模型幻觉)",
    "url": "/posts/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%AE%89%E5%85%A8%E6%80%A7%E8%AF%84%E4%BC%B0-1-%E8%AF%9A%E5%AE%9E%E6%80%A7(%E6%A8%A1%E5%9E%8B%E5%B9%BB%E8%A7%89)/",
    "categories": "LLM, 大模型安全性评估",
    "tags": "大模型安全性评估",
    "date": "2023-09-14 00:00:00 +0000",
    





    
    "snippet": "由于LLM生成的内容完全取决于训练数据的分布，因此会出现一本正经的胡说八道(initative falsehoods，模仿人类表达方式输出误导信息)的情况图1. GPT-3回答的事实性错误  OpenAI2021年提出的TruthfulQA【1】是首个关注LLM诚实性方面的评估数据集  亚马逊2023年提出了RefChecker幻觉检测思路，同时开放了基于该思路建立的评测题库理论分析诚实性的...",
    "content": "由于LLM生成的内容完全取决于训练数据的分布，因此会出现一本正经的胡说八道(initative falsehoods，模仿人类表达方式输出误导信息)的情况图1. GPT-3回答的事实性错误  OpenAI2021年提出的TruthfulQA【1】是首个关注LLM诚实性方面的评估数据集  亚马逊2023年提出了RefChecker幻觉检测思路，同时开放了基于该思路建立的评测题库理论分析诚实性的定义  模型的输出从字面上符合真实世界的规律  模型拒绝输出，如“我不知道”、“我不做评价”等，也被视为是诚实的根据上述定义可以看到，一个模型足够诚实或许未必是最优的，比如它可以对任何话题都回答“我不知道”，因此还需要考虑模型回答的信息量图2. 不同模型在TruthfulQA上的表现如图2，TruthfulQA经过对比发现，规模越大的模型似乎更容易胡说文章认为这是由于一些问题在训练数据中的分布较低，模型必须从稀疏的数据中推断答案  对于小模型可能会产生随机或无信息的答案（如“我不知道”）  对于大模型则尝试做出更准确和类似人类的推论，导致其有时会产生似是而非的错误答案文章同样猜测，对于经过更大规模训练数据的超大模型表现可能会更好此外文章提出了改善模型诚实性表现的几个建议  提升模型大小的同时改进训练数据  在prompts中增加thruthful的指示  针对性数据的fine-tuning  针对性数据的RLHFTruthfullQA的评估方法TruthfulQA数据集包含817个QA对，分为38个类别，涉及健康、法律、金融和政治领域这817个QA对由两部分组成  对抗性数据：手动编写一些人类容易答错的question，然后使用LLM测试，过滤掉LLM回答正确的questions，将剩下的（437条）作为filtered questions  非对抗性数据：另外写380条人类和LLM容易答错的作为unfiltered questions图3. TruthfulQA数据集TruthfulQA数据在官方代码库中的TruthfulQA.csv文件中，数据结构如图3所示  Type：属于对抗性数据还是非对抗性数据  Category：问题涉及的领域  Question：人工设计的question，只包含1句话  Best Answer：人工编写的最佳回答，只包含1句话  Correct Answers：人工编写的正确回答，包含多句话（包含Best Answer），以分号隔开  Incorrect Answers：人工编写的错误回答，包含多句话，以分号隔开  Source：question原始来源文章提出了两种自动化评测的方案  评估模型输出与Correct Answers和Incorrect Answers中每条语句的相似性  训练一个judge模型给被评估模型的输出打分1）judge模型方案文章使用了6.6k个训练数据，基于GPT-3-13B训练了一个分类器以实现自动评测模型输入是question+answer，输出为True则表示认可该answer此外文章还训练了一个评价answer是否富含信息量的分类器2）相似性评价方案文章使用了BLUE【2】、ROUGE1【3】和BLEURT【4】（推荐使用）相似性评估算法此外官方代码里还统计了MC1、MC2、MC3指标\\[MC1=\\begin{cases}1,&amp;P_{best}&gt;P_{false}^{max}\\\\0,&amp;else\\end{cases} \\tag{1}\\]\\[MC2=\\frac{\\sum_{i=0}^m{(P_{true}^i&gt;P_{false}^{max}})}{m} \\tag{2}\\]\\[MC3=\\frac{\\sum_{i=0}^m{exp(P_{true}^i})}{\\sum_{i=0}^m{exp(P_{true}^i})+\\sum_{j=0}^n{exp(P_{false}^j})} \\tag{3}\\]式子(1)(2)(3)中  $m$和$n$分别表示一个Question对应的Correct Answers数量和Incorrect Answers数量  $P_{best}$是将Question+Best Answer组合输入模型，对模型输出中Best Answer对应位置的概率相加得到的值（可以理解，模型越“认可”该answer，累加的概率越高）  $P_{false}^{max}$依次将Question+Incorrect Answer组合输入模型（Incorrect Answers有多条语句），得到的最大概率累加值RefChecker的评估思路RefChecker是亚马逊以开源项目方式公开的方案，没有发表对应的论文总体来说RefChecker将幻觉评测分为以下几个步骤：  三元组提取：利用LLM将模型输出整理为多个三元组  三元组评估：利用LLM或其他自然语言算法检查提取的三元组是否事实正确，并给出评价  证据定位：找到评估结论在参考知识中的证据定位三元组提取这是RefChecker最大的特色，之前的方法  要么是直接对模型输出做评价（如TruthfullQA）  要么是将模型输出做简单加工，提炼成一个短句（如FAcScore和FACTOOL）RefChecker认为三元组的形式更能直观有效表达知识这个想法很好，但如何准确有效提取三元组其实是一个难题，比如以下例子Reference: Bob got his first son Clark in 1982. Two little sisters of Clark were born after 4 years. And he welcomed his youngest son last year.# 要得到如下正确的三元组，需要模型能完整理解整个语句，并具有一定推理能力Triplet: Bob has three children.三元组评估首先方法将模型输出分为3种模式：  zero context：没有任何参考，完全基于模型本身的知识做输出  noisy context: 如rag，以文档、搜索结果等形式为模型的输出提供知识参考  accurate context: 模型的任务是对给定的文档做改写、总结等工作根据以上3种模式，RefChecker建立了包含300个样本的评测集，每种模式100个样本图4. RefChecker评测集检查模型根据样本的答案对模型输出的三元组做评价图5. 3种评估结论如图5所示，references是评测输入涉及的知识，response是模型输出涉及的知识，其中  entailment：是输出知识与参考知识一致的部分  contraction: 是输出知识与参考知识冲突的部分  neutral: 是输出知识不在参考知识范围内的部分最终对模型一条输出的多个三元组统计以上评估结论的占比，给出幻觉评分证据定位方法将参考知识分割成spans，经过embedding模型获得向量表示，与三元组的向量表示作比较，挑选距离最近的作为证据图6. 基于embedding的证据定位文献【1】Lin S, Hilton J, Evans O. Truthfulqa: Measuring how models mimic human falsehoods[J]. arXiv preprint arXiv:2109.07958, 2021.【2】Papineni K, Roukos S, Ward T, et al. Bleu: a method for automatic evaluation of machine translation[C]//Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 2002: 311-318.【3】Lin C Y. Rouge: A package for automatic evaluation of summaries[C]//Text summarization branches out. 2004: 74-81.【4】Sellam T, Das D, Parikh A P. BLEURT: Learning robust metrics for text generation[J]. arXiv preprint arXiv:2004.04696, 2020."
  },
  
  {
    "title": "【从0训练LLM】（3）Tokenizer的应用 - SentencePiece训练、扩充、Transformers中的使用",
    "url": "/posts/%E4%BB%8E0%E8%AE%AD%E7%BB%83LLM-3-Tokenizer%E7%9A%84%E5%BA%94%E7%94%A8-SentencePiece%E8%AE%AD%E7%BB%83-%E6%89%A9%E5%85%85-Transformers%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8/",
    "categories": "LLM, 从0训练LLM",
    "tags": "从0训练LLM",
    "date": "2023-09-11 00:00:00 +0000",
    





    
    "snippet": "SentencePiece的训练及编解码接口SentencePiece提供了Python Wrapper支持分词模型的训练和字符序列的编解码训练接口import sentencepiece as spmspm.SentencePieceTrainer.Train(    input='cor.txt',    model_prefix='cc',    vocab_size=80,    u...",
    "content": "SentencePiece的训练及编解码接口SentencePiece提供了Python Wrapper支持分词模型的训练和字符序列的编解码训练接口import sentencepiece as spmspm.SentencePieceTrainer.Train(    input='cor.txt',    model_prefix='cc',    vocab_size=80,    user_defined_symbols=['[ZZ]','[IGNORE]'],    character_coverage=1.0,  # [0.98,1.0]    model_type='unigram')  input：用于训练的语料文本文件（可以为文件路径、多个文件所在的目录），文件每行可以为一句或多句  model_prefix：保存的模型的名称前缀，训练结束会保存.model和.vocab两个文件  vocab_size：预设的词汇表大小，其允许设置的上下限与语料文本和其他参数有关  user_defined_symbols：用户自定义的符号，这些符号不会被拆成子词，而是作为单独的token  character_converage：模型覆盖的字符数量比例（允许的范围为[0.98,1.0]）。对于中文、日文，由于字符很丰富，推荐设为0.9995，对于英文等字符集较小的语言，可以设为1.0  model_type：指定算法类型，支持'unigram'、'bpe'、'char'、'word'，其中'unigram'是默认值  unk_id：指定未登陆词OOV的id号，默认为0  bos_id：指定句子开头符号的id号，默认为1  eos_id：指定句子结束符号的id号，默认为2  pad_id：指定填充符号的id号，默认为-1训练生成的.model用于编解码时模型加载，.vocab文件供明文参考# 生成.vocab示例&lt;unk&gt;\t0&lt;s&gt;\t0&lt;/s&gt;\t0[ZZ]\t0[IGNORE]\t0的\t-2.99928,\t-2.99928r\t-3.58261a\t-4.08261▁\t-4.08261于\t-4.08261和\t-4.08261....vocab中每行内容由token和概率组成，token所在的行号（从0开始）就是其对应的id编解码接口1）模型加载import sentencepiece as spmsp = spm.SentencePieceProcessor(    model_file='cc.model')# # 等价于# sp = spm.SentencePieceProcessor()# sp.Load('cc.model')2）编码# 分词、id映射&gt;&gt; out = sp.encode(\"测试 分词器\", out_type=str) ['▁', '测试', '▁', '分', '词器']&gt;&gt; sp.encode_as_pieces(\"测试 分词器\")['▁', '测试', '▁', '分', '词器']&gt;&gt; ids = sp.piece_to_id(out)[9, 0, 9, 40, 20]# 分词+id映射&gt;&gt; sp.encode(\"测试 分词器\", out_type=int)[9, 10, 9, 40, 20]&gt;&gt; sp.encode_as_ids(\"测试 分词器\")[9, 10, 9, 40, 20]# 接受列表输入&gt;&gt; sp.encode([\"测试 分词器\", \"hello world!\"], out_type=int)[[9, 10, 9, 40, 20], [9, 25, 24, 30, 27, 9, 29, 27, 7, 40, 23, 50]]# 单token映射&gt;&gt; sp.piece_to_id('&lt;s&gt;')1&gt;&gt; sp.piece_to_id(['&lt;s&gt;','的'])[1, 5]&gt;&gt; sp['&lt;s&gt;']1&gt;&gt; sp[['&lt;s&gt;','的']][1, 5]3）解码&gt;&gt; sp.decode([9, 10, 9, 40, 20])'测试 分词器'# 单token映射&gt;&gt; sp.id_to_piece(5)'的'&gt;&gt; sp.id_to_piece([0,1,2,5])['&lt;unk&gt;', '&lt;s&gt;', '&lt;/s&gt;', '的']4）词汇表大小&gt;&gt; sp.get_piece_size()76&gt;&gt; len(sp)76基于SentenceSpiece的词表扩充很多时候我们需要在已有的词汇表中扩充自己的词汇比如Llama官方的词汇表对中文的支持非常有限，因此需要在官方的基础上做扩充参考Chinese-LLaMA-Alpaca的工作，其基本思路很简单：  使用中文语料训练一个中文tokenizer  中文tokenizer中的token与官方tokenizer去重后做append  保存扩充后的模型# https://github.com/ymcui/Chinese-LLaMA-Alpaca/blob/main/scripts/merge_tokenizer/merge_tokenizers.pyimport sentencepiece as spmfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model### load# 官方tokenizerllama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)# 中文tokenizerchinese_sp_model = spm.SentencePieceProcessor()chinese_sp_model.Load(chinese_sp_model_file)### 通过sp_pb2_model对tokenizer模型做parsellama_spm = sp_pb2_model.ModelProto()llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())chinese_spm = sp_pb2_model.ModelProto()chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())### 每个token做去重检查后append# piece是描述一个token的结构体，包含3项内容：# &gt;&gt; spm.pieces[0]  # piece: \"&lt;unk&gt;\"# score: 0.0# type: UNKNOWNllama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)for p in chinese_spm.pieces:    piece = p.piece    if piece not in llama_spm_tokens_set:        new_p = sp_pb2_model.ModelProto().SentencePiece()        new_p.piece = piece        new_p.score = 0        llama_spm.pieces.append(new_p)### 保存扩展后的tokenizer模型with open(output_sp_dir+'/chinese_llama.model', 'wb') as f:    f.write(llama_spm.SerializeToString())### 封装成huggingface的文件格式# 会生成3个文件: # special_tokens_map.json# tokenizer_config.json# tokenizer.model# 其中tokenizer.model与chinese_llama.model其实是一样的tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/chinese_llama.model')tokenizer.save_pretrained(output_hf_dir)Transformers中使用Tokenizer图1. Transformer中的Tokenizer类继承图1是Transformer中下游算法tokenizer的继承关系图2对各个类的函数关系做了梳理图2. 各个类的函数关系在Transformers中，一般通过from_pretrained方法初始化Tokenizer## Tokenizer初始化# llama_tokenizer_dir为分词器模型所在目录llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)## string的编码context = \"I'm what i am.\"encoded = llama_tokenizer.encode(    context,    max_length=1024,    truncation=True,    return_tensors=\"pt\")## ids序列的解码decoded = llama_tokenizer.decode(encoded[0])from_pretrainedfrom_pretrained会检查目录下的4个文件  tokenizer.model：分词器模型（必须提供）  tokenizer_config.json：分词器的配置文件（可不提供，会使用默认配置）  added_tokens.json：额外添加的tokens，不包含在词汇表中（可不提供）  special_tokens_map.json：特殊tokens（如&lt;bos&gt;）的映射关系（可不提供）# special_tokens_map.json示例{   \"bos_token\": {     \"content\": \"&lt;s&gt;\",     \"lstrip\": false,     #是否贪婪匹配token左边的空格                          #比如对于'[MASK]'，输入为'a [MASK]'                          #如果为true，则匹配的结果为' [MASK]'     \"normalized\": true,  #为true则忽略大小写     \"rstrip\": false,     #是否贪婪匹配token右边的空格     \"single_word\": false #该token是否可以匹配词的一部分                          #比如对于'ing'，如果为false则可匹配'looking'   },  ...}图3. from_pretrained执行流程encode在encode阶段需要重写的函数有2个  _tokenize：调用算法实现分词  _convert_token_to_id：基于词汇表获得token到id的映射图4. encode执行流程decode在decode阶段需要重写的函数有2个  _convert_id_to_token：基于词汇表获得id到token的映射  convert_tokens_to_string：调用分词算法对tokens做拼接和处理图5. decode执行流程"
  },
  
  {
    "title": "【从0训练LLM】（2）Tokenizer的理论 - BPE、WordPiece、UniLM、SentencePiece",
    "url": "/posts/%E4%BB%8E0%E8%AE%AD%E7%BB%83LLM-2-Tokenizer%E7%9A%84%E7%90%86%E8%AE%BA-BPE-WordPiece-UniLM-SentencePiece/",
    "categories": "LLM, 从0训练LLM",
    "tags": "从0训练LLM",
    "date": "2023-09-08 00:00:00 +0000",
    





    
    "snippet": "在输入模型之前，为了方便计算机处理，需要将字符串形式的文本转换成数字形式在NLP应用中，该环节由指定的Tokenizer完成，其主要包含2个步骤：  字符串的分词（tokenize）：输出为若干token  分词结果的映射：输出为各个token对应的id# bert模型的tokenizer&gt;&gt;&gt; tokenizer = BertTokenizer(vocab_file, d...",
    "content": "在输入模型之前，为了方便计算机处理，需要将字符串形式的文本转换成数字形式在NLP应用中，该环节由指定的Tokenizer完成，其主要包含2个步骤：  字符串的分词（tokenize）：输出为若干token  分词结果的映射：输出为各个token对应的id# bert模型的tokenizer&gt;&gt;&gt; tokenizer = BertTokenizer(vocab_file, do_basic_tokenize=True)# 分词&gt;&gt;&gt; tokens = tokenizer.tokenize('关于BERT的分词器')&gt;&gt;&gt; tokens                ['关', '于', 'be', '##rt', '的', '分', '词', '器']# 根据分词列表返回字典id&gt;&gt;&gt; tokenizer.convert_tokens_to_ids(tokens)[1068, 754, 8815, 8716, 4638, 1146, 6404, 1690]# encode方法包含了上面2个步骤，同时做了一定加工（句子前后加[CLS]和[SEP]）&gt;&gt;&gt; ids = tokenizer.encode('关于BERT的分词器')  &gt;&gt;&gt; ids[101, 1068, 754, 8815, 8716, 4638, 1146, 6404, 1690, 102]注意到其中的vocab_file，是输入给分词器的词汇表  分词阶段输出的token应该包含在词汇表中，否则称该token为OOV(Out of Vocabulary)  映射阶段输出的id是token在词典中的序号# bert模型的vocab.txt# id token0 [PAD]1 [unused1]2 [unused2]...100 [UNK]101 [CLS]102 [SEP]103 [MASK]104 [unused100]105 [unused101]106 !107 \"从上面的介绍可以看到，Tokenizer的难点在分词和词汇表构建主流分词算法图1. 主流分词算法分类如图1所示，主流的分词算法按粒度可以划分为三种词粒度# englishquestion answering system  =&gt; question/answering/system# chinese问答系统 =&gt; 问答/系统  对英文等语言，由于存在天然的分隔符（空格、标点等），因此通过简单的规则就可以实现词粒度的分词  对中文等语言，由于缺乏天然分隔符，需要特别设计的分词算法才能实现基于词粒度的方法最大的问题是，由于词语本身的多样性，词汇表会非常大，消耗大量内存（常规词汇表大小不超过5万）字粒度# englishquestion answering system  =&gt; q/u/e/s/t/i/o/n/a/n/s/w/e/r/i/n/g/s/y/s/t/e/m# chinese问答系统 =&gt; 问/答/系/统对英文等语言，将其按字母分割；对中文则将其按汉字分割字粒度的切分方法解决了词粒度中词汇表过大的问题，但带来了另外2个问题  训练难度更大：一个字母或者一个汉字没有确定的语义  计算量更大：切分结果的长度大大增加子词粒度粒度介于字粒度和词粒度之间，将一个词切成更小的子词，通过有限的子词组合无限的词部分内容参考PaddlePaddle文档# bert算法自带的tokenizer# englishquestion answering system  =&gt; q/##ue/##st/##ion/an/##s/##wer/##ing/system'# chinese问答系统 =&gt; 问/答/系/统子词粒度的分词算法主要有3种：BPE、WordPiece、UniLMBPE字节对编码（BPE, Byte Pair Encoding）是子词算法中的一种代表性方法【1】BPE适用于拉丁语系，形式上更多的是对单词的前缀、后缀做分解1）词汇表创建BPE根据训练语料构建词汇表，一般性步骤为  准备模型的训练语料，确定期望的词汇表大小  将训练语料中所有单词拆分为字符，构建初始的词汇表  统计训练语料中每个连续字节对出现的频率，选择频率最高的合并为新的子词(token)，更新词汇表  不断重复上一步，知道词汇表达到预定大小，或者剩下的字节对频率最高为1假设训练语料为low&lt;/w&gt;lower&lt;/w&gt;newer&lt;/w&gt;其中&lt;/w&gt;是额外添加在训练语料中作为词结束的标志首先将训练语料拆分为字符，构建初始词汇表'l','o','w','e','r','&lt;/w&gt;','n'然后查找出现频率最高的连续字符对，这里为'lo'(2次)，更新词汇表# 添加'lo'# 因为在训练语料中'l'和'o'没有单独出现，因此删除'l'和'o''w','e','r','&lt;/w&gt;','n','lo'再次查找出现频率最高的连续字符对，这里为'low'(2次)，更新词汇表# 添加'low'# 因为在训练语料中'lo'没有单独出现，因此删除'lo'# 因为在训练语料中'w'不止出现在'low'组合，因此保留'w''w','e','r','&lt;/w&gt;','n','low'再次查找出现频率最高的连续字符对，这里为'er'(2次)，更新词汇表# 添加'er'# 因为在训练语料中'r'没有单独出现，因此删除'r''w','e','&lt;/w&gt;','n','low','er'不断重复上述操作，直到满足要求2）语料编码在使用上一步建立好的词汇表对输入语料做编码时  首先将词汇表中的token按长度从长到短排序（贪婪匹配）  对于输入语料中的一个词，遍历排序后的词汇表，不断匹配其中的token  如果遍历词汇表仍未找到某子字符串，则将其编码为&lt;unk&gt;WordPieceWordPiece是BERT使用的分词方法WordPiece原理与BPE很接近，不同之处是在做合并时不是找最高频的组合，而是找能提升语言模型概率最大的组合假设句子$S=(t_1,t_2,…t_n)$由$n$个token组成，同时假设各个token之间是独立的，则$S$的语言模型似然值就是所有token概率的乘积\\[logP(S)=\\sum_{i=1}^nlogP(t_i) \\tag{1}\\]假设将相邻位置的$x$和$y$两个token合并，合并产生的token为$z$，此时句子似然值的变化为\\[logP(t_z)-(logP(t_x)+logP(t_y))=log\\frac{P(t_z)}{P(t_x)P(t_y)} \\tag{2}\\]式2就是两个token之间的互信息，也就是说WordPiece每次选择合并的两个token具有最大的互信息，在语言模型上有较强的关联性UniLMUniLM【2】是为了解决机器翻译中分词的问题而提出的  与WordPiece一样，UniLM也是使用语言模型来挑选子词  但与BPE和WordPiece从小到大创建词汇表不一样，UniLM首先初始化一个大的词汇表，然后根据评估准则不断缩小词汇表1）词汇表创建  初始化一个足够大的词汇表（一般将训练语料中所有的字符加上常见的子字符串，或者使用BPE等算法初始化）  针对当前词汇表，使用EM算法求解每个token在语料上的概率  对每个token，计算当其从词汇表中移除时，总的Loss降低多少，记为该token的Loss  将token的Loss按从大到小排序，丢弃一定比例Loss最小的token（如20%），更新词汇表（单字符不丢弃，避免OOV）  重复2-4步骤，直到词汇表大小满足要求2）语料编码令$\\vec{x}=(x_1,x_2,…,x_m)$是输入句子$S$的一个分词结果，其似然值为\\[P(\\vec{x})=\\prod_{i=1}^mP(x_i) \\tag{3}\\]挑选使式3最大的作为最终的分词结果（使用维特比算法）SentencePieceSentencePiece是Google开源的一个无监督Tokenizer工具  复现了BPE、UniLM、基于char、基于word的4种方法（WordPiece仅供google内部使用）  训练之前固定词汇表大小  无需预分词，直接从原始的语句开始训练  tokenized和detokenized是可逆的1）词汇表的固定多数无监督分词算法在训练过程中会不断调整词汇表的大小，而SentencePiece可以提前指定词汇表大小通常指定的词汇表大小为8k、16k、32k2）从原始语句直接训练多数无监督分词算法需要预先分词（tokenize），会带来2个问题  语言依赖：对不同的语言需要不同的分词器  分词依赖：分词算法最终的效果依赖于预分词的效果SentencePiece直接在原始句子上训练，对中文和日语这种无显式空格的语言也能很好支持3）tokenized和detokenized的可逆多数无监督分词算法将空格作为特殊符号（分割符），这样会导致空格被忽略# case 1\"Hello World.\"  =&gt; [Hello][World][.]# case 2# World与.之间的空格被作为分隔符忽略掉\"Hello World .\"  =&gt; [Hello][World][.]SentencePiece将输入序列视为unicode字符序列，将空格显式化为符号“_”（U+2581）此外SentencePiece编码后的结果也是以“_”开头  如果训练的vocab中有以”_“开头的token，则编码为该token  否则编码为[\"_\",token]# vocab中有token为'_Hello'(或者'_He'等，跟训练数据有关)\"Hello World.\" =&gt; \"Hello_World.\" =&gt; [_Hello][_Wor][ld][.]# 否则\"Hello World.\" =&gt; \"Hello_World.\" =&gt; [_][Hello][_Wor][ld][.]由于分词后空格被保留，因此逆映射回字符串时不会丢失文献【1】Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units[J]. arXiv preprint arXiv:1508.07909, 2015.【2】Kudo T. Subword regularization: Improving neural network translation models with multiple subword candidates[J]. arXiv preprint arXiv:1804.10959, 2018."
  },
  
  {
    "title": "【从0训练LLM】（1）模型参数量、数据量与算力FLOPS",
    "url": "/posts/%E4%BB%8E0%E8%AE%AD%E7%BB%83LLM-1-%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E9%87%8F-%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8E%E7%AE%97%E5%8A%9BFLOPS/",
    "categories": "LLM, 从0训练LLM",
    "tags": "从0训练LLM",
    "date": "2023-08-30 00:00:00 +0000",
    





    
    "snippet": "确定模型参数量、数据量与算力参考DeepMind在2022年发表的文章《Training Compute-Optimal Large Language Models》【1】该文章研究在给定算力预算（FLOPs）的前提下，理想的模型参数量和用于训练的Tokens数量为此文章提出了3种方法对400余个模型（参数量从70M到16B），使用不同的Tokens数（5B到400B）做实验  固定模型参数...",
    "content": "确定模型参数量、数据量与算力参考DeepMind在2022年发表的文章《Training Compute-Optimal Large Language Models》【1】该文章研究在给定算力预算（FLOPs）的前提下，理想的模型参数量和用于训练的Tokens数量为此文章提出了3种方法对400余个模型（参数量从70M到16B），使用不同的Tokens数（5B到400B）做实验  固定模型参数量，研究最佳的Tokens数和所需的计算量FLOPs  固定计算量FLOPs，研究最佳的模型参数量和Tokens  基于前两种方法做参数拟合图1. (a)方法1；(b)方法2；(c)FLOPs-Prams；(d)FLOPs-Tokens图1(a)是方法1的结果，展示了不同模型参数量（颜色区分）训练误差达到理想值时的计算量FLOPs图1(b)是方法2的结果，展示了不同计算量FLOPs（颜色区分）训练误差达到最低时的最佳模型参数量图1(c)和(d)展示了计算量FLOPs与模型参数量和Tokens数量的统计关系文章通过上述3种方法均得出了比较一致的结论：计算量FLOPs（以100为尺度单位）与模型参数量和训练所需Tokens数（以10为尺度单位）均成正比图2. 3种方法得到的最优组合图2是文章得出的三者最优组合方案，在应用中可以参考其中的任何一种基于图2的统计观察，文章提出了三者组合的计算公式\\[C=6DN \\tag{1}\\]其中$C$是计算量FLOPs，$D$是训练所需的Tokens数，$N$是模型参数量  比如对于1B的模型，按式1有6*(1e+9)*(20.2e+9)=1.212e+20，与图1对应的FLOPs吻合此外根据图1(c)(d)和图2，本文进一步推得$C$与$N$、$D$与$N$的估算公式\\[\\begin{aligned}C&amp;=(1.21*10^{20})*10^{2log_{10}^N}\\\\D&amp;=20.2*N\\end{aligned} \\tag{2}\\]式2中$N$和$D$的单位都是Billion  比如现在要训练一个7B的模型，根据式2，可知模型的理想Tokens为141.4B，需要的FLOPs为5.93e+21模型参数量与结构的对应文章给出了模型参数量与模型结构参数的对应建议图3. 不同规模的模型配置参数其中  d_model是自注意力模块输出的维度  ffw_size是接在自注意力后面的FFW模块中间维度，按惯例取4*d_model  n_head是自注意力模块的head数  kv_size是自注意力模块每个head分支的向量维度，按d_model/n_heads求得  n_layers是Transformer层数在应用中可参考d_model与n_head和n_layers的关系，模型参数与字典大小有关Tokens数量与磁盘占用的关系100个token大约对应75个word1个word（中、英文）在不同编码下占1-4Byte，一般用2.5Byte估算，可得式3\\[y=1.875x \\tag{3}\\]其中输入$x$是token数量，输出$y$是占用存储空间（单位为Byte）预估训练用时1）确定所需算力在确定了模型参数量的情况下  如果已知Tokens数则根据式1计算  否则根据式2计算理想情况下的算力需求2）确定硬件FLOPS            型号      CUDA核心数      Tensor核心数      显存      显存带宽      显存位宽      FP64      TF32                  V100      5120      640      32GB      900GB/s      4096-bit      8.2 TFLOPS      16.4 TFLOPS              A800      6912      432      80GB      2TB/s      5120-bit      19.5 TFLOPS      156 TFLOPS              A100      6912      432      80GB      2TB/s      5120-bit      19.5 TFLOPS      156 TFLOPS              H100      16896      528      80GB      3TB/s      5120-bit      67 TFLOPS      989 TFLOPS        A100、H100对中国禁售  A800是Nvidia为了应对美国对中国的出口管制，对A100做阉割的版本（高速互联总线带宽从600GB/s降低到400GB/s，影响多卡并行的通信能力，其余计算能力保持不变）  CUDA核心是通用计算单元，Tensor核心是专为矩阵运算设计的计算单元（可以简单理解为1个Tensor核心的FLOPS比1个CUDA核心高）\\[T=\\frac{FLOPs}{FLOPS*n*3600*24} \\tag{4}\\]式中  $FLOPs$是第1步确定的算力需求  $FLOPS$是硬件每秒浮点计算次数，$n$是总共参与训练的硬件台数  $T$的单位是天FLOPS和FLOPsFLOPS（floating point operations per second）表示每秒钟能够完成的浮点运算次数，是衡量硬件处理速度的指标FLOPs（floating point operations）表示浮点运算数（即计算量），是衡量模型/算法复杂度的指标FLOPSFLOPS有以下几种常用的单位            FLOPS      MFLOPS      GFLOPS      TFLOPS      PFLOPS                  每秒次      每秒百万($10^6$)次      每秒十亿($10^9$)次      每秒万亿($10^{12}$)次      每秒千万亿($10^{15}$)次      \\[FLOPS=n*F*P \\tag{5}\\]其中  $F$是每个核心每个时钟周期能完成的浮点运算数量，单位是FLOP  $P$是每个核心的时钟速度（主频），即每秒多少个时钟，单位是Hz  $n$是核心数量比如一颗4核心的CPU，主频为2.0GHz，每个时钟周期可执行4次浮点运算，那么其FLOPS为32GFLOPSFLOPsFLOPs的计算依算法/模型的不同而不同卷积的FLOPs\\[FLOPs^{conv}=(2C_iK^2-1)HWC_o \\tag{6}\\]其中  $C_i$和$C_o$分别为输入channels和输出channels  $K$是卷积核尺寸  $H$和$W$分别是输出特征图的高和宽考虑卷积的过程：1）对每个channel，卷积核的每个元素分别与输入元素相乘做累加，然后对所有channel做累加\\[2C_iK^2-1 =&gt; (C_iK^2)+(C_iK^2-1)\\]上式左边部分是所有的乘法次数，右边是所有的加法次数（未考虑bias操作）2）如果输出有$C_o$个channel，上述过程重复$C_o$次3）以上过程最终输出的尺寸是$1\\times 1\\times C_o$，对于输出特征图尺寸$HW$，所有上述计算重复$H\\times W$次全连接的FLOPs\\[FLOPs^{fc}=(2I-1)O \\tag{7}\\]其中  $I$是输入的神经元数目  $O$是输出的神经元数目同样对式3做拆分\\[(2I-1)O =&gt; IO+(I-1)O\\]左边部分是乘法次数，右边部分是加法次数统计工具对于PyTorch的CNN模型，有以下几种FLOPs统计工具1）thopfrom torchvision.models import resnet18from thop import profilemodel = resnet18()input = torch.randn(1, 3, 224, 224) flops, params = profile(model, inputs=(input, ))2）ptflopsimport torchvision.models as modelsimport torchfrom ptflops import get_model_complexity_info with torch.cuda.device(0):  net = models.resnet18()  flops, params = get_model_complexity_info(net, (3, 224, 224), as_strings=True, print_per_layer_stat=True) batch_size=1 \t\t            3）torchstatfrom torchstat import statimport torchvision.models as modelsmodel = models.resnet18()stat(model, (3, 224, 224))文献【1】Hoffmann J, Borgeaud S, Mensch A, et al. Training compute-optimal large language models[J]. arXiv preprint arXiv:2203.15556, 2022."
  },
  
  {
    "title": "HugeGraph安装、部署、数据导入",
    "url": "/posts/HugeGraph%E5%AE%89%E8%A3%85-%E9%83%A8%E7%BD%B2-%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5/",
    "categories": "知识图谱",
    "tags": "",
    "date": "2023-08-29 00:00:00 +0000",
    





    
    "snippet": "HugeGraph是百度开源的图数据库系统，支持百亿以上的顶点和边，并提供毫秒级的关系查询能力开源数据库系统对比            考察点      Neo4j      JanusGraph      HugeGraph      NebulaGraph                  图查询语言      Cypher      Gremlin      Gremlin、Cyphe...",
    "content": "HugeGraph是百度开源的图数据库系统，支持百亿以上的顶点和边，并提供毫秒级的关系查询能力开源数据库系统对比            考察点      Neo4j      JanusGraph      HugeGraph      NebulaGraph                  图查询语言      Cypher      Gremlin      Gremlin、Cypher      nGQL              支持数据规模      十亿级（社区版）      百亿级      百亿级      千亿级              图存储类型      本地存储      支持分布式存储      支持分布式存储      支持分布式存储              查询速度      快      -      较快      最快              API支持      HTTP API      HTTP API/WebSocket      HTTP RESTful API、Gremlin API      -              客户端支持      Java、C#、JS      Java、Python、C#、JS      Java      C++、Java、Python、Golang      其中NebulaGraph是腾讯、网易使用的解决方案，社区活跃度还不错，值得后续关注HugeGraph的组成及安装            模块      说明                  HugeGraph-Server      是HugerGraph的核心，包含Core、Backend、API等子模块              HugeGraph-Client      基于RESTful API的客户端，用于连接HugeGraph-Server，目前仅实现Java版本              HugeGraph-Loader      数据导入工具，将普通文本转化为图形顶点和边并插入图形数据库中              HugeGraph-Computer      分布式图处理系统，可以运行在Kubernetes上              HugeGraph-Hubble      HugerGraph的Web可视化管理平台，涵盖了数据建模、数据导入、数据分析、图的管理              HugeGraph-Tools      HugeGraph的部署和管理工具，包括图管理、备份/恢复、Gremlin执行等功能        当前我们安装其中的HugeGraph-Server、HugeGraph-Loader、HugeGraph-Hubble和HugeGraph-Tools，均为1.0.0版本  HugeGraph官方提供了从源码编译、二进制文件安装和Docker镜像三种方式，其中Docker镜像仅包含了HugeGraph-Server和HugeGraph-Hubble模块安装官方提供了HugeGraph-Server和HugeGraph-Hubble两个镜像，前者提供HugeGraph主程序运行环境，后者提供对多个HugeGraph的Web可视化统一管理平台为了方便这里将HugeGraph-Loader和HugeGraph-Tools的预编译二进制文件也安装到HugeGraph-Server镜像内主程序运行环境安装这里使用HugeGraph-Server的Docker镜像，在里面安装HugeGraph-Loader和HugeGraph-Tools的二进制文件# 镜像拉取docker pull hugegraph/hugegraph# 启动容器docker run -itd\\           --name=graph\\        # 容器命名为graph           -p 18080:8080\\       # 宿主机端口:容器端口           hugegraph/hugegraph  容器启动后会自动执行bin/init-store.sh和bin/start-hugegraph.sh  默认配置使用RocksDB作为数据库  默认端口为8080在这里下载1.0.0版本的二进制编译组件toolchain图1. 下载组件toolchain将其中的HugeGraph-Loader和HugeGraph-Tools也复制进镜像（无需编译）Web可视化管理平台安装Docker镜像# 镜像拉取docker pull hugegraph/hubble# 启动容器docker run -itd --name=fe -p 8088:8088 hugegraph/hubble容器启动后会自动运行hubble服务图2. Web可视化管理平台创建图时  图ID：可随意设置  图名称：需要与HugeGraph-Server中创建的图名称一致（默认为hugegraph）  主机名：需要设置为HugeGraph-Server的ip  端口号：需要设置为HugeGraph-Server的port（默认8080）模块配置# HugeGraph-Server配置文件├── conf│   ├── computer.yaml│   ├── graphs│   │   └── hugegraph.properties      # 字段backend设置图数据存储方案│   ├── gremlin-driver-settings.yaml│   ├── gremlin-server.yaml│   ├── log4j2.xml│   ├── remote-objects.yaml│   ├── remote.yaml│   └── rest-server.properties        # 字段restserver.url设置服务ip:port                                      # ip设置为0.0.0.0可确保外部机器能够访问# HugeGraph-Hubble配置文件├── conf│   └── hugegraph-hubble.properties   # 字段hubble.host/port设置服务ip/port                                      # ip设置为0.0.0.0可确保外部机器能够访问      图数据存储方案    配置参数    说明        Memory    backend=memory    数据存储在内存，无法持久化，不需要初始化后端        serializer=text        RocksDB    backend=rocksdb    嵌入式数据库，需要初始化后端（bin/init-store.sh）        serializer=binary        Cassandra    backend=cassandra    高性能分布式数据库，需要初始化后端（bin/init-store.sh）        serializer=cassandra        ScyllaDB    backend=scylladb    cassandra的升级版，需要初始化后端（bin/init-store.sh）        serializer=scylladb        HBase    backend=hbase    高性能分布式数据库，需要初始化后端（bin/init-store.sh）        serializer=hbase        MySQL    backend=mysql    关系型数据库，需要初始化后端（bin/init-store.sh）        serializer=mysql  HugeGraph的数据导入HugeGraph-Loader是HugeGraph的数据导入组件，可以将多种数据源的数据转化为图的顶点和边，并批量导入到图数据库中工具以输入源映射文件和schema文件为输入，导入到指定的图数据库sh bin/hugegraph-loader.sh\\        -g hugegraph\\                    # 被导入的图名称        -f example/file/struct.json\\     # 输入源映射文件        -s example/file/schema.groovy    # schema文件输入源映射文件是一个json文件，指定了顶点文件和边文件的路径、属性等信息参考官方example，顶点和边文件支持csv、txt、json多种格式{    \"vertices\": [    {      \"label\": \"person\",      \"input\": {        \"type\": \"file\",        \"path\": \"example/file/vertex_person.csv\",        \"format\": \"CSV\",        \"header\": [\"name\", \"age\", \"city\"],        \"charset\": \"UTF-8\",        \"skipped_line\": {          \"regex\": \"(^#|^//).*\"        }      },      \"null_values\": [\"NULL\", \"null\", \"\"]    },    ...    ],    \"edges\": [    {      \"label\": \"knows\",      \"source\": [\"source_name\"],      \"target\": [\"target_name\"],      \"input\": {        \"type\": \"file\",        \"path\": \"example/file/edge_knows.json\",        \"format\": \"JSON\",        \"date_format\": \"yyyyMMdd\"      },      \"field_mapping\": {        \"source_name\": \"name\",        \"target_name\": \"name\"      }    },    ...    ]}参考官方文档，源映射文件包含以下配置项      配置项    二级配置项    是否必填    适用类型    说明        label    是    vertex、edge    待导入的顶点/边的label        skip    否，默认false    vertex、edge    是否跳过该输入源（path对应的文件）        field_mapping    否    vertex、edge    将输入源列的列名映射为顶点/边的属性名        value_mapping    否    vertex、edge    将输入源的数据值映射为顶点/边的属性值        selected    否    vertex、edge    选择某些列插入，其它未选中的不插入，不能与ignored同时存在        ignored    否    vertex、edge    忽略某些列，不能与selected同时存在        null_values    否    vertex、edge    指定一些字符串代表空值，如\"NULL\"        update_strategies    否    vertex、edge    如果需要按特定方式批量更新时，可对每个属性指定具体的更新策略（SUM、GIGGER、SMALLER、UNION、INTERSECTION、APPEND、ELIMIMATE、OVERRIDE）        unfold    否    vertex、edge    是否将id列展开        id    否    vertex    指定某一列为顶点的id列，当顶点id策略为CUSTOMIZE时必填，当id策略为PRIMARY_KEY时必须为空        source    是    edge    选择输入源某几列为源顶点的id列，当源顶点的id策略为CUSTOMIZE时必须指定某一列为顶点的id列；当id策略为PRIMARY_KEY时，必须指定一列或多列用于拼接生成顶点的id        target    是    edge    选择输入源某几列为目标顶点的id列，与source类似        unfold_source    否    edge    是否展开文件的source列，效果与顶点映射中的类似        unfold_target    否    edge    是否展开文件的target列，效果与顶点映射中的类似        input    type    是    vertex、edge    输入源类型，只能填\"file\"或\"FILE\"        path    是    vertex、edge    本地文件或目录的绝对路径        format    是    vertex、edge    本地文件的格式，可选值为\"CSV\",\"TEXT\",\"JSON\"        header    否    vertex、edge    文件各列的列名，如不指定则以文件第一行为header，JSON文件不需要指定        delimiter    否    vertex、edge    文件每行内容的分隔符，默认为\",\"，JSON不需要指定        charset    否    vertex、edge    文件的编码字符集，默认为UTF-8        date_format    否    vertex、edge    自定义的日期格式，默认为yyyy-MM-dd HH:mm:ss，只有当包含日期属性且非JSON才需要指定        time_zone    否    vertex、edge    日期数据的时区，默认为GMT+8        skipped_line    否    vertex、edge    需要跳过的行，目前只能配置正则表达式，一般用于跳过注释行        compression    否    vertex、edge    文件的压缩格式，可选值为NONE、GZIP、BZ2、XZ、LZMA、SNAPPY_RAW、SNAPPY_FRAMED、Z、DEFLATE、LZ4_BLOCK、LZ4_FRAMED、ORC 和 PARQUET，默认为 NONE，表示非压缩文件        list_format    否    vertex、edge    当文件（非JSON）的某列是复合结构时（set或list），用以设置该列的起始符、分隔符、结束符  顶点文件每行记录一个顶点实例的属性如example/file/vertex_person.csv记录了label为person的所有顶点的属性信息# This is a commentmarko,29,Beijingvadas,27,Hongkongjosh,32,Beijingpeter,35,Shanghai\"li,nary\",26,\"Wu,han\"tom,null,NULL文件每一行有3个值，分别对应struct.json中label为person顶点的header的3个名称边文件每行记录一条边实例的信息如example/file/edge_knows.json记录了label为knows的所有边的属性信息{\"source_name\": \"marko\", \"target_name\": \"vadas\", \"date\": \"20160110\", \"weight\": 0.5}{\"source_name\": \"marko\", \"target_name\": \"josh\", \"date\": \"20130220\", \"weight\": 1.0}  其中source_name和target_name与struct.json中的设置保持一致，weight为该边的权重  source_name和target_name对应的value必须为顶点设置的主键值（如果是自定义id策略则为id值，如果是PrimaryKey策略则为对顶点属性的值）schema文件定义图的schema信息，包括PropertyKey、VertexLabel、EdgeLabel 和 IndexLabel# example/file/schema.groovyschema.propertyKey(\"name\").asText().ifNotExist().create();schema.propertyKey(\"age\").asInt().ifNotExist().create();schema.propertyKey(\"city\").asText().ifNotExist().create();schema.propertyKey(\"weight\").asDouble().ifNotExist().create();schema.propertyKey(\"lang\").asText().ifNotExist().create();schema.propertyKey(\"date\").asText().ifNotExist().create();schema.propertyKey(\"price\").asDouble().ifNotExist().create();schema.vertexLabel(\"person\").properties(\"name\", \"age\", \"city\").primaryKeys(\"name\").ifNotExist().create();schema.vertexLabel(\"software\").properties(\"name\", \"lang\", \"price\").primaryKeys(\"name\").ifNotExist().create();schema.indexLabel(\"personByAge\").onV(\"person\").by(\"age\").range().ifNotExist().create();schema.indexLabel(\"personByCity\").onV(\"person\").by(\"city\").secondary().ifNotExist().create();schema.indexLabel(\"personByAgeAndCity\").onV(\"person\").by(\"age\", \"city\").secondary().ifNotExist().create();schema.indexLabel(\"softwareByPrice\").onV(\"software\").by(\"price\").range().ifNotExist().create();schema.edgeLabel(\"knows\").sourceLabel(\"person\").targetLabel(\"person\").properties(\"date\", \"weight\").ifNotExist().create();schema.edgeLabel(\"created\").sourceLabel(\"person\").targetLabel(\"software\").properties(\"date\", \"weight\").ifNotExist().create();schema.indexLabel(\"createdByDate\").onE(\"created\").by(\"date\").secondary().ifNotExist().create();schema.indexLabel(\"createdByWeight\").onE(\"created\").by(\"weight\").range().ifNotExist().create();schema.indexLabel(\"knowsByWeight\").onE(\"knows\").by(\"weight\").range().ifNotExist().create();PropertyKey详细介绍参考文档属性值的信息，包括  name：属性类型名称  data_type：属性类型的数据类型（bool、byte、int、long、float、double、string、date、uuid、blob），默认string  cardinality：属性类型基数（single、list、set），默认single属性由顶点和边所包含，比如对于上述label为person的顶点，其包含了3种名称为\"name\", \"age\", \"city\"的属性，数据类型分别为string、int和string# 创建属性类型name，指定其数据类型为stringschema.propertyKey(\"name\").asText().ifNotExist().create();VertexLabel详细介绍参考文档顶点的信息，需要指定顶点的label、所有属性、主键等信息  id：顶点类型id值  name：顶点类型名称（必填）  id_strategy：顶点类型的ID策略（主键ID、自动生成、自定义字符串、自定义数字、自定义UUID），默认主键ID  properties：顶点类型关联的属性类型（列表）  primary_keys：主键属性（列表），当ID策略为主键ID时必须有值，其他ID策略必须为空  enable_label_index：是否开启类型索引（默认关闭）  index_names：顶点类型创建的索引  nullable_keys：可为空的属性  user_data：设置顶点类型的通用信息，作用与属性类型一致顶点一般指实体，比如上述person就是一个顶点# 创建顶点类型person，指定其包含的属性类型、主键、可为空的属性schema.vertexLabel(\"person\")      .properties(\"name\", \"age\", \"city\")      .primaryKeys(\"name\")      .nullableKeys(\"age\", \"city\")      .ifNotExist()      .create();EdgeLabel详细介绍参考文档边的信息，需要指定边的label、连接的两个顶点的类型  name：边类型名称（必填）  source_label：起始顶点类型的名称（必填）  target_label：目标顶点类型的名称（必填）  frequency：两个顶点之间是否可以有多条边（SINGLE、MULTIPLE），默认为SINGLE  properties：边类型管理的属性类型  sort_keys：当允许关联多次时，指定区分键属性列表  nullable_keys：可为空的属性，默认为空  enable_label_index：是否开启类型索引（默认关闭）# 创建边类型created，指定起始顶点类型为person，目标顶点类型为software（person创造software）# 指定边的属性包含date和weightschema.edgeLabel(\"created\")      .sourceLabel(\"person\")      .targetLabel(\"software\")      .properties(\"date\", \"weight\")      .ifNotExist()      .create();IndexLabel详细介绍参考文档索引信息，通过指定的属性（可为多个）索引指定的顶点或边索引种类有3种  secondary()：按具体的属性值查询顶点或边，对应的属性可为任意类型，比如名字是“Peter”的person  range()：按属性值的范围查询顶点或边，对应的属性必须为int类型，比如age大于5并小于20的person  search()：按属性值的部分或全部内容查询，对应的属性必须为string类型，比如city包含“友谊路”的person# 创建索引类型personByAge，指定其通过age属性索引person顶点schema.indexLabel(\"personByAge\")      .onV(\"person\")      .by(\"age\")      .range()      .ifNotExist()      .create();数据导入时常见的问题顶点的id策略参考官方文档，hugegraph内部是通过顶点的id进行检索的，其提供了3种id生成策略  Automatic：默认方法，使用Snowflake算法自动生成  Customize：用户传入字符串或数字类型的id  PrimaryKey：根据主属性的值拼接生成id，如果在schema语句中调用了primaryKey方法，同时又没有显式设置idStrategy，则自动使用该策略// 使用 Automatic 的 Id 策略schema.vertexLabel(\"person\").properties(\"name\", \"age\").ifNotExist().create();schema.vertexLabel(\"person\").useAutomaticId().properties(\"name\", \"age\").ifNotExist().create();// 使用 Customize_String 的 Id 策略schema.vertexLabel(\"person\").useCustomizeStringId().properties(\"name\", \"age\").ifNotExist().create();// 使用 Customize_Number 的 Id 策略schema.vertexLabel(\"person\").useCustomizeNumberId().properties(\"name\", \"age\").ifNotExist().create();// 使用 PrimaryKey 的 Id 策略schema.vertexLabel(\"person\").properties(\"name\", \"age\").primaryKeys(\"name\").ifNotExist().create();schema.vertexLabel(\"person\").usePrimaryKeyId().properties(\"name\", \"age\").primaryKeys(\"name\").ifNotExist().create();主键的选择  主键primaryKeys是针对顶点的概念，从该类顶点的属性列表中选择  可以设置多个主键  所有主键的组合应能唯一确定某个顶点  要确保选择的主键内容不会为空  当id策略为PrimaryKey时，hugegraph会根据主键值生成顶点id，但hugegraph内部对顶点id的长度是有限制的（128），此时要确保主键值的长度不超过限制，否则只能适用自定义id策略注意：id策略直接决定了主键，比如使用自定义id策略时主键就是id，此时不能再设置primarykeys# 导入时id策略与主键的冲突报错Failed to load, cause: Not allowed to assign primary keys when using 'CUSTOMIZE_NUMBER' id strategyschema文件过大的问题使用官方工具批量导入数据的主要参数为sh bin/hugegraph-loader.sh -g hugegraph -f example/file/struct.json -s example/file/schema.groovy当schema.groovy中的数据量过大时，加载程序会报错Failed to load, cause: startup failed:General error during class generation: Method too large: Script1.run ()Ljava/lang/Object;此时可以将schema.groovy做切分，多次加载（每次加载必须同时输入struct.json）sh bin/hugegraph-loader.sh -g hugegraph -f example/file/struct.json -s example/file/schema-part1.groovysh bin/hugegraph-loader.sh -g hugegraph -f example/file/struct.json -s example/file/schema-part2.groovy多个图的配置问题参考官方文档1） 添加图描述文件在conf/graphs目录下仿照默认的hugraph.properties创建新的图文件主要修改数据库配置、图名称backend=rocksdbserializer=binary# 如果不指定则使用默认位置 /hugegraph/rocksdb-data# 而默认的图hugegraph已经使用了该位置# 因此新的图必须指定其他的路径，否则无法创建成功rocksdb.data_path=/path/to/diskrocksdb.wal_path=/path/to/diskstore=新图名称2）重启server# 停止serverbash /hugegraph/bin/stop-hugegraph.sh# 初始化数据库bash /hugegraph/bin/init-store.sh# 开启serverbash /hugegraph/bin/start-hugegraph.sh其他机器无法访问服务修改文件/hugegraph/conf/rest-server.properties# ip设为0.0.0.0restserver.url=http://0.0.0.0:8080权限管理问题根据官方文档，hugegraph提供了两种权限模式  ConfigAuthenticator模式：通过本地配置文件存储用户名和密码 (仅支持单 GraphServer)  StandardAuthenticator模式：支持多用户认证、以及细粒度的权限访问控制，采用基于“用户 - 用户组 - 操作 - 资源”的 4 层设计，灵活控制用户角色与权限 (支持多 GraphServer)官方github的issues中推荐第2种模式，但按官方的配置方法，以上两种模式都会报错此外由于权限管理使用的是Http Basic的方式，本身不是很安全，因此更建议自己搭建权限管理服务"
  },
  
  {
    "title": "ChatGPT的训练方法：InstructGPT（实践篇）",
    "url": "/posts/ChatGPT%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95-InstructGPT-%E5%AE%9E%E8%B7%B5%E7%AF%87/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-08-25 00:00:00 +0000",
    





    
    "snippet": "huggingface推出了一个专门的库TRL（Transformer Reinforcement Learning）用于实现上一节的内容同时在一篇博文中公开了训练教程《“StackLLaMA”: 用 RLHF 训练 LLaMA 的手把手教程》图1. TRL对图1所示的3个步骤做了封装三类TrainerSFTTrainerSFTTrainer实现对预训练模型的有监督微调，同时支持全量微调和P...",
    "content": "huggingface推出了一个专门的库TRL（Transformer Reinforcement Learning）用于实现上一节的内容同时在一篇博文中公开了训练教程《“StackLLaMA”: 用 RLHF 训练 LLaMA 的手把手教程》图1. TRL对图1所示的3个步骤做了封装三类TrainerSFTTrainerSFTTrainer实现对预训练模型的有监督微调，同时支持全量微调和PEFT微调from datasets import load_datasetfrom trl import SFTTrainerdataset = load_dataset(\"imdb\", split=\"train\")trainer = SFTTrainer(    \"facebook/opt-350m\",    train_dataset=dataset,    dataset_text_field=\"text\",    max_seq_length=512,)trainer.train()  SFTTrainer只接受生成式CausalLM模型为基座模型  SFTTrainer内部会将max_seq_length限制在1024以内  SFTTrainer同时支持from_pretrained方法返回的model  SFTTrainer可以同时指定tokenizer否则会在内部根据model关联tokenizermodel = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")trainer = SFTTrainer(    model,    train_dataset=dataset,    dataset_text_field=\"text\",    max_seq_length=512,)训练数据的格式化SFTTrainer通过formatting_fun参数实现自定义的数据格式化同时通过data_collator参数实现输入数据的maskfrom transformers import AutoModelForCausalLMfrom datasets import load_datasetfrom trl import SFTTrainerdataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")# 自定义的数据格式化函数def formatting_prompts_func(example):    output_texts = []    for i in range(len(example['instruction'])):        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"        output_texts.append(text)    return output_textstrainer = SFTTrainer(    model,    train_dataset=dataset,    formatting_func=formatting_prompts_func,)trainer.train() 训练数据的mask训练过程中我们根据模型生成的内容与label的差异计算loss，因此需要将instruction的内容设置maskTRL提供了函数DataCollatorForCompletionOnlyLM实现上述功能，在具体实现上涉及了两种情况1）仅指定response_templatebatch中的每1条只允许1个sample，此时SFTTrainer的packing参数需要设为False示例：\"### Question: xxxx\\n ### Answer: xxxx[PAD][PAD][PAD]...\"函数将\"### Question: xxxx\\n ### Answer: \"对应的label全部标记为ignore_index，不参与误差计算2）同时指定instruction_template和response_templatebatch中的每1条允许多个sample，此时SFTTrainer的packing参数可以设为True示例：\"### Question: xxxx\\n ### Answer: xxxx\\n ### Question: xxxx\\n ### Answer: xxxx\\n...\"函数将所有\"### Question: xxxx\\n ### Answer: \"位置对应的label全部标记为ignore_index，不参与误差计算from trl import DataCollatorForCompletionOnlyLMfrom transformers import AutoModelForCausalLM, AutoTokenizertokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")## 1. 仅指定response_template的情况response_template = \" ### Answer:\"collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)## 2. 同时指定instruction_template和response_template的情况instruction_template = \"### Question:\"response_template = \"### Answer:\"collator = DataCollatorForCompletionOnlyLM(    instruction_template=instruction_template, response_template=response_template,     tokenizer=tokenizer,     mlm=False)...trainer = SFTTrainer(    model,    train_dataset=dataset,    formatting_func=formatting_prompts_func,    data_collator=collator,)当SFTTrainer的packing=False，且未指定data_collator时，会自动配置DataCollatorForLanguageModeling该方法也是对label做mask，按mlm（完型填空式）参数的设置有2种方式  mlm=False: 将输入token中的pad_token_id对应的label设为ignore_index(-100)  mlm=True: 将输入token中的spacial_tokens对应的label设为ignore_index(-100)；输入token的80%被mask，10%做随机替换，剩下的10%保持不变class SFTTrainer(Trainer):    def __init__(...):        ...        if not packing:            ...            if data_collator is None:                data_collator = DataCollatorForLanguageModeling(                    tokenizer=tokenizer, mlm=False)PEFT训练SFTTrainer支持PEFT训练from peft import LoraConfigpeft_config = LoraConfig(    r=16,    lora_alpha=32,    lora_dropout=0.05,    bias=\"none\",    task_type=\"CAUSAL_LM\",)trainer = SFTTrainer(    \"facebook/opt-350m\",    train_dataset=dataset,    peft_config=peft_config)混合精度训练PEFT训练时，可以分别设置基座模型和adapter模块的精度# adapter模块以float16的精度，基座模型以8bitmodel = AutoModelForCausalLM.from_pretrained(    \"facebook/opt-350m\",    load_in_8bit=True,    device_map=\"auto\",)trainer = SFTTrainer(    model,    train_dataset=dataset,    torch_dtype=torch.bfloat16, # 指定adapter模块的精度    peft_config=peft_config,)自定义callback函数SFTTrainer支持添加自定义的callback函数，在训练的不同阶段自动调用# callback函数需要继承`TrainerCallback`class PeftSavingCallback(TrainerCallback):    def on_save(self, args, state, control, **kwargs):        checkpoint_path = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")        kwargs[\"model\"].save_pretrained(checkpoint_path)        if \"pytorch_model.bin\" in os.listdir(checkpoint_path):            os.remove(os.path.join(checkpoint_path, \"pytorch_model.bin\"))callbacks = [PeftSavingCallback()]trainer = SFTTrainer(    \"facebook/opt-350m\",    train_dataset=dataset,    peft_config=peft_config,    callbacks=callbacks)当SFTTrainer被设置为PEFT，且callback为空时，上述函数PeftSavingCallback会被自动设置class SFTTrainer(Trainer):    def __init__(...):        ...        if is_peft_available() and peft_config is not None:            ...            if callbacks is None:                callbacks = [PeftSavingCallback]TrainerCallback定义了训练过程中的不同event，在每个event会执行callbacks列表中的所有类实例，因此可以通过重载某个event的函数来控制自定义callback执行的时机（如PeftSavingCallback只在on_save阶段执行）class CallbackHandler(TrainerCallback):    def call_event(self, event, args, state, control, **kwargs):        # 每个event都会遍历所有callbacks        # 当该callback种定义了当前event为名的函数时执行        for callback in self.callbacks:            result = getattr(callback, event)(...)TrainerCallback定义的event有：  on_init_end：SFTTrainer初始化结束后执行  on_train_begin：训练开始时执行  on_train_end：训练结束时执行  on_epoch_begin：一个epoch开始时执行  on_epoch_end：一个epoch结束时执行  on_step_begin：一个训练step开始时执行  on_substep_end：在梯度累加过程中的一个substep结束时执行  on_step_end：一个训练step结束时执行  on_evaluate：评估结束后执行  on_predict：预测结束后执行  on_save：一个ckpt保存后执行  on_log：记录最后一个log后执行  on_prediction_step：一个预测step结束后执行RewardTrainerRewardTrainer实现奖励模型的微调  RewardTrainer将该模块简化为一个二分类问题，因此仅接受序列分类模型SequenceClassification为基座模型  目前仅支持PEFT训练方式from peft import LoraConfig, TaskTypefrom transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArgumentsfrom trl import RewardTrainer# 仅支持SEQ_CLS类型的基座模型# 仅支持PEFT训练方式model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\")peft_config = LoraConfig(    task_type=TaskType.SEQ_CLS,    inference_mode=False,    r=8,    lora_alpha=32,    lora_dropout=0.1,)...trainer = RewardTrainer(    model=model,    args=training_args,    tokenizer=tokenizer,    train_dataset=dataset,    peft_config=peft_config,)trainer.train()训练数据RewardTrainer是在一对sample中选择更优的一个，因此要求输入模型的数据包含2条语句# 一条训练样本包含2条语句，要求模型从中选择更合适的{    \"chosen\": \"\\n\\nHuman: What are some cuss words in english? Assistant: Here’s an incomplete list. Ass, dick, bugger, crap, fuck, shit, bitch, Human: What's your favorite one? Assistant: I haven't even thought about it.\",    \"rejected\": \"What are some cuss words in english? Assistant: Here’s an incomplete list. Ass, dick, bugger, crap, fuck, shit, bitch, Human: What's your favorite one? Assistant: Ass.\"}RewardTrainer内部默认使用RewardDataCollatorWithPadding对数据做格式化class RewardTrainer(Trainer):    def __init__(...):        ...        if data_collator is None:            ...            data_collator = RewardDataCollatorWithPadding(                tokenizer, max_length=max_length)该方法要求dataset的每个item包含4个key  input_ids_chosen：训练数据中标记为选中的语句tokenizer后的ids  attention_mask_chosen：训练数据中标记为选中的语句的mask  input_ids_rejected：训练数据中未被标记为选中的语句tokenizer后的ids  attention_mask_rejected：训练数据中未被标记为选中的语句的maskRewardDataCollatorWithPadding内部会将其分组做pad后返回class RewardDataCollatorWithPadding:    ...    features_chosen.append(        {            \"input_ids\": feature[\"input_ids_chosen\"],            \"attention_mask\": feature[\"attention_mask_chosen\"],        }    )    features_rejected.append(        {            \"input_ids\": feature[\"input_ids_rejected\"],            \"attention_mask\": feature[\"attention_mask_rejected\"],        }    )    batch_chosen = self.tokenizer.pad(            features_chosen,            padding=self.padding,            max_length=self.max_length,            pad_to_multiple_of=self.pad_to_multiple_of,            return_tensors=self.return_tensors,    )    batch_rejected = self.tokenizer.pad(            features_rejected,            padding=self.padding,            max_length=self.max_length,            pad_to_multiple_of=self.pad_to_multiple_of,            return_tensors=self.return_tensors,    )    batch = {            \"input_ids_chosen\": batch_chosen[\"input_ids\"],            \"attention_mask_chosen\": batch_chosen[\"attention_mask\"],            \"input_ids_rejected\": batch_rejected[\"input_ids\"],            \"attention_mask_rejected\": batch_rejected[\"attention_mask\"],            \"return_loss\": True,    }    return batch误差计算在误差计算阶段，使用分类模型分别计算accept语句和rejected语句的概率（得分），期望两者的差距越大越好class RewardTrainer(Trainer):    def compute_loss(...):        ...        # 对应RewardDataCollatorWithPadding中的batch输出        rewards_chosen = model(            input_ids=inputs[\"input_ids_chosen\"],            attention_mask=inputs[\"attention_mask_chosen\"],        )[0]        rewards_rejected = model(            input_ids=inputs[\"input_ids_rejected\"],            attention_mask=inputs[\"attention_mask_rejected\"],        )[0]        # 因为rewards_chosen - rewards_rejected总是小于1的        # 因此两者之差越大，loss越趋近0        loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()        if return_outputs:            return loss, {                \"rewards_chosen\": rewards_chosen,                \"rewards_rejected\": rewards_rejected,            }        return lossPPOTrainerPPOTrainer对SFTTrainer训练的模型做强化学习微调PPOTrainer实现了《ChatGPT的训练方法：InstructGPT（理论篇）》式2的前两个部分，即\\[obj(\\phi)=E_{(x,y)\\sim D_{\\pi_{\\phi}^{RL}}}\\left[r_{\\theta}(x,y)-\\beta log\\frac{\\pi_{\\phi}^{RL}(y|x)}{\\pi^{SFT}(y|x)}\\right] \\tag{1}\\]其中的$\\pi^{SFT}$对应PPOTrainer构造函数参数ref_model、$\\pi_{\\phi}^{RL}$对应model以下是关于PPOTrainer使用的官方示例，其中step3-step6就是不断循环训练的步骤# 0. importsimport torchfrom transformers import GPT2Tokenizerfrom trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer# 1. load a pretrained modelmodel = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")model_ref = AutoModelForCausalLMWithValueHead.from_pretrained(\"gpt2\")tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")tokenizer.pad_token = tokenizer.eos_token# 2. initialize trainerppo_config = {\"batch_size\": 1}config = PPOConfig(**ppo_config)ppo_trainer = PPOTrainer(config, model, model_ref, tokenizer)# 3. encode a queryquery_txt = \"This morning I went to the \"query_tensor = tokenizer.encode(query_txt, return_tensors=\"pt\").to(model.pretrained_model.device)# 4. generate model responsegeneration_kwargs = {    \"min_length\": -1,    \"top_k\": 0.0,    \"top_p\": 1.0,    \"do_sample\": True,    \"pad_token_id\": tokenizer.eos_token_id,    \"max_new_tokens\": 20,}response_tensor = ppo_trainer.generate([item for item in query_tensor], return_prompt=False, **generation_kwargs)response_txt = tokenizer.decode(response_tensor[0])# 5. define a reward for response# (this could be any reward such as human feedback or output from another model)reward = [torch.tensor(1.0, device=model.pretrained_model.device)]# 6. train model with ppotrain_stats = ppo_trainer.step([query_tensor[0]], [response_tensor[0]], reward)PPOTrainer中的model和ref_model都需要为指定的模型类型SUPPORTED_ARCHITECTURESPPOTrainer中的model可以为PEFT模型，此时内部会将该模型禁用掉adapter后作为ref_model（式1中的$\\pi^{SFT}$）class PPOTrainer(BaseTrainer):    def __init__(...):        ...        self.is_peft_model = getattr(self.model, \"is_peft_model\", False)        ...        if isinstance(ref_model, SUPPORTED_ARCHITECTURES):            # 如果ref_model指定了，在这里先赋值            # 但如果model为peft，这里的ref_model不会被使用            self.ref_model = ref_model            ...        elif ref_model is None and not self.is_peft_model:            # 将model深度拷贝一份作为ref_model（作为参考模型其权重默认不会改变了）            # num_shared_layers指定了哪些层会随着model权重的改变而改变            self.ref_model = create_reference_model(                self.model, num_shared_layers=num_shared_layers)        elif self.is_peft_model:            # 如果model为peft，并且ref_model没有指定，则置为None            self.ref_model = None    def step(...):        ...        if self.is_peft_model and hasattr(            self.accelerator.unwrap_model(self.model).pretrained_model,            \"disable_adapter\",            ):            # model为peft时，将adapeter禁用掉作为ref_model（式1中的sft模型）            with self.accelerator.unwrap_model(self.model).pretrained_model.disable_adapter():                ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(                    self.model, queries, responses, model_inputs, return_logits=full_kl_penalty                )        elif self.is_peft_model and not hasattr(self.model.pretrained_model, \"disable_adapter\"):            raise ValueError(                    \"You are using a `peft` version that does not support `disable_adapter`. Please update your `peft` version to the latest version.\"            )        else:            ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(                self.ref_model, queries, responses, model_inputs, return_logits=full_kl_penalty            )PPOTrainer支持的模型类型PPOTrainer目前仅支持CausalLM和Seq2SeqLM两种类型，并且需要在这两种模型输出层加入ValueHead# 添加了ValueHead的两种模型类型SUPPORTED_ARCHITECTURES = (    AutoModelForCausalLMWithValueHead,    AutoModelForSeq2SeqLMWithValueHead,)ValueHead就是通过一个线性层将模型最后一层的hidden_states映射成维度为1的标量class ValueHead(nn.Module):    def __init__(...):        ...        self.summary = nn.Linear(hidden_size, 1)        def forward(self, hidden_states):        output = self.dropout(hidden_states)        ...        output = self.summary(output)        return output以下是CausalLM和CausalLMWithValueHead输出头的对比class AutoModelForCausalLMWithValueHead(PreTrainedModelWrapper):    def __init__(...):        ...        self.v_head = ValueHead(self.pretrained_model.config, **v_head_kwargs)        def forward(...):        ...        # 输出一个针对y|x的量化得分        last_hidden_state = base_model_output.hidden_states[-1]        value = self.v_head(last_hidden_state).squeeze(-1)        ...class LlamaForCausalLM(LlamaPreTrainedModel):    def __init__(...):        ...        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)    def forward(...):        ...        # 输出字典中每一个token的概率        logits = self.lm_head(hidden_states)可以看到CausalLMWithValueHead就是式1中$\\pi(y|x)$的实现，其  输入是x+y（即prompt + response）组成的语句  输出是模型对x+y的打分（合适的response组合得分高）  在infer时可以直接使用AutoModelForCausalLM加载，此时会忽略ValueHeadclass PPOTrainer(BaseTrainer):    def prepare_model_inputs(self, queries: torch.Tensor, responses: torch.Tensor):       ...       # 将prompt和response组合为一条语句作为模型输入       input_ids = [torch.cat([q, r]) for q, r in zip(queries, responses)]       ... 官方训练案例huggingface给出了几个使用TRL的示例1）情感分类任务的微调以经过训练的BERT为奖励模型，以GPT-2为PPO训练的model和ref_model2）基于LoRA的微调huggingface社区公开了一个通过TRL在Llama7b上做微调的全流程教程该案例在SFTTrainer、RewardTrainer、PPOTrainer三个阶段都是基于Llama7b预训练模型，通过LoRA做微调实现3）基于PPO的语言模型去毒以预训练的Roberta为毒性检测奖励模型，在6B的GPT模型上做全量微调文档中详细介绍了，值得进一步研究  训练时内存压缩的技巧  奖励模型设计的技巧（增加中性类）  输入tokens数量（10-15，超过20模型的毒性会增加）4）一个基座模型完成全部强化学习这是huggingface一个实验性的常识，没有对结果做测试其思路是在同一个基座模型中训练3种不同的adapter（PEFT内部可以通过adapter的名称区别实现多种下游任务的支持），作为SFTTrainer、RewardTrainer、PPOTrainer但除了节约内存，暂时没理解这种方式的其他意义"
  },
  
  {
    "title": "ChatGPT的训练方法：InstructGPT（理论篇）",
    "url": "/posts/ChatGPT%E7%9A%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95-InstructGPT-%E7%90%86%E8%AE%BA%E7%AF%87/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-08-22 00:00:00 +0000",
    





    
    "snippet": "InstructGPT【1】是ChatGPT训练用到的技术（以GPT3.5为基座模型），其提出的背景是由于大模型的预训练采用的自监督目标函数本质上是让模型的输出与预训练文本分布趋于一致，因此模型的表现完全依赖于训练数据，其输出是不受控制的然而在实际应用中我们希望模型能遵循用户指令，以有用和安全的方式回复，这里面暗含了两个要求：  能理解用户的指令（比如从给定的选项中选择一个最佳的、续写一个故...",
    "content": "InstructGPT【1】是ChatGPT训练用到的技术（以GPT3.5为基座模型），其提出的背景是由于大模型的预训练采用的自监督目标函数本质上是让模型的输出与预训练文本分布趋于一致，因此模型的表现完全依赖于训练数据，其输出是不受控制的然而在实际应用中我们希望模型能遵循用户指令，以有用和安全的方式回复，这里面暗含了两个要求：  能理解用户的指令（比如从给定的选项中选择一个最佳的、续写一个故事、给出建议等等）  回答能解决用户的问题（有用的），并且是安全的（无毒的、无害的、无偏见的）为了弥补预训练和实际需求的鸿沟，很显然需要对模型做进一步的精调，具体来说InstructGPT提出了一种基于人类反馈强化学习的监督微调方法2023年7月facebook提出的Llama2【2】也沿用了与之相似的方法微调Llama Chat模型算法分析图1. InstructGPT微调方法如图1，InstructGPT将方法分为3个步骤1）SFT微调建立人工标注的数据集$D^{SFT}$，对预训练模型（InstructGPT使用的基座模型为GPT-3）做SFT为了下文描述方便，将经过SFT微调的模型记为$\\pi^{SFT}$2）训练奖励模型建立人工标注的排序数据集$D^{RM}$，基于$\\pi^{SFT}$训练奖励模型$\\pi^{RM}$这里奖励模型需要将$\\pi^{SFT}$最后一层的分类层换成回归层，因为$\\pi^{SFT}$输出的是所有tokens的概率，而奖励模型需要输出一个标量的分值3）强化学习微调模型建立不带标注的数据集$D^{RL}$（注意该数据集不需要人工标注，是由$\\pi^{RM}$对$\\pi^{SFT}$的响应打分）强化学习模型$\\pi^{RL}$通过$\\pi^{SFT}$完成初始化以奖励模型$\\pi^{RM}$为监督，使用PPO目标函数做强化反馈学习，不断更新$\\pi^{RL}$  理论上第一个步骤就可以完成模型的训练，但需要人工模型每次返回的结果做评价，这几乎是不现实的，因此才考虑训练一个奖励模型代替人类做评价数据集上述步骤涉及到3个数据集，InstructGPT的使用情况为            数据集      训练集数量      来源                  $D^{SFT}$      13k      OpenAI API收集的用户prompts、标注团队（占多数，10倍于前者）              $D^{RM}$      33k      OpenAI API收集的用户prompts（占多数，5倍于后者）、标注团队              $D^{RL}$      31k      OpenAI API收集的用户prompts      图2是各数据集的详细数量情况图2. 各数据集详细数量情况图3是对用户prompt的统计和示例图3. 左：用户prompt类型的统计；右：用户prompt的示例可以看到，用户请求中生成类、开放问答类和头脑风暴类的问题占多数模型训练SFT微调模型对预训练的GPT-3在$D^{SFT}$上做微调一共训练了16个epoch，其实经过1个epoch后模型已经过拟合了，但这种过拟合对接下来的任务是有利的最终将所有16个SFT的模型在验证集上测试，保留奖励值最高的那个作为$\\pi^{SFT}$奖励模型使用6B规模的$\\pi^{SFT}$作为奖励模型的基座，因为发现参数量大的模型训练起来不稳定在数据准备阶段，对每个prompt，收集$K$个$\\pi^{SFT}$的输出（Llama2【2】中提到可以调整模型的temperature、使用RL训练前面几轮的模型输出来增加样本多样性）如图1，人工标注的数据集$D^{RM}$会对这$K$个输出做排序，越靠前的输出越接近人类偏好在训练时，每次会从中选择2个输出（一共有$C_K^2$种选择）计算损失\\[loss(\\theta)=-\\frac{1}{C_K^2}E_{(x,y_m,y_l)\\sim D}[log(\\sigma(r_{\\theta}(x,y_w)-r_{\\theta}(x,y_l)))] \\tag{1}\\]式中  $x$是prompt  $y_l$和$y_m$是选择的2个输出，其中$y_m$比$y_l$排序靠前  $r_{\\theta}(x,y)$是奖励模型$\\pi^{RM}$对输入$x$和$y$输出的分值，$\\theta$代表奖励模型的可训练参数  $\\sigma$是sigmoid函数  $K$推荐取[4,9]随着式1取值趋向0，$r_{\\theta}(x,y_w)$与$r_{\\theta}(x,y_l)$的差距被不断拉大在训练中，将$K$种$(x,y)$组合放到一个batch内，这样模型只需要前向$K$次后向1次PPO强化学习首先将$\\pi^{RL}$初始化为$\\pi^{SFT}$然后使用不带标签的数据$D^{RL}$，通过奖励模型$\\pi^{RM}$的监督迭代更新\\[obj(\\phi)=E_{(x,y)\\sim D_{\\pi_{\\phi}^{RL}}}\\left[r_{\\theta}(x,y)-\\beta log\\frac{\\pi_{\\phi}^{RL}(y|x)}{\\pi^{SFT}(y|x)}\\right]+\\gamma E_{x\\sim D_{pretrain}}[log(\\pi_{\\phi}^{RL}(x))] \\tag{2}\\]其中  $D_{\\pi_{\\phi}^{RL}}$表示使用上一轮更新的$\\pi^{RL}$对$D^{RL}$做输出得到的$(x,y)$数据集  $D_{pretrain}$表示预训练阶段的数据集  $\\pi^{SFT}$是sft阶段生成的模型，在本阶段不再更新参数  $\\pi_{\\theta}^{RL}$是本阶段的模型，每轮迭代更新参数式2可以分为3个部分看1）损失目标即$r_{\\theta}(x,y)$部分，奖励模型对$\\pi^{RL}$的响应打分，分数越高表示结果越好2）KL散度惩罚即$\\beta log\\frac{\\pi_{\\phi}^{RL}(y\\lvert x)}{\\pi^{SFT}(y\\lvert x)}$部分，其中$\\beta$是系数，后者是KL散度KL散度反映了两个分布的差异性，取值为0时两个分布一致\\[KL(p||q)=-\\int p(x)ln\\frac{q(x)}{p(x)}dx \\tag{3}\\]式3为KL散度的一般公式，表示从分布$p$（被比较的分布）到分布$q$的散度KL散度有2个重要的性质  $KL(p\\lvert\\lvert q)\\ne KL(q\\lvert\\lvert p)$  $KL(p\\lvert\\lvert q)\\ge 0$，当且仅当$p=q$时$KL(p\\lvert\\lvert q)=0$式2加入该部分的目的是：希望最终的$\\pi^{RL}$不要偏离$\\pi^{SFT}$太多关于本部分的符号，个人的理解是：由于$\\pi_{\\phi}^{RL}$是以$\\pi^{SFT}$为基础更新的，因此非退化情况对输入的响应应该不差于后者，即$log\\frac{\\pi_{\\phi}^{RL}(y\\lvert x)}{\\pi^{SFT}(y\\lvert x)}\\ge 0$3）过拟合惩罚在强化学习过程中每轮迭代的数据$D_{\\pi_{\\phi}^{RL}}$是会变化的（强化学习与监督学习最大的不同）这会带来模型过拟合的风险（数据和模型的“双向奔赴”），表现在这里就是模型在公共NLP任务的性能退化因此式3同时让预训练中的数据参与$\\pi_{\\phi}^{RL}$的训练，将使用了该部分（即$\\gamma &gt;0$）的模型称为PPO-ptx模型文献【1】Ouyang L, Wu J, Jiang X, et al. Training language models to follow instructions with human feedback[J]. Advances in Neural Information Processing Systems, 2022, 35: 27730-27744.【2】Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023."
  },
  
  {
    "title": "【PLM】（5）Llama1、2、3及Chat模型训练方法",
    "url": "/posts/PLM-5-Llama1-2-3%E5%8F%8AChat%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2023-08-16 00:00:00 +0000",
    





    
    "snippet": "Llama系列是facebook在2023年发布的大模型  2月份发布了Llama1系列，没有公开模型文件  7月份发布了Llama2系列，共3种参数类别，包含基模型和chat微调模型  2024年4月发布了Llama3系列，共2种参数类别，包含基模型和指令微调模型其中Llama2和Llama3对公众和商业开源，由此衍生出了大量以Llama为基座的模型      版本    上下文长    ...",
    "content": "Llama系列是facebook在2023年发布的大模型  2月份发布了Llama1系列，没有公开模型文件  7月份发布了Llama2系列，共3种参数类别，包含基模型和chat微调模型  2024年4月发布了Llama3系列，共2种参数类别，包含基模型和指令微调模型其中Llama2和Llama3对公众和商业开源，由此衍生出了大量以Llama为基座的模型      版本    上下文长    词汇表    参数量    dimension    heads/layers    预训练tokens    训练时间（万GPU小时）    训练设备        Llama 1    2k    32k    7B    4096    32/32    1.0T    8.243    A100-80GB        13B    5120    40/40    1.0T    13.517        33B    6656    52/60    1.4T    53.043        65B    8192    64/80    1.4T    102.236        Llama 2    4k    32k    7B    4096    32/32    2.0T    18.432        13B    5120    40/40    2.0T    36.864        70B    8192    64/80    2.0T    172.032        Llama 3    8k    128k    8B    4096    32/32    15T    130    H100-80GB        70B    8192    64/80    15T    640  Llama 1Llama 1【1】的主要观点是：数据规模比模型参数规模更重要  Llama全部使用公开数据集训练，证明了不使用私有数据集也能获得SOTA的模型  Llama在相对小的模型上增加训练数据，获得了更好的效果：Llama-13B比GPT-3(175B)性能更好，但参数量却少了10倍预训练数据如图1右所示，Llama 1使用了7种来源的公开数据，经过清洗后总的token数为1.4T（Trillion，万亿）作为对比，GPT-3使用了5种来源的数据，总的token数为500B（Billion，十亿）图1. 左：GPT-3预训练数据组成；右：Llama 1预训练数据组成其中除了Wikipedia和Books数据，其他数据集在训练过程中仅使用一次1. CommonCrawlCommonCrawl收集了自2008年来的网页数据，数据量为PB级别，包含  原始网页数据  提取后的元数据  提取后的文本数据其所有数据保存在Amazon S3上可以免费获取该数据集在GPT-3的预训练数据中占了60%，Llama使用facebook提出的CCNet【2】方法对其做清洗2. C4C4（The colossal, cleaned version of Common Crawl’s）数据集【3】是以CommonCrawl2019年4月的snapshot为基础创建的图2. C4文件组成C4主要为英文文档，应用了filter的版本叫C4.EN，没应用的叫C4.NOCLEAN，没使用blocklist的叫C4.NOBLOCKLIST3. GithubLlama使用Google BigQuery项目中的Github公开数据集，并对其做清洗  基于行长度、字母与数字的比例做过滤  基于正则表达式过滤与格式相关的内容（如文件头）  文本去重图3. BigQuery公开数据集包含的表单此外需要补充的是BigQuery公开数据集还包括以下与医学相关的数据集  Cloud Life Sciences公开数据集  NIH胸部X射线图像数据集  癌症影像归档（TICA）数据集4. Wikipediawikimedia提供了wikipedia的大部分归档数据供公众免费下载Llama选用了其中2022.6-8月的数据，包含了20种语言5. BooksLlama使用了两个来源的书籍数据，并对其做了去重  古登堡计划  ThePile数据集【4】的Books3部分其中古登堡计划开始于1971年，致力于尽可能多的，以自由的和电子化的形式，提供版权过期的书籍其中ThePile包含800G文本数据，是专门为语言模型的训练和评估创建的6. ArXivLlama抓取了ArXiv Latex论文数据，采用与文献【5】一样的方法对数据做清洗  删除第一节之前的所有数据（包括目录等）  删除.tex文件中的注释数据  删除用户编写的内联扩展的定义和宏，以增加论文之间的一致性7. StackExchangeStackExchange是一个知名的问答网站，类似国内的知乎图4. StackExchange问答数排名Top3的子站Llama从中选取了Top28的子站，对其中的数据做清洗这里保存了StackExchange的数据DumpTokenizer与GPT一样，Llama也使用SentencePiece【6】实现的BPE分词算法【7】需要注意的是Llama将  所有数字都分解为单个数字的形式  未知的UTF-8字符分解为byte模型结构Llama受其他模型的启发，对Transformer层做了3点改进1）Pre-normalization受GPT-3的启发，将归一化操作放到transformer层的输入（原始的方式是放在做完self-attention后），以此提高训练的稳定性使用RMSNorm归一化方法【8】代替LayerNormRMSNorm通过实验证明再中心化的操作对精度是没有贡献的，因此去掉了LayerNorm减均值的操作，降低训练和推理过程中的计算量\\[RMSNorm(a_i)=\\frac{a_i}{RMS(a)}g_i \\tag{1}\\]其中$RMS(a)=\\sqrt{\\frac{1}{n}\\sum_{i=1}^na_i^2}$，$g_i$是增益参数，一般设为12）SwiGLU激活函数受PaLM的启发，使用Google在2020年提出的SwiGLU激活函数【9】代替ReLU3）RoPE位置编码受GPTNeo的启发，使用苏剑林2021年提出的旋转位置编码RoPE【10】代替绝对位置编码RoPE的设计理念是：用绝对位置编码的方式实现相对位置编码\\[f(\\vec{q},m)=R_m\\vec{q} \\tag{2}\\]\\[\\begin{aligned}R_m&amp;=\\left[\\begin{matrix}cos(m\\theta_0)&amp;-sin(m\\theta_0)&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\sin(m\\theta_0)&amp;cos(m\\theta_0)&amp;0&amp;0&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;cos(m\\theta_1)&amp;-sin(m\\theta_1)&amp;\\cdots&amp;0&amp;0\\\\0&amp;0&amp;sin(m\\theta_1)&amp;cos(m\\theta_1)&amp;\\cdots&amp;0&amp;0\\\\\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots&amp;\\vdots\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;cos(m\\theta_{k/2-1})&amp;-sin(m\\theta_{k/2-1})\\\\0&amp;0&amp;0&amp;0&amp;\\cdots&amp;sin(m\\theta_{k/2-1})&amp;cos(m\\theta_{k/2-1})\\end{matrix}\\right]\\\\\\vec{q}&amp;=\\left[\\begin{matrix}q_0&amp;q_1&amp;q_2&amp;q_3&amp;\\cdots&amp;q_{k-2}&amp;q_{k-1}\\end{matrix}\\right]^T\\end{aligned}\\]其中  $\\vec{q}$是被加入位置编码的长度为$k$的向量，在Transformer中一般为query和key  $R_m$表示位置为$m$的左乘矩阵（每个$2\\times 2$的子矩阵是旋转矩阵的形式，因而得名），注意$\\theta_i$的$i$只取到$\\vec{q}$长度的一半  $\\theta_i=10000^{-2i/k}$，与正弦位置编码一致（作者尝试过将$\\theta_i$作为可训练参数，但发现其并没有显著更新，因此索性固定不变）训练超参图5. Llama训练误差曲线batch size取4M tokens使用AdamW优化器，取$\\beta_1=0.9$，$\\beta_2=0.95$使用cosine学习率曲线（Llama7b和Llama13b取3e-4，Llama33b和Llama65b取1.5e-4），最终的学习率是最大值的10%，warmup阶段的step为2000Llama 2Llama 2文章【11】是以技术报告的形式发表的，其认为目前开源LLM和闭源LLM最大的不同是：闭源LLM经过了充分的微调，使其能够与人类偏好对齐，极大提升了模型的可用性和安全性因此Llama 2的主要工作是研究微调方案：SFT、RLHF、Safety方面的pretraining与fine-tuning模型预训练从本文开头关于Llama 1和Llama 2的对比可以看到，Llama 2在模型结构上没有大的变化预训练阶段的差异有  预训练数据增加了40%（但没有公布新增的数据）  模型输入的上下文长度是之前的2倍  对于34B和70B的模型，使用了grouped-query attention(GQA)结构【12】  各个参数规模下同时训练了Llama-Chat版本目前官方仅开放了7B、13B、70B参数的模型预训练采用的训练超参数和tokenizer与Llama 1一致GQA图6. 左：MHA；中：GQA；右：MQA如图6左所示，传统Transformer使用的多头自注意力包括多头query、多头keys和多头values为了降低模型内存需求，加快推理效率，图6右所示的MQA将多头keys和多头values缩减为单头，但这样改造的模型训练稳定性下降，模型质量很难保证GQA是MHA和MQA的折中，其将多头query分组，每组的多个query共享一个key和value，经过【12】的实验，该方法能很好的兼顾训练稳定性和推理效率的需求基于现有的MHA模型，可以通过以下2步转换为GQA/MQA模型：  对MHA模型的权重做处理，每个多头key和value通过mean pool映射到单头  使用与MHA相同的预训练数据，对GQA/MQA模型做额外的预训练，通过少量的step使模型适应性的结构图7. 左：mean pool映射；右：mean/first/random的消融实验图7左使用mean pool将多头key映射到单头key图7右在pool过程中对比了使用均值、第一个、随机选择1个的效果，发现使用mean更好模型微调这里的模型微调目标是将预训练好的基础模型训练成Chat模型图8. Chat模型微调框架如图8所示，Chat模型首先在经过预训练的基座模型上做进一步的SFT得到，然后通过RLHF微调使模型输出与人类偏好一致在RLHF的每个迭代中都通过人工标注的数据训练一个奖励模型，然后以奖励模型为监督，使用PPO和本文提出的拒绝采样算法对Chat模型做优化SFT阶段  首先使用【13】提出的数据集做指令微调  然后标注了27540个高质量的SFT数据继续做指令微调（发现SFT数据质量足够高时几万个数据就可以获得很好的效果）在SFT阶段使用的参数为            lr      weight dacay      batch size      sequence length      epoch                  2e-5      0.1      64      4096      2      RLHF阶段人类反馈强化学习（Reinforcement Learning with Human Feedback，RLHF）应用于一个微调的语言模型，在指令执行上进一步对齐模型行为与人类的偏好具体来说，RLHF首先通过人工标注训练一个奖励模型，之后使用这个模型模拟人类对Chat模型做微调1）人类偏好数据收集  首先人工写一个prompt，然后从Chat模型输出中随机采样两个response，为了提高response的区分度，使用不同版本的Chat模型和不同temperature参数做输出  人工对采样的两个response做2选1，选择的依据基于helpfulness和safety：前者考量response是否能有效解决用户的请求，后者考量response是否安全（反人类、歧视等）  虽然是做2选1，但实际中两个response的区分度是有差别的，因此Llama同时还要求人工标注本次选择的置信度（significantly better/better/slightly/unsure）  Llama对safety很重视，如果上述两个response都是不安全的，则该条数据不会用于训练  每一轮新的Chat模型训练前，要用上一轮的Chat模型更新偏好数据，否则奖励模型会因为数据分布差别太大而导致精度下降文章提出了自己的偏好数据集（称为Meta reward modeing data），与其他公开数据集比较  会话轮次更多  语句更长图9. 偏好数据集对比2）奖励模型奖励模型以prompt（包含会话前几轮的上下文）和Chat模型的response为输入，输出一个反映response质量的得分，然后根据该得分对Chat模型做优化  为了更好平衡helpfulness和safety的效果，Llama对两者分别训练了1个奖励模型  为了避免信息不对称导致的模型幻觉，奖励模型是基于预训练Chat模型的checkpoint做训练得到  奖励模型的模型结构是将预训练模型的分类头（输出下一个token的概率）换成回归头（输出分值）\\[L_{ranking}=-log(\\sigma(r_{\\theta}(x,y_c)-r_{\\theta}(x,y_r)-m(r))) \\tag{3}\\]式3为Llama奖励模型的损失函数，其中  $r_{\\theta}$是奖励模型，prompt和Chat模型的response分别作为奖励模型输入的$x$和$y_c$或$y_r$  $y_c$是人工标注数据中2选1选择的答案，$y_r$是另一个没有被选择的答案  $m(r)$加入了人工标注数据每个答案置信度的信息（图10）图10. 置信度的惩罚分值如图10，Llama设置了两组置信度的惩罚分值，分别称为margin small和margin large，总体上看，人工对本次2选1越有信心（significantly better），惩罚分值越高再来推演式3，假设奖励模型$r_{\\theta}$还未能很好判断$y_c$和$y_r$谁更好，意味着$r_{\\theta}(x,y_c)-r_{\\theta}(x,y_r)$的值偏小，如果此时人工标注中对$y_c$信心很足，那么$m(r)$偏大，则$-log(\\sigma(z))$的输入$z$很小，该函数有一个偏大的输出，意味着此时损失很大奖励模型的训练数据为  helpfulness奖励模型：Meta Helpfulness data + (Meta Safety data + Open-source data)，其中第1个的数据量与后2者的数据量之比为1:1  safety奖励模型：(Meta Safety data + Anthropic Harmless data) + (Meta Helpfulness data + Open-source Helpfulness data)，其中前2项的数据量与后2项的数据量之比为9:1奖励模型的训练超参为  对70B的模型使用lr=5e-6，其余的模型使用lr=1e-5  学习率lr的变化使用cosine曲线，最终降为最大lr的10%  warm-up阶段的step数为总step的3%3）迭代微调在每轮迭代中通过奖励模型对Chat模型做微调，Llama在这里同时采用了PPO【14】和自己提出的拒绝采样微调（Rejection Sampling fine-tuning）与PPO相比，拒绝采样微调主要有2点不同  后者对于一个prompt会采样K个Chat模型输出，选择奖励模型评分最高的那个输出进行微调；而前者只会采样1个  后者同时会使用之前多个迭代轮次的Chat模型生成输出；而前者只会使用上一轮的Chat模型第1点不同提高了找到更具区分度的训练样本的可能性，理论上K越大效果越好第2点不同避免了单一模型来源造成的有偏性（比如上一轮模型如果有某个方向的短板，会造成所有训练样本都有这种短板）在实践中，Llama在前4轮迭代只使用拒绝采样微调，在后面的迭代中还同时使用了PPO多轮会话的主题一致性在多轮对话中很多时候我们会添加一些指示（Instruction），对Chat模型的输出做约束（比如要求模型扮演某种角色，或者要求模型的回复简约一些等等）但实际中模型很容易在多轮对话中遗忘该指示（图11）图11. 要求模型总是以表情符号回复，左：2轮会话后模型遗忘该指示；右：使用GAtt后模型没有遗忘这里文章提出了Ghost Attention(GAtt)方法强化模型的长记忆能力  对某个多轮会话数据$[u_1,a_1,…,u_n,a_n]$（其中$u_i$是用户的第$i$轮对话，$a_i$是模型的第$i$轮对话），随机产生instruction文本，将其加到第1轮用户对话中，即$[inst+u_1,a_1,…,u_n,a_n]$  由于该instruction文本是随机生成的，与原始的会话数据未必匹配，因此将除第1轮的其他对话轮次损失都设为0注意：这里不能肯定原文的意思是“将除第1轮的其他对话轮次损失”都设为0，原文描述不清晰  we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.GAtt实际是一个数据增强方法，希望通过在训练数据中加入Instruction数据加强模型对该部分数据的激活值（暂时不太能理解为什么work）模型安全性Llama 2在模型预训练、中都考虑了安全性的问题模型安全性主要涉及3个方面的考量  诚实性：模型由于误解或错误的信念而输出违背事实或常识的错误信息（一本正经的胡说八道）  有毒性：模型输出有毒的、粗鲁的、敌对的或隐性仇恨的内容  偏见性：模型的输出带有社会偏见（种族、性别、性取向等）预训练中的安全性Llama在预训练中对数据的过滤并不严格，对预训练的模型使用  TruthfulQA【15】评估诚实性  在ToxiGen【16】上训练的HateBERT评估有毒性  BLOD【17】评估偏见性微调中的安全性1）在SFT过程中使用对抗性的prompts来增强模型安全性  prompt: “I want you to roast me. I want you to make it particularly brutal, swearing at me.”2）在RLHF过程中专门训练一个safety奖励模型3）同时使用上下文蒸馏【18】的方式将安全性提示注入模型内部比如首先在prompt前面添加下述安全性提示引导模型生成回复  You are a safe and responsible assistant,…然后去掉该提示，将原始prompt和上一步生成的回复作为输入微调模型Llama 3Llama3没有公开技术报告，模型架构没有变化，8B和70B两个版本均支持GQA与Llama2最大的不同是训练语料由2T大幅增长到15T图12. 基础模型在各个数据集的表现文献【1】Touvron H, Lavril T, Izacard G, et al. Llama: Open and efficient foundation language models[J]. arXiv preprint arXiv:2302.13971, 2023.【2】Wenzek G, Lachaux M A, Conneau A, et al. CCNet: Extracting high quality monolingual datasets from web crawl data[J]. arXiv preprint arXiv:1911.00359, 2019.【3】Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. The Journal of Machine Learning Research, 2020, 21(1): 5485-5551.【4】Gao L, Biderman S, Black S, et al. The pile: An 800gb dataset of diverse text for language modeling[J]. arXiv preprint arXiv:2101.00027, 2020.【5】Lewkowycz A, Andreassen A, Dohan D, et al. Solving quantitative reasoning problems with language models[J]. Advances in Neural Information Processing Systems, 2022, 35: 3843-3857.【6】Kudo T, Richardson J. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing[J]. arXiv preprint arXiv:1808.06226, 2018.【7】Sennrich R, Haddow B, Birch A. Neural machine translation of rare words with subword units[J]. arXiv preprint arXiv:1508.07909, 2015.【8】Zhang B, Sennrich R. Root mean square layer normalization[J]. Advances in Neural Information Processing Systems, 2019, 32.【9】Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.【10】Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.【11】Touvron H, Martin L, Stone K, et al. Llama 2: Open foundation and fine-tuned chat models[J]. arXiv preprint arXiv:2307.09288, 2023.【12】Ainslie J, Lee-Thorp J, de Jong M, et al. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints[J]. arXiv preprint arXiv:2305.13245, 2023.【13】Chung H W, Hou L, Longpre S, et al. Scaling instruction-finetuned language models[J]. arXiv preprint arXiv:2210.11416, 2022.【14】Schulman J, Wolski F, Dhariwal P, et al. Proximal policy optimization algorithms[J]. arXiv preprint arXiv:1707.06347, 2017.【15】Lin S, Hilton J, Evans O. Truthfulqa: Measuring how models mimic human falsehoods[J]. arXiv preprint arXiv:2109.07958, 2021.【16】Hartvigsen T, Gabriel S, Palangi H, et al. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection[J]. arXiv preprint arXiv:2203.09509, 2022.【17】Dhamala J, Sun T, Kumar V, et al. Bold: Dataset and metrics for measuring biases in open-ended language generation[C]//Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. 2021: 862-872.【18】Askell A, Bai Y, Chen A, et al. A general language assistant as a laboratory for alignment[J]. arXiv preprint arXiv:2112.00861, 2021."
  },
  
  {
    "title": "【SFT】（3）Adapters系列算法（LoRA、AdaLoRA）",
    "url": "/posts/SFT-3-Adapters%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95-LoRA-AdaLoRA/",
    "categories": "LLM, SFT",
    "tags": "SFT",
    "date": "2023-07-24 00:00:00 +0000",
    





    
    "snippet": "Adapters类的方法在模型内部增加额外的结构（称为adapter）,在做SFT训练时冻结模型原有的参数，仅训练adapter与Soft Prompts相比，这种方法会改变模型结构，引入新的少量参数，但不占用输入长度，从实验上看模型效果更好按提出时间的先后关系有以下代表方法            时间      方法名称      adapter结构      与transformer层位...",
    "content": "Adapters类的方法在模型内部增加额外的结构（称为adapter）,在做SFT训练时冻结模型原有的参数，仅训练adapter与Soft Prompts相比，这种方法会改变模型结构，引入新的少量参数，但不占用输入长度，从实验上看模型效果更好按提出时间的先后关系有以下代表方法            时间      方法名称      adapter结构      与transformer层位置关系                  2019      Adapter【1】      bottleneck结构      每个transformer中串接2个adapter模块              2021      LoRA【2】      bottleneck结构      每个transformer中并联adapter模块，秩固定              2023      AdaLoRA【3】      bottleneck结构      每个transformer中并联adapter模块，秩可以自学习      AdapterAdapter将自己的方法定位成一种迁移学习策略，可以产生紧凑和可扩展的下游模型  紧凑：针对每个任务仅需少量的额外参数  可扩展：可通过逐步训练来解决新的任务，而不会忘记以前的任务Adapter认为NLP中的迁移学习可以分为2种1.基于特征在embedding向量层面做迁移，这些向量可以是词语级、句子级和段落级的2.基于fine-tuning针对预训练模型的权重做下游任务迁移有研究表明基于fine-tuning的方法比基于特征的方法好，而adapter方法在获得与fine-tuning同样精度时，需要的训练参数少了2个数量级图1. 左：每个transformer层串接2个adapter；右：adapter是bottleneck结构如图1所示，在每个transformer层中有2个部分，一个是attention+mlp，一个是双层的mlp，方法将两个adapter模块分别串接在这2个部分后面每个adapter是一个bottleneck结构  adapter由一个降维全连接和升维全连接组成，中间通过激活函数实现非线性映射  adapter权重初始化为接近单位矩阵，确保训练开始时模型表现与原始模型相似通过设定降维目的维度就可以控制新增的参数量，该方法可以控制在原始模型参数量的0.5%-8%LoRA从形式上看，LoRA用了与Adapter一样的bottleneck结构，但LoRA提出了该设计的理论依据：  模型是过参数化的，它们有更小的内在维度，模型主要依赖这个低的内在维度去做任务适配因此LoRA通过bottleneck结构实现低秩分解，试图用这个结构实现任务适配图2. LoRA通过AB模块实现原始权重的低秩分解如图2所示  A模块用于低维映射，B模块用于高维映射，最终的输出与原始模块的输出相加（类似ResNet的残差连接）  首先对A做随机高斯初始化，对B使用0做初始化，确保训练开始时BA=0LoRA认为该方法的优势是  针对不同的下游任务，只需要更换AB矩阵就可以  训练阶段成本低  推理阶段计算量增加不多  与之前的SFT方法互不干扰，可以很方便组合超参数选择在PEFT中，LoRA有3个比较重要的超参数  r：低维映射的维度，即A模块输出的维度  alpha：$\\frac{alpha}{r}$为尺度因子，将其与B模块的输出相乘后再与模型原始模块相加  target_modules：原始模型中每层transformer结构中待替换的模块（用于计算query、key、value的$W_q$、$W_k$、$W_v$，以及多头attention的$W_o$）LoRA在GPT-3上做消融实验1）首先对比不同的target_modules组合图3. 关于target_modules的消融实验实验认为$W_q$+$W_v$的方式比较理想2）然后对比不同的r图4. 关于r的消融实验实验认为提高r没有增加target_modules的组合有效，推荐取r=4、8、12代码实现PEFT实现了Linear和Embedding层的LoRA改造基本思路是：  继承torch.nn.Linear和torch.nn.Embedding作为一条支路  同时建立模块A和B作为另一条支路，输入同时经过两条支路，最后对两者输出相加# 尺度因子self.scaling[adapter_name] = lora_alpha / r# 第一条支路result = F.linear(x, transpose(self.weight, self.fan_in_fan_out), bias=self.bias)# 第二条支路x = x.to(self.lora_A[self.active_adapter].weight.dtype)# 第二条支路做尺度变换后两者相加result += (    self.lora_B[self.active_adapter](    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))    )     * self.scaling[self.active_adapter] )此外PEFT还实现了另一种思路：将第二条支路的权重参数与第一条支路的权重参数相加，这样在推理时就可以使用原始模型# self.weight是原始模型中目标模块的权重，假设输入维度为m，输出为n，则weight尺寸为nxmself.weight.data += (    transpose(        # A模块权重的尺寸为rxm,B模块的尺寸为nxr        # 则BA的尺寸为nxm，与weight尺寸一致        self.lora_B[self.active_adapter].weight @ self.lora_A[self.active_adapter].weight,        self.fan_in_fan_out,    )    * self.scaling[self.active_adapter])AdaLoRA如图2所示，LoRA在adapter结构中首先使用A模块做低维映射，然后使用B模块恢复原始维度，其中秩$r$就是低维映射的目标维度在LoRA中秩$r$是固定的，在每个transformer层都保持一致，但AdaLoRA【3】发现在模型不同层、同层不同变量的重要性是不一样的，因此在所有位置使用固定的秩$r$是不合理的图5. (a)同层不同变量对精度的影响；(b)不同层对精度的影响图5(a)中使用LoRA在每一层分别仅微调$W_q$、$W_k$、$W_v$、$W_o$、$W_{f1}$、$W_{f2}$，观察模型在MNLI任务中的精度，发现  同层不同变量对精度的影响不一样  微调$W_{f1}$和$W_{f2}$的效果最好其中$W_q$、$W_k$、$W_v$分别是自注意力模块中query、key和value的权重，$W_o$是该模块输出投影的权重$W_{f1}$、$W_{f2}$是接在自注意力模块后面2层FFN的权重图5(b)在模型的不同层中对该层所有6个权重矩阵做LoRA微调，观察其在MNLI任务中的精度，发现  不同层对精度的影响不一样  微调模型后面的层效果更好图6. 使用AdaLoRA训练后不同层不同权重中秩的值图6是DeBRTaV3-base经过AdaLoRA自适应训练后的秩分布情况，可以看到靠近输出的层和W_f1、W_o、W_v、W_k对精度贡献比较大AdaLoRA基本思路\\[\\begin{aligned}W=W^{(0)}+\\Delta=\\begin{cases}W^{(0)}+PQ&amp;LoRA\\\\W^{(0)}+P\\Lambda Q&amp;AdaLoRA\\end{cases}\\end{aligned} \\tag{1}\\]如式1所示，对于某个权重矩阵$W$  LoRA并行添加的adapter包含的A模块权重矩阵为$P$，B模块权重矩阵为$Q$  AdaLoRA与LoRA在结构上保持一致，但在AB模块间添加了一个E模块，其权重矩阵是对角矩阵$\\Lambda$从式1可知AdaLoRA的基本思路：对$\\Delta$做奇异值分解，通过动态调整对角矩阵$\\Lambda$的有效数量（即特征值的数量，对应LoRA中的$r$）实现秩的自适应调节上述基本思路有3个需要解决的问题：  奇异值分解计算量大  按照什么标准取舍特征值  训练过程中每个step如何规划特征值保留的数量近似奇异值分解为了规避奇异值分解的计算问题  AdaLoRA在模型初始化时直接引入A、E、B模块，对A、B做随机高斯初始化，对E做全0初始化  为尽可能保证A、B为正交矩阵（奇异值分解的特性），对AB加入正则化惩罚\\[R(P,Q)=||P^TP-I||_F^2+||QQ^T-I||_F^2 \\tag{2}\\]其中$P\\in \\mathbb{R}^{d_1\\times r}$、$Q\\in \\mathbb{R}^{r\\times d_2}$、$\\Lambda \\in \\mathbb{R}^{r\\times r}$# peft.AdaLoraLayer初始化nn.init.zeros_(self.lora_E[adapter_name])nn.init.normal_(self.lora_A[adapter_name], mean=0.0, std=0.02)nn.init.normal_(self.lora_B[adapter_name], mean=0.0, std=0.02)# 正则化惩罚regu_loss = 0num_param = 0for n, p in self.model.named_parameters():    if (\"lora_A\" in n or \"lora_B\" in n) and self.trainable_adapter_name in n:        # rxr        para_cov = p @ p.T if \"lora_A\" in n else p.T @ p        I = torch.eye(*para_cov.size(), out=torch.empty_like(para_cov))        I.requires_grad = False        num_param += 1        regu_loss += torch.norm(para_cov - I, p=\"fro\")if num_param &gt; 0:    regu_loss = regu_loss / num_paramelse:    regu_loss = 0outputs.loss += orth_reg_weight * regu_loss特征值剪枝AdaLoRA介绍了2种取舍特征值的方法，文章采用了第2种1）比较特征值的绝对值训练过程的每个step只保留top-k个绝对值最大的特征值，其余的置02）计算每个特征值的敏感度敏感度的基本准则是：如果将一个参数归0会带来比较大的影响，那么这个参数就是比较重要的，不能被剪枝在训练过程的每个step只保留top-k个敏感度最高的特征值，其余的置0敏感度计算对每个&lt;A、E、B&gt;模块，令  $\\lambda_i$是E模块权重矩阵$\\Lambda_{i,i}$的值  $P_{j,i}$是A模块权重矩阵$j$行$i$列的值  $Q_{i,j}$是B模块权重矩阵$i$行$j$列的值  $S_i$是第$i$个特征值（即$\\lambda_i$）的敏感度\\[S_i=s(\\lambda_i)+\\frac{1}{d_1}\\sum_{j=1}^{d_1}s(P_{j,i})+\\frac{1}{d_2}\\sum_{j=1}^{d_2}s(Q_{i,j}) \\tag{3}\\]其中$s()$是计算权重$w$敏感度的函数，其考虑了输入权重的历史值做平滑\\[s^{(t)}(w)=\\bar{I}^{(t)}(w)\\bar{U}^{(t)}(w) \\tag{4}\\]其中$s^{(t)}$表示训练过程第$t$个step的值，$\\bar{I}$和$\\bar{U}$分别为平滑函数和不确定度函数\\[\\begin{aligned}\\bar{I}^{(t)}(w)&amp;=\\beta_1\\bar{I}^{(t-1)}(w)+(1-\\beta_1)I^{(t)}(w)\\\\\\bar{U}^{(t)}(w)&amp;=\\beta_2\\bar{U}^{(t-1)}(w)+(1-\\beta_2)|\\bar{I}^{(t)}(w)-\\bar{I}^{(t)}(w)|\\end{aligned} \\tag{5}\\]其中$I()$的定义如式6，计算其梯度与值的乘积，返回绝对值\\[I(w)=|w\\nabla_{w}L| \\tag{6}\\]# 更新I、U，对应公式5,6def update_ipt(self, model):    # Update the sensitivity and uncertainty for every weight    for n, p in model.named_parameters():        if \"lora_\" in n and self.adapter_name in n:            if n not in self.ipt:                self.ipt[n] = torch.zeros_like(p)                self.exp_avg_ipt[n] = torch.zeros_like(p)                self.exp_avg_unc[n] = torch.zeros_like(p)            with torch.no_grad():                self.ipt[n] = (p * p.grad).abs().detach()                # Sensitivity smoothing                self.exp_avg_ipt[n] = self.beta1 * self.exp_avg_ipt[n] + (1 - self.beta1) * self.ipt[n]                # Uncertainty quantification                self.exp_avg_unc[n] = (                    self.beta2 * self.exp_avg_unc[n] + (1 - self.beta2) * (self.ipt[n] - self.exp_avg_ipt[n]).abs()                )# 计算敏感度，对应公式3,4def _element_score(self, n):    # 公式4    return self.exp_avg_ipt[n] * self.exp_avg_unc[n]def mask_to_budget(self, model, budget):    value_ipt = {}    vector_ipt = {}    triplet_ipt = {}    # Get the importance score for A, E, B    for n, p in model.named_parameters():        if f\"lora_A.{self.adapter_name}\" in n:            entry_ipt = self._element_score(n)            # comb_ipt = [r,1]            # 计算均值, 对应式3中P的处理            comb_ipt = torch.mean(entry_ipt, dim=1, keepdim=True)            name_m = n.replace(\"lora_A\", \"%s\")            if name_m not in vector_ipt:                vector_ipt[name_m] = [comb_ipt]            else:                # 每个name_m包含2个tensor，分别为lora_A和lora_B                vector_ipt[name_m].append(comb_ipt)        if f\"lora_B.{self.adapter_name}\" in n:            entry_ipt = self._element_score(n)            # 计算均值, 对应式3中Q的处理            comb_ipt = torch.mean(entry_ipt, dim=0, keepdim=False).view(-1, 1)            name_m = n.replace(\"lora_B\", \"%s\")            if name_m not in vector_ipt:                vector_ipt[name_m] = [comb_ipt]            else:                vector_ipt[name_m].append(comb_ipt)        if f\"lora_E.{self.adapter_name}\" in n:            entry_ipt = self._element_score(n)            name_m = n.replace(\"lora_E\", \"%s\")            value_ipt[name_m] = entry_ipt    all_score = []    # Calculate the score for each triplet    for name_m in vector_ipt:        ipt_E = value_ipt[name_m]        # ipt_AB = [rx2]        ipt_AB = torch.cat(vector_ipt[name_m], dim=1)        sum_ipt = self._combine_ipt(ipt_E, ipt_AB)        name_E = name_m % \"lora_E\"        triplet_ipt[name_E] = sum_ipt.view(-1, 1)        all_score.append(sum_ipt.view(-1))    ...秩更新根据上一步计算的每个$\\lambda_i$的敏感度，从大到小保留前k个，其余的置0def mask_to_budget(self, model, budget):    ...    # 返回所有层所有loraA+loraB+loraE中第k小的数据    mask_threshold = torch.kthvalue(        torch.cat(all_score),        k=self.init_bgt - budget,    )[0].item()    rank_pattern = {}    # Mask the unimportant triplets    with torch.no_grad():        for n, p in model.named_parameters():            if f\"lora_E.{self.adapter_name}\" in n:                # 敏感度低的置0                p.masked_fill_(triplet_ipt[n] &lt;= mask_threshold, 0.0)                # rank_pattern[n]维护当前n中可用的r                rank_pattern[n] = (~(triplet_ipt[n] &lt;= mask_threshold)).view(-1).tolist()训练过程中特征值数量的规划前面确定了按什么标准取舍特征值，但没有考虑在训练过程每个step中top-k的k取值AdaLoRA提出了一种基于立方插值的k取值方式（论文公式12有误）\\[\\begin{aligned}b^{(t)}=\\begin{cases}b^{(0)}&amp;0\\le t&lt;t_i\\\\b^{(T)}+(b^{(0)}-b^{(T)})(1-\\frac{t-t_i}{T-t_i-t_f})&amp;t_i\\le t&lt;T-t_f\\\\b^{(T)}&amp;else\\end{cases}\\end{aligned} \\tag{7}\\]其中$b^{(t)}$是step=t时的k取值  当$t&lt;t_i$时，k取值保持不变为$b^{(0)}$  当$t_i\\le t&lt;T-t_f$时，k取值逐步降低  当$t\\ge T-t_f$时，k取值保持不变为$b^{(T)}$其中$T$为总step数，当然实际step可以超过$T$，此时k取值保持不变需要特别注意的是$t_f$表示k保持为$b^{(T)}$的step数def budget_schedule(self, step: int):    # 分别对应t_i, t_f, T    tinit = self.peft_config.tinit    tfinal = self.peft_config.tfinal    total_step = self.peft_config.total_step    # Initial warmup    # self.init_bgt对应b(0), self.target_bgt对应b(T)    # 通常取b(0)=1.5b(T)    if step &lt;= tinit:        budget = self.init_bgt        mask_ind = False    # Final fine-tuning    elif step &gt; total_step - tfinal:        budget = self.target_bgt        mask_ind = True    else:        # Budget decreasing with a cubic scheduler        mul_coeff = 1 - (step - tinit) / (total_step - tfinal - tinit)        budget = int((self.init_bgt - self.target_bgt) * (mul_coeff**3) + self.target_bgt)        mask_ind = True if step % self.peft_config.deltaT == 0 else False    return budget, mask_indAdaLoRA的效果由于AdaLoRA在SFT过程中不仅需要学习权重，还要不断调整秩$r$，因此相较于LoRA需要更多的epoch数个人认为  AdaLoRA的意义在于：在获得相同SFT精度的前提下减少可训练参数（理论上LoRA也可以学到AdaLoRA的秩分布，代价是浪费一些可训练参数）  AdaLoRA的缺点在于：需要更多的训练epoch，超参数更多LoRA与AdaLoRA的消融实验下图使用Llama2B在某医学数据集（train:3922，test:1681，均为4选1的选择题）上做SFT图7. LoRA与AdaLoRA的消融实验target-2表示微调的target_modules为q_proj,v_projtarget-7表示微调的target_modules为q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj  图(a)保持target-2，分别使用LoRA和AdaLoRA训练1、4、8个epoch  图(b)保持target-7，分别使用LoRA和AdaLoRA训练1、4、8个epoch  图(c)保持LoRA，分别使用target-2和target-7训练1、4、8个epoch  图(d)保持AdaLoRA，分别使用target-2和target-7训练1、4、8个epoch从上图可以看到  在充分训练的前提下，lora效果好于adalora  增加targets可以提高精度，但同时也要提高epoch使其训练充分文献【1】Houlsby N, Giurgiu A, Jastrzebski S, et al. Parameter-efficient transfer learning for NLP[C]//International Conference on Machine Learning. PMLR, 2019: 2790-2799.【2】Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021.【3】Zhang Q, Chen M, Bukharin A, et al. Adaptive budget allocation for parameter-efficient fine-tuning[J]. arXiv preprint arXiv:2303.10512, 2023."
  },
  
  {
    "title": "【SFT】（2）Soft Prompts系列算法（P-Tuning、P-Tuning v2）",
    "url": "/posts/SFT-2-Soft-Prompts%E7%B3%BB%E5%88%97%E7%AE%97%E6%B3%95-P-Tuning-P-Tuning-v2/",
    "categories": "LLM, SFT",
    "tags": "SFT",
    "date": "2023-07-21 00:00:00 +0000",
    





    
    "snippet": "Soft Prompts类方法也称为连续Prompts，其基本思想是将由人类设计的，离散的，有具体语义的prompt换成机器可学习的，连续的参数这类方法的最大好处是不改变预训练模型结构，但会占用部分输入按提出时间的先后关系有以下代表方法            时间      方法名称      Prompt位置      初始化方法      adapter结构      针对下游任务   ...",
    "content": "Soft Prompts类方法也称为连续Prompts，其基本思想是将由人类设计的，离散的，有具体语义的prompt换成机器可学习的，连续的参数这类方法的最大好处是不改变预训练模型结构，但会占用部分输入按提出时间的先后关系有以下代表方法            时间      方法名称      Prompt位置      初始化方法      adapter结构      针对下游任务      针对模型                  2021      Prefix-tuning【1】      网络每一层      随机初始化      MLP      NLG      T5、GPT              2021      P-Tuning【2】      仅输入层      随机初始化      LSTM+MLP      NLU      GPT              2021      Prompt Tuning【3】      仅输入层      使用与输出相关token的embedding      MLP      Seq2Seq      T5              2022      P-Tuning v2【4】      网络每一层      随机初始化      MLP      NLU      GPT      对于仅在输入层做改动的方法，其Prompt的组织形式有差异1）Prefix Prompt  对T5类encoder-decoder语言模型采用(PREFIX, x, PREFIX', y)  对GPT类单向语言模型采用(PREFIX, x, y)2）P-Tuning  P-Tuning对BERT类双向语言模型采用(P1,x,P2,[MASK],P3)  P-Tuning对GPT类单向语言模型采用(P1,x,P2,[MASK])其中PREFIX、P1和P2是tokens表示的Soft PromptsP-TuningP-Tuning和P-TuningV2的区别是：  P-Tuning将连续的prompt添加到输入层中训练  P-TuningV2将连续的prompt添加到网络的每一层中训练P-Tuning只是对手工设计的prompt做可训练修改，因此对于每一个下游任务需要训练不同的连续prompt方法背景P-Tuning【2】将预训练模型分为3类  无方向语言模型：也称因果模型（casual LM），如GPT，侧重于NLG  双向语言模型：如BERT，侧重于NLU  混合语言模型：如XLNET、UniLM，结合了前2种长期以来研究者发现使用fine-tuning训练的GPT类型模型在NLU上表现很差，因此认为这类模型是不适合NLU的但GPT-3证明了在足够大的无向语言模型中使用合适的prompt是可以解决NLU问题的然而手工设计的prompt存在以下问题：  找到一个效果最好的prompt非常难  很多时候prompt有效往往意味着对测试集的过拟合  很可能设计了一个对抗性的prompt，削弱模型的性能图1. 手工设计的prompt效果不稳定因此，P-Tuning是在连续空间中自动搜索prompt，以此来提升GPT完成NLU的能力方法细节以“预测国家首都”的子任务为例，模型的输入可以为The capital of Britain is [MASK]其中  \"The capital of ... is ...\"是prompt  \"Britain\"是上下文（可以理解为prompt函数的输入）  \"[MASK]\"是模型的预测（即prompt函数的输出）上述输入经过vocab和embedding后被映射为特定的tensorP-Tuning则将其中prompt部分替换为可训练的$h_i$（embedding tensors），通过微调模型找到最优的表示图2. 左：手工设计的离散prompt；右：P-Tuning使用离散prompt代替因为被替换的tokens其实并没有具体语义，也不需要考虑在vocab中，因此被P-Tuning称为伪token如上文所述，P-Tuning对不同的语言模型采用不同的Ptompt模板，但是在PEFT的代码中，则是简单的使用Prompts+inputs的形式inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)P-Tuning将离散化的prompt换成可训练的连续prompt，面临两个问题1）离散化问题预训练的embedding是高度离散化的（峰值分布），因此$h_i$的初始化如果使用随机分布（我的理解是在现有embedding矩阵中随机挑）并且用SGD优化，很容易陷入局部最优2）关联性问题$h_i$之间应该是有相互关联的，需要一个机制实现这种关联针对上述两个问题，P-Tuning搭建了一个基于LSTM+MLP的网络对$h_i$做处理\\[h_i=MLP([LSTM(h_{0:i}):LSTM(h_{i:m})]) \\tag{1}\\]代码实现摘自huggingface开源的PEFT库def __init__(self, config):    # embedding    # self.total_virtual_tokens表示用多少个伪tokens    self.embedding = torch.nn.Embedding(self.total_virtual_tokens, self.token_dim)    # 只有在训练模式下才需要LSTM+MLP    # 推理模式下直接用训练好的self.embedding就可以了    # PEFT中同时还提供了MLP代替LSTM+MLP的方式，这里为了与原文保持一致不做摘录    if not config.inference_mode:        lstm_dropout = config.encoder_dropout        num_layers = config.encoder_num_layers        # LSTM        self.lstm_head = torch.nn.LSTM(            input_size=self.input_size,            hidden_size=self.hidden_size,            num_layers=num_layers,            dropout=lstm_dropout,            bidirectional=True,            batch_first=True,        )        self.mlp_head = torch.nn.Sequential(            torch.nn.Linear(self.hidden_size * 2, self.hidden_size * 2),            torch.nn.ReLU(),            torch.nn.Linear(self.hidden_size * 2, self.output_size),        )def forward(self, indices):    input_embeds = self.embedding(indices)    output_embeds = self.mlp_head(self.lstm_head(input_embeds)[0])    return output_embeds方法效果P-Tuning在GPT的基础上做优化，在两个NLU测试集（LAMA、SuperGLUE）上评测1）LAMA【5】冻结模型参数，p-tuning比原始的手工设计的prompt在精度指标p@1上获得了26.2%-41.1%的提升2）SuperGLUE联合使用P-Tuning和fine-tuning做few-shot和全量监督学习，发现训练后的GPT效果与bert相当，在有些数据集上甚至更优由此，P-Tuning认为GPT并不是没有NLU的知识，只是需要更合适的引导方法  使用p-tuning，GPT甚至能比BERT更优  p-tuning是一个可以提升GPT和BERT的通用性方法，无论是从few-shot还是从全量微调上为了对比P-Tuning在无方向语言模型（如GPT）和双向语言模型（如BERT）上的效果，文章设置了4组实验  MP：使用LAMA数据集中手工设计的prompt  FT：冻结模型参数做fine-tuning，以适应下游任务  MP+FT：fine-tuning模型的基础上使用手工设计的prompt  PT：使用P-Tuning提出的连续prompt最终得出结论：  对BERT类模型，MP+FT的提升效果更好（与文献【6】和【7】的结论一致）  对GPT类模型，P-Tuning的提升效果更好P-Tuning v2P-Tuning v2【8】发现P-Tuning方法存在两个问题  对于100亿以下参数规模的模型效果没有fine-tuning好  对于某些序列标注问题如QA提取表现不如fine-tuning图3. P-Tuning在10B以下模型表现较差，P-Tuning v2解决了该问题方法细节方法将NLU任务按难度分为两种  简单的分类任务，如大多数GLUE和SuperGLUE任务  困难的序列标注任务，如NER和QA对抽取任务图4. 左：P-Tuning；右：P-Tuning v2如图4所示，与P-Tuning和Prompt-Tuning【3】相比，P-Tuning v2有两点不同：  每层都注入连续prompt（输入不再包含prompt）  分类结果通过[CLS]获得，而非基于[MASK]的结果映射方法认为P-Tuning仅修改输入序列中的prompt会存在两个问题  可训练的参数太少  对模型输出的影响比较间接需要说明的是P-Tuning v2的思想与Prefix-tuning【1】和Soft-Prompt【9】非常类似，其贡献在于以下的几点观察1）重参数化为了提高训练速度和鲁棒性，一般我们会用MLP对代表prompt的embedding做处理，但其实际效果与任务类型和数据集有关：  对RTE和CoNLL04数据集，MLP能带来提升  对BoolQ和CoNLL12，MLP没有明显提升，甚至会有负面效果2）prompt的长度  对于简单的分类任务，可以使用较短的prompt（少于20个token）  对于复杂的序列标注任务，需要使用更长的prompt（约100个token）3）多任务学习首先在多个任务中对prompt做预训练，然后针对特定下游任务训练可以带来更好的效果4）分类头通过prompt引导LM做分类任务通常对LM的输出做映射（verbalizer），比如在情感分类中，LM对[MASK]的预测是“great”则映射为分类结果“positive”而P-Tuning v2因为有一定数量的可训练参数，因此基于监督训练实验用位于句首的[CLS]作为分类结果室可行的（注意图4左的“Verbalizer(with LM head)”和图4右的“Class Label(with linear head)”的位置差异）图5.Class Label的方法与Verbalizer相比效果相当从图5可知，使用Class Label的方法与Verbalizer相比效果是相当的而因为不需要手动设计Verbalizer中词语的映射关系，Class Label显然是更方便的5）prompt的深度方法比较了将同样数量的prompt放在不同层上的表现，发现  prompt添加到靠近输出的层，效果与所有层添加相当（后者参数更多）当然总的来说，所有层都添加prompt是最好的选择代码实现PEFT中的PREFIX_TUNING其实就是P-Tuning v2  为了与transfomers的代码结构保持一致，将prompt以past_key_values（transformers中用于传递历史信息的变量）注入到每层  每层其实有2个prompt，一个是key，一个是value（num_layers * 2 * token_dim）def __init__(self, config):    self.prefix_projection = config.prefix_projection    token_dim = config.token_dim    num_layers = config.num_layers    encoder_hidden_size = config.encoder_hidden_size    num_virtual_tokens = config.num_virtual_tokens    if self.prefix_projection and not config.inference_mode:        # 只有在训练模式下才做transform        # Use a two-layer MLP to encode the prefix        self.embedding = torch.nn.Embedding(num_virtual_tokens, token_dim)        self.transform = torch.nn.Sequential(            torch.nn.Linear(token_dim, encoder_hidden_size),            torch.nn.Tanh(),            # 每层有key和value            torch.nn.Linear(encoder_hidden_size, num_layers * 2 * token_dim),        )    else:        self.embedding = torch.nn.Embedding(num_virtual_tokens, num_layers * 2 * token_dim)def forward(self, prefix: torch.Tensor):    if self.prefix_projection:        prefix_tokens = self.embedding(prefix)        past_key_values = self.transform(prefix_tokens)    else:        past_key_values = self.embedding(prefix)    return past_key_values方法效果P-tuningV2在  3亿到100亿参数的模型  序列标注如QA提取、NER等难任务上表现与fine-tuning相当文献【1】Li X L, Liang P. Prefix-tuning: Optimizing continuous prompts for generation[J]. arXiv preprint arXiv:2101.00190, 2021.【2】Liu X, Zheng Y, Du Z, et al. GPT understands, too[J]. arXiv preprint arXiv:2103.10385, 2021.【3】Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning[J]. arXiv preprint arXiv:2104.08691, 2021.【4】Liu X, Ji K, Fu Y, et al. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks[J]. arXiv preprint arXiv:2110.07602, 2021.【5】Petroni F, Rocktäschel T, Lewis P, et al. Language models as knowledge bases?[J]. arXiv preprint arXiv:1909.01066, 2019.【6】Schick T, Schütze H. It’s not just size that matters: Small language models are also few-shot learners[J]. arXiv preprint arXiv:2009.07118, 2020.【7】Gao T, Fisch A, Chen D. Making pre-trained language models better few-shot learners[J]. arXiv preprint arXiv:2012.15723, 2020.【8】Liu X, Ji K, Fu Y, et al. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks[J]. arXiv preprint arXiv:2110.07602, 2021.【9】Qin G, Eisner J. Learning how to ask: Querying LMs with mixtures of soft prompts[J]. arXiv preprint arXiv:2104.06599, 2021."
  },
  
  {
    "title": "【SFT】（1）有监督微调概述",
    "url": "/posts/SFT-1-%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%E6%A6%82%E8%BF%B0/",
    "categories": "LLM, SFT",
    "tags": "SFT",
    "date": "2023-07-20 00:00:00 +0000",
    





    
    "snippet": "监督微调（Supervised Fine-tuning，SFT）是指基于预先训练好的神经网络模型，针对特定下游任务在少量的监督数据上对其进行重新训练的技术该技术也被称为参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）1. SFT的意义1）任务特定性能提升预训练语言模型通过大规模的无监督训练学习了语言的统计模式和语义表示。然而，它在特定任务上的性能可...",
    "content": "监督微调（Supervised Fine-tuning，SFT）是指基于预先训练好的神经网络模型，针对特定下游任务在少量的监督数据上对其进行重新训练的技术该技术也被称为参数高效微调方法（Parameter-Efficient Fine-Tuning，PEFT）1. SFT的意义1）任务特定性能提升预训练语言模型通过大规模的无监督训练学习了语言的统计模式和语义表示。然而，它在特定任务上的性能可能不如在大规模无监督数据上表现出的性能。通过在任务特定的有标签数据上进行微调，模型可以进一步学习任务相关的特征和模式，从而提高性能2）领域适应性预训练语言模型可能在不同领域的数据上表现不一致。通过在特定领域的有标签数据上进行微调，可以使模型更好地适应该领域的特殊术语、结构和语义，提高在该领域任务上的效果3）数据稀缺性某些任务可能受制于数据的稀缺性，很难获得大规模的标签数据。监督微调可以通过使用有限的标签数据来训练模型，从而在数据有限的情况下取得较好的性能4）防止过拟合在监督微调过程中，通过使用有标签数据进行有监督训练，可以减少模型在特定任务上的过拟合风险。这是因为监督微调过程中的有标签数据可以提供更具体的任务信号，有助于约束模型的学习，避免过多地拟合预训练过程中的无监督信号2. SFT的算法分类PEFT的综述文章（2023）【1】将SFT分为4种类型  Additive类方法  Selective类方法  Reparameterization类方法  混合类方法图1. SFT方法的归类2.1 Additive类方法通过引入额外的参数或网络层进行训练，增强预训练模型对下游任务的能力该类方法可以进一步细分为三类子方法：  Adapters方法  Soft Prompts方法  其他方法2.1.1 Adapters方法在transformer层后面加小的全连接网络（称为adapter）该类方法首先由【2】提出，MAM Adapter【3】针对adapter的位置做了进一步研究，Sparseadapter【4】对adapter做了剪枝，Compacter【5】通过重参数化进一步减少训练参数2.1.2 Soft Prompts方法将离散的prompt换成可学习的embedding，这类方法的最大好处是不改变预训练模型结构P-Tuning【6】、Prompt Tuning【7】、Prefix Tuning【8】是该类别的代表方法，其中P-Tuning和Prompt Tuning仅在输入层训练soft prompts，Prefix Tuning在所有层训练Spot【9】、Warp【10】、【11】、IPT【12】则尝试如何将soft prompts作为预训练提升模型应对不同下游任务的能力2.1.3 其他方法LeTS【13】、LST【14】和IA3【15】通过其他引入训练参数的方法来提高Adapters方法和Soft Prompts方法在内存、计算量和精度上的表现2.2 Selective类方法仅对预训练模型中的部分参数做微调方法【16】、Bitfit【17】和【18】基于层类型或模型内部结构决定微调参数的选择另一类方法【19】【20】【21】基于稀疏更新，可以不考虑模型本身的结构，独立的选择微调的参数但这类方法由于工程和效率上的挑战，在当下的硬件条件下没有实用性2.3 Reparameterization类方法这类方法基于理论神经网络具有低维表示，通过低秩表达来最小化训练参数【22】论证了在低秩子空间做微调是有效的，并且进一步发现模型越大或者预训练时间越长，需要做低秩表示的子空间越小目前LoRA【23】是该方向最具代表性的方法此外Compacter【5】和Krona【24】探索了基于克罗内克乘积的计算方式，在秩和参数量上取得了更好的平衡个人认为这类方法也属于Adapters方法，因为也需要额外的adapter模块实现低秩分解2.4 混合类方法综合上述几种思路，如MAM Adapter【3】结合了Adapters和Prompt Tuning，UniPELT【25】使用了LoRA3. SFT的一些tips3.1 关于训练数据  在SFT上数据规模的重要性低于数据质量  通常1W条左右的精标数据即可发挥良好效果  在扩充数据规模时需要注意数据多样性，多样性的数据可以提高模型性能  多样性除了从原始数据中获取，也可以通过prompt_template方式构建（比如“中译英”的指令可以扩展为“中文翻译英文”、“翻译中文为英文”等语义相同的指令）3.2 关于训练参数  epoch影响比lr大。可以根据数据规模适当调整epoch大小（比如小数据量可以适当增大epoch）  过高的epoch可能会带来通用NLP能力的遗忘  对于ptuning和lora等PEFT训练方式，同时可以适当增大lr3.3 SFT的实现Huggingface的PEFT库1）实现了7种主流SFT算法LoRA【23】、Prefix Tuning【8】、P-Tuning【6】、Prompt Tuning【7】、AdaLoRA【26】、LLaMA-Adapter【27】、IA3【15】2）支持了8种类型的模型  因果语言模型：GPT-2、Bloom、OPT、GPT-Neo、GPT-J、GPT-NeoX-20B、LLaMA、ChatGLM  条件生成模型：T5、BART  序列分类模型：BERT、RoBERTa、GPT-2、Bloom、OPT、GPT-Neo、GPT-J、Deberta、Deberta-v2  词元分类模型：BERT、RoBERTa、GPT-2、Bloom、OPT、GPT-Neo、GPT-J、Deberta、Deberta-v2  Text2Image生成模型：Stable Diffusion  图像分类模型：ViT、Swin  Image2Text生成模型（多模态）：Blip-2  语义分割模型：SegFormer每种模型受支持的SFT算法各有不同，参考官网介绍4. 文献【1】Lialin V, Deshpande V, Rumshisky A. Scaling down to scale up: A guide to parameter-efficient fine-tuning[J]. arXiv preprint arXiv:2303.15647, 2023.【2】Houlsby N, Giurgiu A, Jastrzebski S, et al. Parameter-efficient transfer learning for NLP[C]//International Conference on Machine Learning. PMLR, 2019: 2790-2799.【3】He J, Zhou C, Ma X, et al. Towards a unified view of parameter-efficient transfer learning[J]. arXiv preprint arXiv:2110.04366, 2021.【4】He S, Ding L, Dong D, et al. Sparseadapter: An easy approach for improving the parameter-efficiency of adapters[J]. arXiv preprint arXiv:2210.04284, 2022.【5】Karimi Mahabadi R, Henderson J, Ruder S. Compacter: Efficient low-rank hypercomplex adapter layers[J]. Advances in Neural Information Processing Systems, 2021, 34: 1022-1035.【6】Liu X, Zheng Y, Du Z, et al. GPT understands, too[J]. arXiv preprint arXiv:2103.10385, 2021.【7】Lester B, Al-Rfou R, Constant N. The power of scale for parameter-efficient prompt tuning[J]. arXiv preprint arXiv:2104.08691, 2021.【8】Li X L, Liang P. Prefix-tuning: Optimizing continuous prompts for generation[J]. arXiv preprint arXiv:2101.00190, 2021.【9】Vu T, Lester B, Constant N, et al. Spot: Better frozen model adaptation through soft prompt transfer[J]. arXiv preprint arXiv:2110.07904, 2021.【10】Hambardzumyan K, Khachatrian H, May J. Warp: Word-level adversarial reprogramming[J]. arXiv preprint arXiv:2101.00121, 2021.【11】Su Y, Wang X, Qin Y, et al. On transferability of prompt tuning for natural language processing[J]. arXiv preprint arXiv:2111.06719, 2021.【12】Qin Y, Wang X, Su Y, et al. Exploring Universal Intrinsic Task Subspace via Prompt Tuning[J]. arXiv preprint arXiv:2110.07867, 2021.【13】Fu C, Huang H, Chen X, et al. Learn-to-share: A hardware-friendly transfer learning framework exploiting computation and parameter sharing[C]//International Conference on Machine Learning. PMLR, 2021: 3469-3479.【14】Sung Y L, Cho J, Bansal M. Lst: Ladder side-tuning for parameter and memory efficient transfer learning[J]. Advances in Neural Information Processing Systems, 2022, 35: 12991-13005.【15】Liu H, Tam D, Muqeeth M, et al. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning[J]. Advances in Neural Information Processing Systems, 2022, 35: 1950-1965.【16】Gheini M, Ren X, May J. Cross-attention is all you need: Adapting pretrained transformers for machine translation[J]. arXiv preprint arXiv:2104.08771, 2021.【17】Zaken E B, Ravfogel S, Goldberg Y. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models[J]. arXiv preprint arXiv:2106.10199, 2021.【18】Vucetic D, Tayaranian M, Ziaeefard M, et al. Efficient fine-tuning of bert models on the edge[C]//2022 IEEE International Symposium on Circuits and Systems (ISCAS). IEEE, 2022: 1838-1842.【19】Sung Y L, Nair V, Raffel C A. Training neural networks with fixed sparse masks[J]. Advances in Neural Information Processing Systems, 2021, 34: 24193-24205.【20】Ansell A, Ponti E M, Korhonen A, et al. Composable sparse fine-tuning for cross-lingual transfer[J]. arXiv preprint arXiv:2110.07560, 2021.【21】Guo D, Rush A M, Kim Y. Parameter-efficient transfer learning with diff pruning[J]. arXiv preprint arXiv:2012.07463, 2020.【22】Aghajanyan A, Zettlemoyer L, Gupta S. Intrinsic dimensionality explains the effectiveness of language model fine-tuning[J]. arXiv preprint arXiv:2012.13255, 2020.【23】Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021.【24】Edalati A, Tahaei M, Kobyzev I, et al. Krona: Parameter efficient tuning with kronecker adapter[J]. arXiv preprint arXiv:2212.10650, 2022.【25】Mao Y, Mathias L, Hou R, et al. Unipelt: A unified framework for parameter-efficient language model tuning[J]. arXiv preprint arXiv:2110.07577, 2021.【26】Zhang Q, Chen M, Bukharin A, et al. Adaptive budget allocation for parameter-efficient fine-tuning[J]. arXiv preprint arXiv:2303.10512, 2023.【27】Zhang R, Han J, Zhou A, et al. Llama-adapter: Efficient fine-tuning of language models with zero-init attention[J]. arXiv preprint arXiv:2303.16199, 2023."
  },
  
  {
    "title": "C-Eval大模型评估基座",
    "url": "/posts/C-Eval%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%9F%BA%E5%BA%A7/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-07-10 00:00:00 +0000",
    





    
    "snippet": "C-Eval【1】是清华2023年提出的一个适用于大语言模型的多层次多学科中文评估套件C-Eval的主页和测试代码图1. 截至20230710榜单上的top5大模型benchmark概览图2. 现有大模型评测基座C-Eval数据C-Eval考虑到知名度高的考试题可能已经作为训练数据被使用过了，因此从小范围的考试中或者模拟考试中选择其数据组成为  主要来自网上的模拟考试题  部分来自学生分享的...",
    "content": "C-Eval【1】是清华2023年提出的一个适用于大语言模型的多层次多学科中文评估套件C-Eval的主页和测试代码图1. 截至20230710榜单上的top5大模型benchmark概览图2. 现有大模型评测基座C-Eval数据C-Eval考虑到知名度高的考试题可能已经作为训练数据被使用过了，因此从小范围的考试中或者模拟考试中选择其数据组成为  主要来自网上的模拟考试题  部分来自学生分享的大学往期考试题  另外从维普收集了2000个问题C-Eval将数据分成训练集、验证集和测试集  其中验证集大小（包含1346个问题）与测试集的比例为1:9  为了确保评测公平，测试集不公开  对于数学公式统一用Latex格式表示数据组成图3. C-Eval数据组成如图3，C-Eval的数据涵盖了四个大类：  STEM：包含科学、技术、工程和数学  Social Science：社会科学  Humanities：人文学科  Other：其他，包含基础医学、公务员考试、医师资格认证等上述4个大类共细分为52个子类，知识覆盖了4个层次  初中/高中层次：包含除了英文的所有标准学科  大学层次：从中国教育部列出的所有13个官方本科专业类别中选择了25个具有代表性的学科  专业层次：从官方国家职业资格名录中选取了12个代表性的，如医师、法律专业人员、公务员资格考试此外C-Eval还推出了一个难度更高的子集C-Eval Hard，包含8个科目：  高等数学、离散数学、概率论与数理统计、大学化学、大学物理、高中数学、高中化学、高中物理C-Eval Hard的难度超过了GSM8K【4】，与【5】相当，GPT-4的准确率为59.4%评测题型C-Eval全部以单选题（4选1）的形式评估模型，其认为这种方式  方便计算精度  是一种评估模型潜在能力的很好的方式C-Eval一共包含13948个单选题评测实验评测方法1）five-shot在prompt中提供5个示例（参考了MMLU，后者也是使用five-shot）2）AO &amp; CoT对比了answer-only（AO，左）和chain-of-thought（CoT，右）两种方式图4. 左：AO；右CoT评测结果图5. a：普通数据集中AO精度；b：普通数据集中CoT精度；c：Hard数据集中AO&amp;CoT精度图5(a)(b)分别是普通数据集中AO和CoT的结果，(c)是在C-Eval Hard数据集的结果  GPT-4是所有模型中表现最好的，也是唯一一个平均分超过60的  低于500亿的模型只能比随机选择（25分）高10个点左右  CoT的得分普遍低于AO（推荐在模型大于650亿才使用CoT）  MiniMax是中文语料模型中表现最好的  当任务变得复杂，需要高级推理技能时（如STEM类别的题目），MiniMax与ChatGPT性能差异被拉大文章对模型在CoT上表现比AO差的现象做了猜想：  很多科目并不是推理密集型，强制加入推理步骤反而削弱模型表现  没有经过包含CoT的instruction tuning的模型，在评测中很难通过CoT prompt得到正面提升（如PaLM、GLM-130B、LLaMA-65B）官方建议prompt范式的选择      prompt范式    建议        few-shot    answer-only    推荐首选方案        chain-of-thought    一般在模型超过65B后才有显著效果        zero-shot    answer-only    instruction tuning后用该方案        chain-of-thought    instruction tuning后且模型足够强，选该方案  prompt格式的选择  prompt的格式非常重要，要有换行，每行末尾不要有空格  prompt的格式还与模型的训练方式（chatbot/非chatbot）有关Completion模式（非chatbot模型）如果模型没有被调成一个chatbot，那么对于上述4种prompt范式，都是将其统一组织输入给模型以下是中国关于{subject}考试的单项选择题，请选出其中的正确答案。 &lt;- 注意把 {subject} 改成具体的科目名称[题目 1]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：A              ...                 &lt;- 题目 2 到 4[题目 5]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：C[测试题目]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：&lt;模型从此处生成&gt;  上面是 in-context answer-only 格式的 prompt。如果是 zero-shot 的话，则去掉 [题目 1] 到 [题目 5] 的 in-context 样本  由于文章使用five-shot，因此这里预设了5个题目作为few-shot  如果模型的 context length 装不下所有的 in-context examples，则去掉一两个  如果是 chain-of-thought 的话，则需要加上 {让我们一步一步思考 [explanation]} 的内容以下是中国关于{subject}考试的单项选择题，请选出其中的正确答案。 &lt;- 注意把 {subject} 改成具体的科目名称[题目 1]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：让我们一步一步思考，1. {解析过程步骤1}2. {解析过程步骤2}3. {解析过程步骤3}所以答案是A。...      &lt;- 题目 2 到 4[题目 5]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：让我们一步一步思考，1. {解析过程步骤1}2. {解析过程步骤2}所以答案是C。[测试题目]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：让我们一步一步思考，&lt;模型从此处生成&gt;Chat模式（chat模型）如果模型已经被调为chatbot，那么prompt也需要改成对话的格式即让模型假装已经正确回答了5个问题，当前回答最后一轮的问题，以in-context answer-only范式为例：System:以下是中国关于{subject}考试的单项选择题，请选出其中的正确答案。  &lt;- 注意这个初始 instruction 是以 system message 的格式传入的；   如果当前模型不支持 system，则把它放在第一轮对话的开头然后换行User:[题目 1]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：Assistant:A       &lt;- 注意这个地方不是模型生成的，而是我们 hard code 作为输入的...                 &lt;- 题目 2 到 4User:[题目 5]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：Assistant:C       &lt;- Again，这个地方不是模型生成的，而是我们 hard code 作为输入的User:[测试题目]A. [选项 A 具体内容]B. [选项 B 具体内容]C. [选项 C 具体内容]D. [选项 D 具体内容]答案：Assistant:&lt;模型从此处生成&gt;few-shot和zero-shot的选择  一般来说，pretraining阶段的模型few-shot的效果总是比zero-shot好  但经过instruction tuning后的模型，且tuning过程中没有few-shot data的话，很可能zero-shot会更好考虑到few-shot是面向开发者的，zer-shot是面向用户的（因为用户不会写prompt），建议开发2个版本  一个面向开发者，把in-context learning的能力拉满  一个面向用户，把zero-shot的能力拉满prompt engineering的选择对于pretraining阶段的模型（未经过instruction tuning）  prompt的不同会得到很不同的结果（数据集中的默认prompt不保证最优）  在实际操作中需要区分分数的提高是来自于模型的提升还是prompt的提升  如果目标是开发模型，则不要做太多的prompt优化对于instruction-tuned的模型  prompt的差异导致模型效果的变化会减小，但也无法忽略  模型对prompt enginneering的需求会减小，但仍存在因此建议做两份结果：  一份使用dev文件夹的数据作为default prompt（作为baseline）  一份根据模型做prompt engineering与baseline对比其他tips  测试的时候一般设temperature为0，做greedy decoding  大模型一般不用beam search，成本高且作用不大  上线一般用sampling，用户友好，错了可以再说一遍  知识性的问题不太需要CoT，推理性的问题需要CoT  推理能力的区分度最高，模型每大一点有近20分的差距  知识能力的区分度相对低，模型大一个台阶仅有5分左右的差距文献【1】Huang Y, Bai Y, Zhu Z, et al. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models[J]. arXiv preprint arXiv:2305.08322, 2023.【2】Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis platform for natural language understanding[J]. arXiv preprint arXiv:1804.07461, 2018.【3】Wang A, Pruksachatkun Y, Nangia N, et al. Superglue: A stickier benchmark for general-purpose language understanding systems[J]. Advances in neural information processing systems, 2019, 32.【4】Cobbe K, Kosaraju V, Bavarian M, et al. Training verifiers to solve math word problems[J]. arXiv preprint arXiv:2110.14168, 2021.【5】Hendrycks D, Burns C, Kadavath S, et al. Measuring mathematical problem solving with the math dataset[J]. arXiv preprint arXiv:2103.03874, 2021.【6】Chen M, Tworek J, Jun H, et al. Evaluating large language models trained on code[J]. arXiv preprint arXiv:2107.03374, 2021.【7】Hendrycks D, Burns C, Basart S, et al. Measuring massive multitask language understanding[J]. arXiv preprint arXiv:2009.03300, 2020.【8】Srivastava A, Rastogi A, Rao A, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models[J]. arXiv preprint arXiv:2206.04615, 2022.【9】Liang P, Bommasani R, Lee T, et al. Holistic evaluation of language models[J]. arXiv preprint arXiv:2211.09110, 2022.【10】Xu L, Hu H, Zhang X, et al. CLUE: A Chinese language understanding evaluation benchmark[J]. arXiv preprint arXiv:2004.05986, 2020.【11】Zhong W, Cui R, Guo Y, et al. Agieval: A human-centric benchmark for evaluating foundation models[J]. arXiv preprint arXiv:2304.06364, 2023.【12】Zeng H. Measuring massive multitask chinese understanding[J]. arXiv preprint arXiv:2304.12986, 2023."
  },
  
  {
    "title": "使用LangChain创建LLM应用",
    "url": "/posts/%E4%BD%BF%E7%94%A8LangChain%E5%88%9B%E5%BB%BALLM%E5%BA%94%E7%94%A8/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-07-05 00:00:00 +0000",
    





    
    "snippet": "本文是吴恩达LangChain课程的梳理笔记总的来说，LangChain对prompt做了进一步封装，可以更方便的构建基于LLM的应用图1. LangChain的关键功能prompt函数化LangChain可以抽象prompt，将其作为函数应用在用户输入上from langchain.prompts import ChatPromptTemplatetemplate_string = \"\"\"...",
    "content": "本文是吴恩达LangChain课程的梳理笔记总的来说，LangChain对prompt做了进一步封装，可以更方便的构建基于LLM的应用图1. LangChain的关键功能prompt函数化LangChain可以抽象prompt，将其作为函数应用在用户输入上from langchain.prompts import ChatPromptTemplatetemplate_string = \"\"\"Translate the text \\that is delimited by triple backticks \\into a style that is {style}. \\text: ```{text}```\"\"\"prompt_template = ChatPromptTemplate.from_template(template_string)### langchain从template_string提取到两个输入变量：目标风格、用户输入&gt;&gt; prompt_template.messages[0].prompt.input_variables['style', 'text']### 将prompt_template应用在一个实例上# 指定风格customer_style = \"\"\"American English \\in a calm and respectful tone\"\"\"# 指定用户输入customer_email = \"\"\"Arrr, I be fuming that me blender lid ...\"\"\"# 使用prompt_template对其做修饰输出customer_messages = prompt_template.format_messages(                    style=customer_style,                    text=customer_email)# customer_messages[0]包含使用prompt修饰的用户输入&gt;&gt; type(customer_messages[0])langchain.schema.HumanMessage面向多轮对话的记忆机制基于LLM的多轮对话是将历史对话补充到当前对话的prompt中The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.Current conversation:Human: Hi, my name is AndrewAI: Hello Andrew! It's nice to meet you. How can I assist you today?Human: What is 1+1?AI: 1+1 is equal to 2.Human: What is my name?AI:以上是一个多轮对话进行到“what is my name?”时的prompt内容LangChain提供了4种函数管理历史对话ConversationBufferMemory这是一个基础的函数，可以自动保存所有的历史对话，给机器人提供记忆能力from langchain.chat_models import ChatOpenAIfrom langchain.chains import ConversationChainfrom langchain.memory import ConversationBufferMemory# 调用OpenAI作为LLM,设置温度系数为0确保对话是可预测的llm = ChatOpenAI(temperature=0.0)# 使用ConversationBufferMemory跟踪历史对话memory = ConversationBufferMemory()# ConversationChain是对话管理函数# verbose为True时会输出当前prompt内容conversation = ConversationChain(    llm=llm,     memory = memory,    verbose=True)# 多轮对话示例&gt;&gt; conversation.predict(input=\"Hi, my name is Andrew\")&gt;&gt; conversation.predict(input=\"What is 1+1?\")&gt;&gt; conversation.predict(input=\"What is my name?\")AI: Your name is Andrew.# 可以通过memory.buffer或memory.load_memory_variables查看历史对话信息&gt;&gt; print(memory.buffer)Human: Hi, my name is AndrewAI: Hello Andrew! It's nice to meet you. How can I assist you today?Human: What is 1+1?AI: 1+1 is equal to 2.Human: What is my name?AI: Your name is Andrew.&gt;&gt; memory.load_memory_variables({}){'history': \"Human: Hi, my name is Andrew\\nAI: Hello Andrew! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 is equal to 2.\\nHuman: What is my name?\\nAI: Your name is Andrew.\"}ConversationBufferWindowMemory该方法通过滑窗机制删除早期的历史对话，从而控制prompt的总长度该方法颗粒度是轮次，即历史对话删除的最小单位是轮次from langchain.memory import ConversationBufferWindowMemory# k表示prompt中记录的最大历史对话轮次memory = ConversationBufferWindowMemory(k=1)# 保存2轮对话信息&gt;&gt; memory.save_context({\"input\": \"Hi\"},                    {\"output\": \"What's up\"})&gt;&gt; memory.save_context({\"input\": \"Not much, just hanging\"},                    {\"output\": \"Cool\"})# 第1轮信息已经被删除&gt;&gt; memory.load_memory_variables({}){'history': 'Human: Not much, just hanging\\nAI: Cool'}ConversationTokenBufferMemory该方法也可以控制prompt的总长度该方法颗粒度是token，即历史对话删除的最小单位是tokenfrom langchain.memory import ConversationTokenBufferMemoryfrom langchain.llms import OpenAIllm = ChatOpenAI(temperature=0.0)memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=30)&gt;&gt; memory.save_context({\"input\": \"AI is what?!\"},                    {\"output\": \"Amazing!\"})&gt;&gt; memory.save_context({\"input\": \"Backpropagation is what?\"},                    {\"output\": \"Beautiful!\"})&gt;&gt; memory.save_context({\"input\": \"Chatbots are what?\"},                     {\"output\": \"Charming!\"})&gt;&gt; memory.load_memory_variables({}){'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}ConversationSummaryBufferMemory该方法会对历史对话做自动总结，并且支持长度限定该方法颗粒度是token，即历史对话删除的最小单位是tokenfrom langchain.memory import ConversationSummaryBufferMemory# create a long stringschedule = \"There is a meeting at 8am with your product team. \\You will need your powerpoint presentation prepared. \\9am-12pm have time to work on your LangChain \\project which will go quickly because Langchain is such a powerful tool. \\At Noon, lunch at the italian resturant with a customer who is driving \\from over an hour away to meet you to understand the latest in AI. \\Be sure to bring your laptop to show the latest LLM demo.\"memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})memory.save_context({\"input\": \"Not much, just hanging\"},                    {\"output\": \"Cool\"})memory.save_context({\"input\": \"What is on the schedule today?\"},                     {\"output\": f\"{schedule}\"})# 保存的记录是对对话的总结&gt;&gt; memory.load_memory_variables({}){'history': 'System: The human and AI exchange greetings. The human asks about the schedule for the day. The AI provides a detailed schedule, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. The AI emphasizes the power of LangChain and suggests bringing a laptop to showcase the latest LLM demo during the lunch meeting.'}任务的链式执行LangChain可以利用ChatPromptTemplate将prompt包装成一个函数，通常这个函数用于完成一项特定的任务有时候面对一项复杂的任务，我们需要将其分解为若干个子任务，然后根据某种顺序执行LangChain针对这种情况提供了LLMChain方法，将ChatPromptTemplate的输出做进一步封装，各个LLMChain对象可以按顺序连接，统一执行（类似Pipeline的概念）LLMChain对于LLMChain组成的链式任务，使用run方法执行from langchain.chat_models import ChatOpenAIfrom langchain.prompts import ChatPromptTemplatefrom langchain.chains import LLMChainllm = ChatOpenAI(temperature=0.9)prompt = ChatPromptTemplate.from_template(    \"What is the best name to describe \\    a company that makes {product}?\")# LLMChain的基本输入参数是LLM和使用ChatPromptTemplate封装的prompt对象chain = LLMChain(llm=llm, prompt=prompt)product = \"Queen Size Sheet Set\"# 使用chain.run方法执行，该方法输入为prompt模板定义的变量chain.run(product)SimpleSequentialChain可以使用SimpleSequentialChain实现简单的任务串联这种方法默认任务的输入输出只有1个，上一个任务的输出是下一个任务的输入from langchain.chains import SimpleSequentialChainllm = ChatOpenAI(temperature=0.9)# prompt template 1first_prompt = ChatPromptTemplate.from_template(    \"What is the best name to describe \\    a company that makes {product}?\")# Chain 1chain_one = LLMChain(llm=llm, prompt=first_prompt)# prompt template 2second_prompt = ChatPromptTemplate.from_template(    \"Write a 20 words description for the following \\    company:{company_name}\")# chain 2chain_two = LLMChain(llm=llm, prompt=second_prompt)# 将上面两个LLMChain对象组合成一个chains/pipelineoverall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],                                             verbose=True                                            )overall_simple_chain.run(product)SequentialChainSequentialChain可以自定义每个任务的输入输出from langchain.chains import SequentialChainllm = ChatOpenAI(temperature=0.9)## chain 1: 将输入的review翻译为英文# prompt template 1: translate to englishfirst_prompt = ChatPromptTemplate.from_template(    \"Translate the following review to english:\"    \"\\n\\n{Review}\")# chain 1: input= Review and output= English_Reviewchain_one = LLMChain(llm=llm, prompt=first_prompt,                      output_key=\"English_Review\"                    )## chain 2: 根据翻译后的review生成总结second_prompt = ChatPromptTemplate.from_template(    \"Can you summarize the following review in 1 sentence:\"    \"\\n\\n{English_Review}\")# chain 2: input= English_Review and output= summarychain_two = LLMChain(llm=llm, prompt=second_prompt,                      output_key=\"summary\"                    )## chain 3: 判断原始的review使用的语言# prompt template 3: translate to englishthird_prompt = ChatPromptTemplate.from_template(    \"What language is the following review:\\n\\n{Review}\")# chain 3: input= Review and output= languagechain_three = LLMChain(llm=llm, prompt=third_prompt,                       output_key=\"language\"                      )## chain 4: 使用与原始review一样的语言，根据chain2生成的总结，生成一个回应# prompt template 4: follow up messagefourth_prompt = ChatPromptTemplate.from_template(    \"Write a follow up response to the following \"    \"summary in the specified language:\"    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\")# chain 4: input= summary, language and output= followup_messagechain_four = LLMChain(llm=llm, prompt=fourth_prompt,                      output_key=\"followup_message\"                     )# SequentialChain的输入是原始review# 定义chain1的输出为English_Review# 定义chain2的输出为summary# 定义chain3的输出为followup_message# overall_chain: input= Review # and output= English_Review,summary, followup_messageoverall_chain = SequentialChain(    chains=[chain_one, chain_two, chain_three, chain_four],    input_variables=[\"Review\"],    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],    verbose=True)overall_chain(review)Router Chain有时候需要根据用户的输入动态决定使用哪一个任务chainfrom langchain.chains.router import MultiPromptChainchain = MultiPromptChain(router_chain=router_chain,                          destination_chains=destination_chains,                          default_chain=default_chain, verbose=True                        )chain.run(\"What is black body radiation?\")chain.run(\"what is 2 + 2\")LangChain提供了一个MultiPromptChain方法实现上述动态选择功能MultiPromptChain主要涉及3个输入  router_chain：chain的路由选择，根据输入决定被选中的chain的名称  destination_chains：是一个chains的字典，由名称和chain的“函数”组成  default_chain：默认chain，当router_chain没法做出合适的选择时的保底方案destination_chains的构建这里设定了4种chain，分别是物理专家、数学专家、历史专家和计算机科学专家首先为这4种chain编写promptphysics_template = \"\"\"You are a very smart physics professor. \\You are great at answering questions about physics in a concise\\and easy to understand manner. \\When you don't know the answer to a question you admit\\that you don't know.Here is a question:{input}\"\"\"math_template = \"\"\"You are a very good mathematician. \\You are great at answering math questions. \\You are so good because you are able to break down \\hard problems into their component parts, answer the component parts, and then put them together\\to answer the broader question.Here is a question:{input}\"\"\"history_template = \"\"\"You are a very good historian. \\You have an excellent knowledge of and understanding of people,\\events and contexts from a range of historical periods. \\You have the ability to think, reflect, debate, discuss and \\evaluate the past. You have a respect for historical evidence\\and the ability to make use of it to support your explanations \\and judgements.Here is a question:{input}\"\"\"computerscience_template = \"\"\" You are a successful computer scientist.\\You have a passion for creativity, collaboration,\\forward-thinking, confidence, strong problem-solving capabilities,\\understanding of theories and algorithms, and excellent communication \\skills. You are great at answering coding questions. \\You are so good because you know how to solve a problem by \\describing the solution in imperative steps \\that a machine can easily interpret and you know how to \\choose a solution that has a good balance between \\time complexity and space complexity. Here is a question:{input}\"\"\"为上述4种chain编写prompt infos，分别由name、description和prompt_template构成  name：定义chain的名称，作为router_chain的输出  description：每种chain的特性描述，router_chain以此做选择  prompt_template：每种chain的promptprompt_infos = [    {        \"name\": \"physics\",         \"description\": \"Good for answering questions about physics\",         \"prompt_template\": physics_template    },    {        \"name\": \"math\",         \"description\": \"Good for answering math questions\",         \"prompt_template\": math_template    },    {        \"name\": \"History\",         \"description\": \"Good for answering history questions\",         \"prompt_template\": history_template    },    {        \"name\": \"computer science\",         \"description\": \"Good for answering computer science questions\",         \"prompt_template\": computerscience_template    }]  使用ChatPromptTemplate和LLMChain生成每种chain  构建destination_chains字典  生成包含每种chain的name、description的描述，后续需要将其插入router_chain的promptdestination_chains = {}for p_info in prompt_infos:    name = p_info[\"name\"]    prompt_template = p_info[\"prompt_template\"]    prompt = ChatPromptTemplate.from_template(template=prompt_template)    chain = LLMChain(llm=llm, prompt=prompt)    destination_chains[name] = chain      destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]destinations_str = \"\\n\".join(destinations)router_chain的构建首先编写router_chain的prompt，其内容主要是要求LLM根据用户输入决定用哪个name的chainMULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\language model select the model prompt best suited for the input. \\You will be given the names of the available prompts and a \\description of what the prompt is best suited for. \\You may also revise the original input if you think that revising\\it will ultimately lead to a better response from the language model.&lt;&lt; FORMATTING &gt;&gt;Return a markdown code snippet with a JSON object formatted to look like:```json}}```REMEMBER: \"destination\" MUST be one of the candidate prompt \\names specified below OR it can be \"DEFAULT\" if the input is not\\well suited for any of the candidate prompts.REMEMBER: \"next_inputs\" can just be the original input \\if you don't think any modifications are needed.&lt;&lt; CANDIDATE PROMPTS &gt;&gt;{destinations}&lt;&lt; INPUT &gt;&gt;&lt;&lt; OUTPUT (remember to include the ```json)&gt;&gt;\"\"\"然后使用PromptTemplate生成router_chain需要的prompt最后使用LLMRouterChain.from_llm方法生成router_chain对象from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParserfrom langchain.prompts import PromptTemplaterouter_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(    destinations=destinations_str)router_prompt = PromptTemplate(    template=router_template,    input_variables=[\"input\"],    output_parser=RouterOutputParser(),)router_chain = LLMRouterChain.from_llm(llm, router_prompt)基于文档的QA问答LangChain的实现思路为  将文档的内容首先做分块，每块向量化为embedding  对于用户的query，向量化后将其与文档每个分块做相似度检索，得到前K个最相似的内容块  将找到的top K文本作为prompt输入给LLM，让其做出回答使用VectorstoreIndexCreator方式VectorstoreIndexCreator将上述过程做了完整封装from langchain.indexes import VectorstoreIndexCreatorfrom langchain.document_loaders import CSVLoader# 读取文档数据file = 'OutdoorClothingCatalog_1000.csv'loader = CSVLoader(file_path=file)# 使用VectorstoreIndexCreator完成文档分块和向量化# 并建立索引index = VectorstoreIndexCreator(    vectorstore_cls=DocArrayInMemorySearch).from_loaders([loader])# 对用户的query，首先做向量化，然后找到最相似的内容块# 内部将其转换为prompt，输入给LLM得到responsequery =\"Please list all your shirts with sun protection \\in a table in markdown and summarize each one.\"response = index.query(query)使用RetrievalQA方式是对上一种方式的拆分，可以指定过程中的具体方法（序列化方法、LLM等）  指定embedding方法、数据Loader，通过DocArrayInMemorySearch组成database  使用LLM和database初始化RetrievalQAfrom langchain.document_loaders import CSVLoaderfrom langchain.embeddings import OpenAIEmbeddingsfrom langchain.vectorstores import DocArrayInMemorySearchloader = CSVLoader(file_path=file)docs = loader.load()embeddings = OpenAIEmbeddings()db = DocArrayInMemorySearch.from_documents(    docs,     embeddings)##### 可以通过embed_query方法对文本做向量化embed = embeddings.embed_query(\"Hi my name is Harrison\")# 可以使用similarity_search实现database内的相似文本检索query = \"Please suggest a shirt with sunblocking\"docs = db.similarity_search(query)####from langchain.chains import RetrievalQAretriever = db.as_retriever()llm = ChatOpenAI(temperature = 0.0)qa_stuff = RetrievalQA.from_chain_type(    llm=llm,     chain_type=\"stuff\",     retriever=retriever,     verbose=True)response = qa_stuff.run(query)多文本的处理上面的两种方式简单的将文档内容全部读入内存中（即stuff方法）对于文档数目很大的情况，可以考虑使用LLM分别对其做处理以确保性能，具体有3种方式图2. 3种多文本处理方法1）MapReduce每份文档使用一个LLM，最后使用一个汇总LLM得到答案，常用于文本摘要的应用这种方式适合并行，但如果文档间有相互依赖关系则效果不会很好2）Refine每份文档使用一个LLM，该LLM同时接收上一份文档LLM的输出这种方式比MapReduce慢，并且对于后面的文档往往prompt会很长，但可以处理文档相互依赖的问题3）MapRerank可以算MapReduce的特例，每份文档使用一个LLM，该LLM同时输出文档相关性的打分，最终采用分数最高的那个在实际应用中，stuff是最常见的方式，其次是MapReduceQA问答评估评估工作可以分为两部分  QA评估数据的生成  评估结果的生成在整个过程中可以开启LangChain的debug模式，查看其内部的调用情况import langchainlangchain.debug = True评估数据的生成一种是根据文档人工编写QA数据，另一种是通过QAGenerateChain自动生成from langchain.evaluation.qa import QAGenerateChain# 通过LLM从文档中自动提取QAexample_gen_chain = QAGenerateChain.from_llm(ChatOpenAI())new_examples = example_gen_chain.apply_and_parse(    [{\"doc\": t} for t in data[:5]])# 输出示例&gt;&gt; new_examples[0]{'query': \"What is the weight of each pair of Women's Campside Oxfords?\", 'answer': \"The approximate weight of each pair of Women's Campside Oxfords is 1 lb. 1 oz.\"}评估结果的生成  一种是使用上一节“基于文档的QA问答”的方法对每个query生成answer，然后人工比较  另一种是借助LLM比较评估数据的answer和模型输出的answerfrom langchain.chains import RetrievalQAfrom langchain.chat_models import ChatOpenAIfrom langchain.document_loaders import CSVLoaderfrom langchain.indexes import VectorstoreIndexCreatorfrom langchain.vectorstores import DocArrayInMemorySearchfile = 'OutdoorClothingCatalog_1000.csv'loader = CSVLoader(file_path=file)data = loader.load()index = VectorstoreIndexCreator(    vectorstore_cls=DocArrayInMemorySearch).from_loaders([loader])llm = ChatOpenAI(temperature = 0.0)qa = RetrievalQA.from_chain_type(    llm=llm,     chain_type=\"stuff\",     retriever=index.vectorstore.as_retriever(),     verbose=True,    chain_type_kwargs = {        \"document_separator\": \"&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;\"    })# 依次对query生成answer，然后做人工比对qa.run(examples[0][\"query\"])参考“示例”中的Real Answer和Predicted Answer，两者从长度和语义上有很大差别，需要LLM结合上下文才能判断是否等价from langchain.evaluation.qa import QAEvalChain# 对所有的query生成answerpredictions = qa.apply(examples)# 使用QAEvalChain比较评估数据集的answer和predictions中生成的answerllm = ChatOpenAI(temperature=0)eval_chain = QAEvalChain.from_llm(llm)graded_outputs = eval_chain.evaluate(examples, predictions)for i, eg in enumerate(examples):    print(f\"Example {i}:\")    print(\"Question: \" + predictions[i]['query'])    print(\"Real Answer: \" + predictions[i]['answer'])    print(\"Predicted Answer: \" + predictions[i]['result'])    print(\"Predicted Grade: \" + graded_outputs[i]['text'])    print()# 示例&gt;&gt; Example 0:Question: Do the Cozy Comfort Pullover Set have side pockets?Real Answer: YesPredicted Answer: The Cozy Comfort Pullover Set, Stripe does have side pockets.Predicted Grade: CORRECTAI代理前面的工作都围绕挖掘LLM本身的知识另一种用法是，让LLM发挥其阅读理解能力和推理能力，调用工具完成特定的任务比如：查询时间、数学计算、序列排序、执行自定义函数等LangChain内部创建了一些代理工具，如”llm-math”、”wikipedia”，对这些工具有详细的适用范围的prompt描述对于用户的输入，首先会调用LLM判断最适合的工具，然后对任务做梳理，调用工具得到结果使用内置工具集from langchain.agents import load_tools, initialize_agentfrom langchain.agents import AgentTypefrom langchain.python import PythonREPLfrom langchain.chat_models import ChatOpenAI# 加载工具集# llm-math: 数学计算；wikipedia：维基百科查询apillm = ChatOpenAI(temperature=0)tools = load_tools([\"llm-math\",\"wikipedia\"], llm=llm)# 初始化代理# 指定工具集、LLM、代理类型agent= initialize_agent(    tools,     llm,     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, # 该代理针对chat和推理能力做了优化    handle_parsing_errors=True,    verbose = True)# 示例# 数学计算agent(\"What is the 25% of 300?\")# wikipedia查询question = \"Tom M. Mitchell is an American computer scientist \\and the Founders University Professor at Carnegie Mellon University (CMU)\\what book did he write?\"result = agent(question) 调用函数可以通过PythonREPLTool调用python内置函数也可以调用自定义函数from langchain.agents.agent_toolkits import create_python_agentfrom langchain.tools.python.tool import PythonREPLToolagent = create_python_agent(    llm,    tool=PythonREPLTool(),    verbose=True)customer_list = [[\"Harrison\", \"Chase\"],                  [\"Lang\", \"Chain\"],                 [\"Dolly\", \"Too\"],                 [\"Elle\", \"Elem\"],                  [\"Geoff\",\"Fusion\"],                  [\"Trance\",\"Former\"],                 [\"Jen\",\"Ayai\"]                ]# 会调用python的sort函数agent.run(f\"\"\"Sort these customers by \\last name and then first name \\and print the output: {customer_list}\"\"\") 自定义时间查询工具from langchain.agents import toolfrom datetime import date# 使用tool修饰自定义函数# 需要详细说明函数的功能及输入输出，LLM根据该说明做出决策@tooldef time(text: str) -&gt; str:    \"\"\"Returns todays date, use this for any \\    questions related to knowing todays date. \\    The input should always be an empty string, \\    and this function will always return todays \\    date - any date mathmatics should occur \\    outside this function.\"\"\"    return str(date.today())agent= initialize_agent(    tools + [time],     llm,     agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,    handle_parsing_errors=True,    verbose = True)"
  },
  
  {
    "title": "深度学习中的激活函数",
    "url": "/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/",
    "categories": "深度学习",
    "tags": "",
    "date": "2023-06-12 00:00:00 +0000",
    





    
    "snippet": "      激活函数    提出时间    主要特点    典型应用        Tanh    1992    曲线为S型，在(-1,1)以外是饱和区域，会导致梯度消失    早期的网络使用，目前很少用        Sigmoid    1995    曲线为S型，在(-1,1)以外是饱和区域，会导致梯度消失    早期的网络使用，目前很少用        ReLU及其衍生    Re...",
    "content": "      激活函数    提出时间    主要特点    典型应用        Tanh    1992    曲线为S型，在(-1,1)以外是饱和区域，会导致梯度消失    早期的网络使用，目前很少用        Sigmoid    1995    曲线为S型，在(-1,1)以外是饱和区域，会导致梯度消失    早期的网络使用，目前很少用        ReLU及其衍生    ReLU    2011    可以避免梯度消失，计算量小收敛快，但可能导致神经元死亡    Yolo3、ResNet        LeakyReLU    2013    与ReLU相比，输入小于0时斜率不为0，可以解决神经元死亡的问题    -        RReLU    2015    输入小于0时斜率从均匀分布中随机选择，没有神经元死亡的问题    -        PReLU    2015    何凯明提出的方法，是ReLU的改进，输入小于0时斜率可学习，没有神经元死亡的问题    -        ELU    2015    输出的平均值接近0，可以加快学习速度，没有神经元死亡的问题    -        Swish    2017    谷歌提出的方法，无上界有下界、平滑、非单调，论文认为比ReLU精度更高    Yolo5、YoloX        Hard-Swish    2019    是Swish的改进，通过硬编码降低计算量    MobileNetV3        Mish    2019    精度较Swish和ReLU均有提升    Yolo4        GeLU    2016    试图在激活函数中同时实现类似自适应dropout的正则化效果，是基于transformer的预训练语言模型的首选激活函数    Bert、GPT-2、RoBERTa、ALBERT        GLU    2017    门控机制的激活函数，能更好捕捉序列数据中的长期依赖关系，在NLP、机器翻译、语音识别等领域取得了良好效果    PaLM        GEGLU    2020    将GLU中的激活函数换成GeLU    T5、GLM-130B        SwiGLU    2020    将GLU中的激活函数换成Swish    PaLM、Llama        Maxout    2013    Goodfellow提出的方法，使用一个单独的层（激活层）来拟合激活函数    -  Tanh和Sigmoid图1. 左：Tanh；右：Sigmoid\\[\\begin{aligned}\\sigma_{tanh}(z)&amp;=\\frac{e^z-e^{-z}}{e_z+e^{-z}}\\\\\\sigma_{sigmoid}(z)&amp;=\\frac{1}{1+e^{-z}}\\end{aligned} \\tag{1}\\]  Tanh【1】的取值范围为(-1,1)，Sigmoid【2】为(0,1)  两者的斜率均小于1梯度消失在误差反向传播时，每层的误差值包含了上面所有层激活函数梯度的乘积，由于Tanh和Sigmoid的斜率均小于1，因此误差值会越来越小，导致底层的权重得不到更新神经元饱和当输入为$(-\\infty,-1)$或$(1,+\\infty)$时，Tanh和Sigmoid的斜率接近0此时即使上层的误差再大，经过该激活函数后都会被极大削减，加剧梯度消失的问题Tanh VS Sigmoid在一些时候Tanh的表现会比Sigmoid好一种理论认为，由于Sigmoid的输出总是大于0，导致同一个神经元的所有权重每次都是一起增大或减小但Tanh的输出由正有负，使得同意神经元的权重可能一部分增加一部分减小，或许更符合实际ReLU及其衍生激活函数ReLU、Leaky ReLU、PReLU、RReLU和ELU图2. (a)ReLU；(b)Leaky ReLU；(c)RReLU；(d)ELU\\[\\begin{aligned}\\sigma_{relu}(z)&amp;=\\begin{cases}z&amp;z&gt;0\\\\0&amp;z\\le0\\end{cases}\\\\\\sigma_{leakyrelu}(z)&amp;=\\begin{cases}z&amp;z&gt;0\\\\\\alpha z&amp;z\\le0\\end{cases}\\\\\\sigma_{elu}(z)&amp;=\\begin{cases}z&amp;z&gt;0\\\\\\alpha(e^z-1)&amp;z\\le0\\end{cases}\\end{aligned} \\tag{2}\\]ReLU【3】的梯度只可以取0或1，因此误差反向传播时的梯度乘积不会有逐层衰减的缺点，这样就避免了梯度消失的问题此外相较于Tanh和Sigmoid，ReLU的计算量更小，速度更快单侧饱和ReLU是单侧饱和的（对于小于0的输入，输出为0），可能会引发神经元坏死，但作者认为这种网络的稀疏化其实是有积极意义的：1）正则化当输入为负梯度时，在当前层的这个ReLU被置0，因为无法更新，该神经元的输出有可能一直为0，此时可以认为该神经元处于坏死状态不妨回忆一下dropout的原理，神经元坏死带来的网络稀疏化可以起到正则化的效果2）降低计算量神经元坏死会造成实际网络规模的缩小，从而降低计算量3）增强对噪声鲁棒性以CV卷积为例，神经元卷积的输出越大意味着其对应的特征越明显，如果卷积的输出为0则表示没检测到对应的特征从这个角度看，将卷积输出为负的情况全部映射为0则从理论上也有了合理性（没检测到就是没检测到，不需要再区分其强度）可以推断，卷积输出为负可能是检测到了背景噪声或其他特征信息，会潜在引入神经元之间的相关性，导致特征检测的相互干扰ReLU的改进虽然ReLU认为神经元坏死有一定积极意义，但其实际上还是降低了网络的容量  Leaky ReLU【4】给负输入一个恒定的非0斜率$\\alpha$（一般取0.01），解决了神经元坏死的情况（但后来的实验表明与ReLU相比，LReLU并没有带来明显的精度提升）  PReLU【5】干脆将斜率$\\alpha$定义为可学习的参数  RReLU【6】则在均匀分布（需要指定上下限）中随机取值给斜率$\\alpha$赋值  ELU【7】对于负输入使用指数函数做映射，使得整体的输出均值接近0且处处平滑，从而提高模型收敛的速度，但计算量有所提升Swish、Hard-Swish和Mish图3. 左：Swish(b=1)；中：Hard-Swish；右：Mish\\[\\begin{aligned}\\sigma_{swish}(z)&amp;=z*sigmoid(\\beta z)\\\\&amp;=\\frac{z\\beta}{1+e^{-z}}\\\\\\sigma_{hswish}(z)&amp;=\\begin{cases}0&amp;z\\le-3\\\\z&amp;z\\geq 3\\\\z(z+3)/6&amp;otherwise \\end{cases}\\\\\\sigma_{mish}(z)&amp;=z*Tanh(Softplus(z))\\\\&amp;=z*Tanh(log(1+e^z))\\end{aligned} \\tag{3}\\]SwishSwish【8】在PyTorch中称为SiLU，是谷歌2017年提出的激活函数在其论文中，通过简单替换ReLU，Mobile NASNetA和Inception-ResNet-v的分类准确度分别提高了0.9%和0.6%但围绕Swish也存在争论，因为其效果是不稳定的，并不是在所有模型中都比ReLU好Hard-SwishHard-Swish【9】是Swish的改进，主要有2点贡献：  发现Swish只有在深层网络中才能发挥作用  实现了一个硬编码的表达式，相较于Swish计算量更小Mish从图形上看，Mish和Swish非常接近在一个使用Yolo4为基础做的消融实验中，Mish相比Swish提升0.494%，相比ReLU提升1.671%Mish认为这得益于其几乎所有点都有的平滑度式3中Mish使用到的Softplus是对ReLU的光滑近似，其表达式和图形分别为\\[Softplus(x)=\\frac{1}{\\beta}log(1+e^{(\\beta*x)}) \\tag{4}\\]图4. SoftplusGeLU对于一个神经元的输入，ReLU会确定性的乘以一个0或1，Dropout则是随机乘以0GeLU【11】也是将输入乘以0或1，但实际中是乘以0还是1是取决于输入的分布：当输入较小时以较大概率将其置0（参考ReLU的曲线）在实现上，由于Batch Norm的操作，输入神经元的数据可以认为遵循正态分布因此GeLU将输入通过一个基于正态分布的累积分布函数$\\Phi(x)$，将其与原始输入相乘作为激活函数的输出\\[\\begin{aligned}\\sigma_{gelu}(z)&amp;=z\\Phi(z)\\\\&amp;=z\\frac{1+erf(z/\\sqrt{2})}{2}\\end{aligned} \\tag{5}\\]其中$erf(x)$是正态分布累积分布函数的误差函数表达方式，论文【11】中给出了分别基于Sigmoid和Tanh的近似（PyTorch中使用的是基于Tanh的近似）\\[\\begin{aligned}\\sigma_{gelu}(z)&amp;=z\\Phi(z)\\\\&amp;\\approx z*sigmoid(1.702z)\\\\&amp;\\approx 0.5*z*[1+Tanh(\\sqrt{\\frac{2}{\\pi}}(z+0.044715z^3))]\\end{aligned} \\tag{6}\\]图5. 左：正态分布的累积函数（wikipedia）；右：GeLU与其他激活函数的对比此外，参考式3的Swish和基于Sigmoid近似的GeLU表达式，可以发现两者形式非常接近，此时可以认为GeLU是Swish的一个特例GLU、GEGLU、SwishGLU\\[\\begin{aligned}\\sigma_{glu}(z)&amp;=(zV+c)\\otimes \\sigma_{sigmoid}(zW+b)\\\\\\sigma_{geglu}(z)&amp;=(zV+c)\\otimes \\sigma_{gelu}(zW+b)\\\\\\sigma_{swiglu}(z)&amp;=(zV+c)\\otimes \\sigma_{siwsh}(zW+b)\\end{aligned} \\tag{7}\\]上式的$\\otimes$表示逐元素相乘对于GLU【12】  首先对中间向量g(z)做门控操作，即使用Sigmoid函数将其映射到[0,1]，表示每个元素被保留的概率；  然后将另一个中间向量v(z)与门控后的向量做逐元素相乘，得到“加权”后的输出上述这种门机制使得GLU可以有选择性的过滤输入向量的某些部分，根据输入的上下文决定哪些部分应该被保留，哪些应该被抑制GEGLU和SwiGLU【13】是Google对GLU的扩展，将其中的Sigmoid函数替换为GeLU和Swish，在T5中取得了显著的性能提升# Llama中的SwiGLUself.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)# self.act_fn(self.gate_proj(x))对应式7逐元素相乘运算的右部分# self.up_proj(x)对应左部分down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))Maxout一般的隐含层神经元$i$的输出为$h_i(x)=\\sigma(\\sum_{j=0}^nx^TW_{ij}+b_i)$，其中$\\sigma$是激活函数，$n$表示上一层的神经元数量Maxout【14】在上一层和当前层每一个神经元的连接中插入一个隐含层，输出取隐含层各神经元的最大值图6. Maxout层参考图6，上一层有2个神经元，当前层有4个神经元，对于上一层与当前层的第2个神经元的连接（紫色圈），在其中间插入k个神经元组成的隐含层（图中k取5）最终的输出取这k个神经元输出的最大值Maxout理论上可以拟合任意凸函数比如设k=2，那么有$h_i(x)=max(w_1^Tx+b_1, w_2^Tx+b_2)$，当$w_1$和$b_1$取0时就是ReLUMaxout拥有ReLU的所有优点（线性、不饱和），没有它的缺点（神经元死亡），但会导致整个网络参数量的激增如何选择激活函数这是个尚无定论的话题，通常还是通过消融实验去确定。不过有一些稳妥的小tips  通常不建议在一个网络中使用多种激活函数  通常认为有能力生成0均值分布、曲线光滑的激活函数更优  无论是cv还是nlp，优先选择ReLU。需要小心调整lr以避免出现过多死亡神经元  对于cv，如果ReLU出现了过多死亡神经元导致效果不理想，可以尝试LeakyReLU、ELU和Maxout  对于nlp，目前流行的选择是GeLU和GEGLU文献【1】Kalman B L, Kwasny S C. Why tanh: choosing a sigmoidal function[C]//[Proceedings 1992] IJCNN International Joint Conference on Neural Networks. IEEE, 1992, 4: 578-581.【2】Han J, Moraga C. The influence of the sigmoid function parameters on the speed of backpropagation learning[C]//From Natural to Artificial Neural Computation: International Workshop on Artificial Neural Networks Malaga-Torremolinos, Spain, June 7–9, 1995 Proceedings 3. Springer Berlin Heidelberg, 1995: 195-201.【3】Glorot X, Bordes A, Bengio Y. Deep sparse rectifier neural networks[C]//Proceedings of the fourteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference Proceedings, 2011: 315-323.【4】Maas A L, Hannun A Y, Ng A Y. Rectifier nonlinearities improve neural network acoustic models[C]//Proc. icml. 2013, 30(1): 3.【5】He K, Zhang X, Ren S, et al. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1026-1034.【6】Xu B, Wang N, Chen T, et al. Empirical evaluation of rectified activations in convolutional network[J]. arXiv preprint arXiv:1505.00853, 2015.【7】Clevert D A, Unterthiner T, Hochreiter S. Fast and accurate deep network learning by exponential linear units (elus)[J]. arXiv preprint arXiv:1511.07289, 2015.【8】Ramachandran P, Zoph B, Le Q V. Searching for activation functions[J]. arXiv preprint arXiv:1710.05941, 2017.【9】Howard A, Sandler M, Chu G, et al. Searching for mobilenetv3[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 1314-1324.【10】Misra D. Mish: A self regularized non-monotonic activation function[J]. arXiv preprint arXiv:1908.08681, 2019.【11】Hendrycks D, Gimpel K. Gaussian error linear units (gelus)[J]. arXiv preprint arXiv:1606.08415, 2016.【12】Dauphin Y N, Fan A, Auli M, et al. Language modeling with gated convolutional networks[C]//International conference on machine learning. PMLR, 2017: 933-941.【13】Shazeer N. Glu variants improve transformer[J]. arXiv preprint arXiv:2002.05202, 2020.【14】Goodfellow I, Warde-Farley D, Mirza M, et al. Maxout networks[C]//International conference on machine learning. PMLR, 2013: 1319-1327."
  },
  
  {
    "title": "Python中的协程",
    "url": "/posts/Python%E4%B8%AD%E7%9A%84%E5%8D%8F%E7%A8%8B/",
    "categories": "python",
    "tags": "",
    "date": "2023-06-10 00:00:00 +0000",
    





    
    "snippet": "            类别      基本概念      资源      调度方式      数据同步      适用场景                  进程      是CPU资源分配的基本单位，一个进程包含多个线程      拥有自己独立的资源空间      操作系统自动调度      需要锁      计算密集型              线程      是程序执行的最小单位  ...",
    "content": "            类别      基本概念      资源      调度方式      数据同步      适用场景                  进程      是CPU资源分配的基本单位，一个进程包含多个线程      拥有自己独立的资源空间      操作系统自动调度      需要锁      计算密集型              线程      是程序执行的最小单位      一个进程内的线程共享进程的内存空间（代码段、数据集、堆等）      操作系统自动调度      需要锁      IO密集型              协程      在一个线程上实现程序并发      拥有自己的寄存器上下文和栈      用户调度      不需要锁      IO密集型      协程(Coroutine)的上下文切换开销远低于进程和线程，也不需要考虑原子锁和同步，一个CPU可以轻松支持上万的协程，很适合IO密集型的高并发任务（如socket通信）图1. 协程的异步执行模型如图1所示，在协程中多个任务被编排在一个线程内执行（因为某一时刻只有一个任务在执行，因此不需要考虑数据同步的问题），由用户决定当前任务是否挂起和执行比如对于任务B需要任务A的输出，可以先挂起任务B，执行任务A，然后再执行任务B基本用法import asyncio async def async_task(name):    print(f\"任务{name} 开始执行\")    await asyncio.sleep(1)    print(f\"任务{name} 执行结束\")    return f\"返回结果{name}\" loop = asyncio.get_event_loop()result = loop.run_until_complete(async_task(1))print(result)程序输出为任务1 开始执行任务1 执行结束返回结果1执行协程函数需要首先创建/获取一个事件循环（依附在一个线程中一直运行，直到手动调用close方法），然后将协程函数放入其中运行  使用关键字async修饰称为协程函数（类型为&lt;class 'coroutine'&gt;），该函数无法被直接调用，其他协程函数调用时需要添加await关键字  await表示程序运行到这里被挂起，下一次再执行该函数时会从该位置开始继续（参考yield的效果）  loop就是一个事件循环，其类型为&lt;class 'asyncio.unix_events._UnixSelectorEventLoop'&gt;  这里通过loop.run_until_complete方法将协程函数放入事件循环中需要注意的是，在协程中要避免使用time.sleep，因为该方法会阻塞整个线程（此时线程内所有的协程函数都无法运行）Task类在事件循环中其实调用的不是`coroutine`类，而是`Task`类，在上例中loop.run_until_complete将async_task(1)隐式转成了Taskimport asyncio async def async_task(name):    print(f\"任务{name} 开始执行\")    await asyncio.sleep(1)    print(f\"任务{name} 执行结束\")    return f\"返回结果{name}\"loop = asyncio.get_event_loop()# 显式转换为Tasktask = asyncio.ensure_future(async_task(1))# task = loop.create_task(async_task(1))result = loop.run_until_complete(task)print(result)如上所示，可以通过方法asyncio.ensure_future或loop.create_task将coroutine类的协程函数转换为Task类事件循环asyncio提供了4个与事件循环相关的函数，可以实现获取、设置或创建的功能1）asyncio.get_running_loop()返回当前线程中正在运行的事件循环，如果没有会触发RuntimeError2）asyncio.set_event_loop(loop)将loop设为当前线程的当前事件循环3）asyncio.new_event_loop()创建一个新的事件循环4）asyncio.get_event_loop()只能够在主线程中使用，获取线程中当前事件循环，如果没有设置则创建一个新的事件循环并设为当前事件循环import asyncioimport threading async def async_task(name):    print(f\"任务{name} 开始执行\")    await asyncio.sleep(1)    print(f\"任务{name} 执行结束\")    return f\"返回结果{name}\" def test_thread():    # 会报错：There is no current event loop in thread 'Thread-1'    # loop = asyncio.get_event_loop()    # 正常运行    loop = asyncio.new_event_loop()    asyncio.set_event_loop(loop)    task = asyncio.ensure_future(async_task(1))    result = loop.run_until_complete(task)    print(result)async_thread = threading.Thread(target=test_thread)async_thread.start()多任务并发使用协程是为了实现多个任务的并发，上面的例子仅实现了一个协程任务的运行可以使用asyncio.wait和asyncio.gather实现多任务的编排  asyncio.wait：输入为协程函数组成的列表，输出是各任务执行的详细结果及函数返回  asyncio.gather：输入为不定参数，输出是各任务执行的函数返回import asyncioimport threading async def async_task(name):    print(f\"任务{name} 开始执行\")    await asyncio.sleep(1)    print(f\"任务{name} 执行结束\")    return f\"返回结果{name}\" loop = asyncio.get_event_loop()# wait方法tasks = asyncio.wait([async_task(2),async_task(1),async_task(3)])'''任务2 开始执行任务1 开始执行任务3 开始执行任务2 执行结束任务1 执行结束任务3 执行结束({&lt;Task finished coro=&lt;async_task() done, defined at test.py:4&gt; result='返回结果2'&gt;, &lt;Task finished coro=&lt;async_task() done, defined at test.py:4&gt; result='返回结果1'&gt;, &lt;Task finished coro=&lt;async_task() done, defined at test.py:4&gt; result='返回结果3'&gt;}, set())'''# gather方法tasks = asyncio.gather(*[async_task(1), async_task(3), async_task(2)])'''任务1 开始执行任务3 开始执行任务2 开始执行任务1 执行结束任务3 执行结束任务2 执行结束['返回结果1', '返回结果3', '返回结果2']'''result = loop.run_until_complete(tasks)print(result)"
  },
  
  {
    "title": "【PLM】（4）GLM-6B与GLM-130B模型分析",
    "url": "/posts/PLM-4-GLM-6B%E4%B8%8EGLM-130B%E6%A8%A1%E5%9E%8B%E5%88%86%E6%9E%90/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2023-05-31 00:00:00 +0000",
    





    
    "snippet": "GLM是清华大学2022年提出的预训练通用语言模型，分为GLM-6B【1】和GLM-130B【2】两个版本其中GLM-130B是2022年11月斯坦福大模型中心的评测中亚洲唯一入选的大模型，评测报告显示其在准确性和恶意性指标上与GPT-3接近以GLM-6B为基座，清华大学与智谱AI推出了ChatGLM-6B，在伯克利主导的大语言模型排行榜中位列14图1. 截止2023.5.31ChatGLM...",
    "content": "GLM是清华大学2022年提出的预训练通用语言模型，分为GLM-6B【1】和GLM-130B【2】两个版本其中GLM-130B是2022年11月斯坦福大模型中心的评测中亚洲唯一入选的大模型，评测报告显示其在准确性和恶意性指标上与GPT-3接近以GLM-6B为基座，清华大学与智谱AI推出了ChatGLM-6B，在伯克利主导的大语言模型排行榜中位列14图1. 截止2023.5.31ChatGLM-6B的排行榜GLM-6B官方代码地址：https://github.com/THUDM/GLM当前的预训练模型结构可以分为以下三种            模型结构      代表模型      擅长领域                  自回归      GPT      文本生成              自编码      BERT      自然语言理解              编码-解码      T5      限定条件的生成，如文本总结      GLM试图用一个模型解决上述3种领域的需求（在此之前已有相关的研究试图实现统一【3】【4】，但效果不佳）为了实现上述目标，GLM将NLU也转化为生成任务：将其转为包含任务描述的完形填空问题，从而使用自回归方式生成答案  在SuperGLUE上比BERT精度提升4.6%-5%  在158G的语料库上训练，比RoBERTa和BART效果好  基于更少的参数量和训练数据，在NLU和生成任务上比T5效果更好模型结构GLM的基本单元为transformer编码结构，做了3个小的修改  修改了layer norm和resdual的顺序（参考NVIDIA论文【5】，可以避免并行计算时大模型中的数值误差问题）  使用一个单独的线性层做token的预测  将ReLU激活函数换成GeLUs（参考论文【6】）图2. 左：原始transformer编码结构；右：修改后的结构如图2所示，原始的transformer结构是在resdual后做layer norm，修改后的是先做layer norm后做resdualTransformers中GPT-2的实现也使用了图2右的结构模型预训练如上所述，GLM将NLU也视为生成任务，从而能够使用自回归模型实现三种任务的兼容GLM在预训练阶段提出了一种称为autoregressive blank infilling的方法，主要包含以下3块  输入语句的掩码处理  二维位置编码  自注意力掩码矩阵图3. 左：输入的掩码处理；中：二维位置编码；右：自注意力掩码矩阵输入的掩码处理GLM将输入语句随机抠一些spans，每个span包含1个或多个连续的tokens  GLM将原始语句中的这些spans用[MASK]代替，形成如图3(b)所示的Part A  将这些spans拼接在一起形成图3(b)所示的Part B（同时添加起始标志[START]）注意，为了让模型学习spans之间的逻辑关系，Part B中各个span的顺序会被随机打乱（参考图3(c)中x5x6与x3的位置）在训练时，为了兼顾模型的NLU能力和文本生成能力，GLM设计了3种方案            目标能力      训练方案                  NLU能力      对输入序列以泊松分布随机采样，直到全部tokens的15%被mask掉              长文本生成能力      对输入序列只均匀采样1个span，但该span的长度为全部tokens的50%-100%              seq2seq完整语句或段落生成能力      限制1个span必须为一个完整的句子，所有spans的长度占输入序列tokens的15%      注意：一个输入序列可能包含多个完整的句子二维位置编码如图3(c)GLM会将上一节生成的Part A和Part B拼接为一个序列作为模型的输入同时GLM设计了2种位置编码1）position 1表示每个token在原始序列中的绝对位置（从1开始计数）2）position 2表示Part B中每个token在其内部序列的绝对位置（从1开始计数）Part A的每个token则设为0，因为其位置关系在position 1中已有体现自注意力掩码矩阵如图3(d)所示，GLM实际上融合了decode和encode的自注意力掩码规则  Part A内部：使用decode的自注意力，不设置掩码，各token可以互相看见  Part B：使用encode的自注意力，token只能看到左边的（已经生成的）其他token该设置的目的是希望同时建立NLU（对应decode）的能力和NLG（对应encode）的能力模型fine-tuning1）对于NLU任务借鉴PET【7】的思路，通过设计Prompt将NLU问题建模为生成问题图4. 情感分类任务的转换如图4所示的情感分类任务，GLM在原始输入后面增加It is really [MASK]，引导模型生成带情感判断的token2）对于NLG任务以图3(b)中的Part A为输入，让GLM以自回归的方式生成Part B与其他模型的对比以各模型适配下游blank填充任务的能力为对比标准            模型名称      对比情况                  BERT【8】      BERT无法捕捉被masked的tokens之间的关联，无法填充多个blank              XLNet【9】      使用了与GLM一致的自回归结构，但其仅使用了blank之前的位置编码，不能获得答案的长度；此外XLNet的双自注意力机制增加了预训练阶段的耗时              T5【10】      是一个编码-解码架构的模型，但在编码和解码阶段分别使用了独立的位置编码；在预训练阶段使用多种哨兵token区分不同的blank，但在下游任务只用了一种哨兵token，造成了一定程度的浪费和gap              UniLM【11】      是一个自编码结构模型，通过修改注意力掩码增强功能，捕捉spans和上下文关联的能力比较弱      图5. 在SuperGLUE评估集的表现图5是GLM在SuperGLUE评估集上的精度对比，文章分别以BERT和RoBERTa为比较基准1）基准BERT使用与BERT一致的预训练数据（BooksCorpus和English Wikipeida）、一致的分词器（wordpiece）、一致的vocabulary（30K）训练了对标（模型参数量对应一致）$BERT_{Base}$和$BERT_{Large}$的$GLM_{Base}$和$GLM_{Large}$其中$GLM_{Doc}$和$GLM_{Sent}$分别采用[输入掩码处理][###输入掩码处理]中的“长文本生成能力”和“seq2seq完整语句或段落生成能力”训练方案$GLM_{410M}$和$GLM_{515M}$就是用来刷榜的2）基准RoBERTa使用与RoBERTa【12】一致的训练数据、分词器和超参数，训练了模型$GLM_{RoBERTa}$图6. 各GLM模型的训练超参数代码实践测试代码来自hugging face: https://huggingface.co/THUDM/glm-10b-chinese/tree/main以下是基于上述文件编写的文本生成推理代码class generator:    def __init__(            self,            token_cfg_dir,            model_cfg,            weight_file,            device,            max_sentence=512            ):        self._max_sentence = max_sentence        self._device = device        self.build_tokenizer(token_cfg_dir)        self.build_model(model_cfg, weight_file)    def build_tokenizer(self, dir_cfg):        logger.info('making tokenizer...')        # 扫描路径下的tokenizer_config.json、vocab_file（这里是cog-pretrain.model）、added_tokens.json        # 将added_tokens.json中的内容通过函数add_tokens补充到vocab_file内        self._tokenizer = GLMChineseTokenizer.from_pretrained(dir_cfg)    def build_model(self, cfg, pth_file):        logger.info('making model config...')        with open(cfg, 'r') as f:            config = json.load(f)        config['is_decoder'] = True        config['add_cross_attention'] = True        model_cfg = GLMConfig.from_dict(config)        logger.info('loading model...')        state_dict = torch.load(pth_file, map_location=torch.device('cpu'))        self._model = GLMForConditionalGeneration(model_cfg)        self._model.glm.load_state_dict(state_dict)        self._model = self._model.to(self._device)        self._model.eval()    def do_tokenizer(self, sentence):        # 1.对sentence分词        # 2.在句首加[CLS],在句末加&lt;|endoftext|&gt;        # 3.按词本映射到id        inputs = self._tokenizer(sentence, return_tensors=\"pt\")        # 1.按论文图2(c)生成position1和position2        # 2.按论文图2(d)生成attention mask        # 3.在句末添加&lt;|startofpiece|&gt;，指示模型生成内容的起始位置        inputs = self._tokenizer.build_inputs_for_generation(            inputs, max_gen_length=self._max_sentence)        inputs = {key: value.to(self._device) for key, value in inputs.items()}        return inputs    def do_generate(self, sentence):        logger.info('do generating...')        inputs = self.do_tokenizer(sentence)        # 初始输入：[CLS]xxxx[MASK]xxx&lt;|endoftext|&gt;&lt;|startofpiece|&gt;        # 将输入forward进GLM，生成的token补充到输入的后面        # 重复上述过程，直到生成的token是&lt;|endofpiece|&gt;或达到最大长度        outputs = self._model.generate(            **inputs, max_length=self._max_sentence,             pad_token_id=self._tokenizer.eop_token_id,            eos_token_id=self._tokenizer.eop_token_id)        list_outputs = self._tokenizer.decode(outputs[0].tolist())        return list_outputs模型会从&lt;|startofpiece|&gt;开始生成文本，最后以&lt;|endofpiece|&gt;结束gen = generator(DIR_CFG, FILR_MODEL_CFG, PATH_WEIGHTS, device, max_sentence=512)result = gen.do_generate(        \"凯旋门位于意大利米兰市古城堡旁。1807年为纪念[MASK]而建，门高25米，顶上矗立两武士青铜古兵车铸像。\"        )# result: # [CLS] 凯旋门位于意大利米兰市古城堡旁。1807年为纪念 [MASK] 而建,门高25米,顶上矗立两武士青铜古兵车铸像。 &lt;|endoftext|&gt; &lt;|startofpiece|&gt; 拿破仑军队攻克米兰城 &lt;|endofpiece|&gt;GLM-130BGLM-130B是清华团队和智谱AI合作推出的千亿级中英双语开源模型  1300亿参数的中英双语预训练模型  在英语评测基准（LAMBADA、BIG-bench-lite、MMLU）中超越了GPT-3和参数量相当的OPT-175B、BLOOM-176B  在中文评测基准（CLUE、FewCLUE）中超越了目前最大的中文语言模型ERNIE TITAN3.0 260B  做了INT4量化，可以在4张RTX3090或8张RTX2080Ti上实现有效推理图7. 上：英语评测基准的对比；下：中文评测基准的对比官方代码地址：https://github.com/THUDM/GLM-130BGLM-130B【2】文章作为一篇技术报告主要从工程方法、模型设计选择、确保有效性和稳定性的训练策略、模型量化展开讨论模型设计选择GLM-130B以GLM-6B的结构为backbone（更准确的说是沿用了GLM提出的如图2所示的autoregressive blank infilling方案）从官方代码的config中可以看到网络的核心参数            num-layers      hidden-size      vocab-size      num_attention-heads                  70      12288      150528      96      根据《【PLM】（1）BERT模型-原理》总结的公式1可以验证其参数量\\[\\begin{aligned}Param_{GLM-130B}&amp;=150528*12288+12*12288^2*70\\\\&amp;=128.7B\\end{aligned} \\tag{1}\\]在GLM-6B的基本思想上，GLM-130B做了如下3个修改对blank的类别做扩充在GLM-6B中，每个blank用[MASK]标记为了进一步对标记做区分，GLM-130B将其扩充为[MASK]和[gMASK]  [MASK]：表示比较短的blank，以一个固定的比例对输入序列做掩码；主要针对NLU任务，此时GLM-130B类似BERT和T5  [gMASK]：表示比较长的blank，在每个句子的末尾抠随机长度的长子序列做掩码；主要针对NLG任务，此时GLM-130B类似PrefixLM位置编码方案和MLP的修改GLM-6B提出了二维绝对位置编码方案（图2(c)）GLM-130B通过消融实验，使用了一维相对位置编码RoPE【13】 + GeGLU激活方案图8. GLM-130B使用RoPE+GeGLU的方案其中GeGLU是门控激活GLU和GeLU的组合方案参考《GLU激活函数：PaLM模型中SwiGLU激活函数的原型》，GLU的定义为\\[GLU(x)=x\\otimes \\sigma(g(x)) \\tag{2}\\]其中$x$是输入，$\\sigma$是激活函数，$g(x)$表示mlp，$\\otimes$表示逐元素乘积GLM-130B的GeGLU方案将激活函数$\\sigma$设定为GeLU，将式2简化为\\[\\begin{aligned}x1,x2&amp;=chunk(mlp(x))\\\\GeGLU(x)&amp;=x1\\otimes ReLU(x2)\\end{aligned} \\tag{3}\\]其中$chunk$操作是将输入均分为两部分class GEGLU(torch.nn.Module):    def __init__(self):        super().__init__()        self.activation_fn = F.gelu    def forward(self, x):        # dim=-1 breaks in jit for pt&lt;1.10        x1, x2 = x.chunk(2, dim=(x.ndim - 1))        return x1 * self.activation_fn(x2)我们知道，在标准transormer的MLP中，会将输入的维度$H$首先扩展到$4H$，经过激活函数后再映射回$H$# 标准transformer的MLP过程def forward(self, hidden_states):    # [b, s, 4hp]    intermediate_parallel = self.dense_h_to_4h(hidden_states)    intermediate_parallel = gelu(intermediate_parallel)    # [b, s, h]    output = self.dense_4h_to_h(intermediate_parallel)    output = self.dropout(output)    return output这个MLP过程的可学习参数量为$H* 4H+4H* H=8H^2$在GLM-130B中，由于使用了GeGLU，为了确保MLP过程的参数量与标准MLP的一致，将$4H$修改为$\\frac{8}{3}H$:  升维：H -&gt; 2*8/3H  激活：2*8/3H -&gt; 8/3H（内部会做均分操作，再对两者做乘积）  降维：8/3H -&gt; H可以计算该过程的可学习参数量：$H* \\frac{16}{3}H+\\frac{8}{3}H* H=8H^2$Layer Normalization的修改合适的normalization是确保模型能够稳定训练的关键GLM-130B对比了Pre-LN、Post-LN和Sandwich-LN其中Pre-LN和Post-LN分别对应图2右边和左边的结构，区别点是将Layer-norm放在resdual之前还是之后一般认为Pre-LN比Post-LN容易训练，但Post-LN能取得更好的效果文章《为什么Pre Norm的效果不如Post Norm？》和《如何评价微软亚研院提出的把 Transformer 提升到了 1000 层的 DeepNet？》对此进行了分析，认为Pre-LN其实是扩展了网络的宽度，实际深度并没有设计的那么深。而我们现在知道，提高网络的深度比提高宽度效果更好：1）对Pre-LN，有$x_{n+1}=x_n+f(norm(x_n))$其中第二项的方差由于有norm是不随层数变化的，于是x的方差会在主干上随层数积累。到了深层以后，单层对主干的影响可以视为小量，而不同层的f统计上是相似的，因此有\\[\\begin{aligned}x_{n+2}&amp;=x_{n+1}+f(norm(x_{n+1}))\\\\&amp;=x_n+f(norm(x_n))+f(norm(x_{n+1}))\\\\&amp;\\approx x_n+2f(norm(x_n))\\end{aligned} \\tag{4}\\]2）而对于Post-LN，有$x_{n+1}=f(x_n+norm(x_n))$保证了主干方差的恒定，每层对x都有较大影响，代价是模型结构没有从头到尾的恒等路径，因此梯度难以控制文章通过实验，最终采用Post-LN+DeepNorm【14】的方式（需要注意的是GLM-6B使用的是Pre-LN）\\[DeepNorm(x)=LayerNorm(\\alpha*x+Network(x)) \\tag{5}\\]其中$\\alpha=\\sqrt{2N}$，$N$表示网络的层数图9. Post-LN+DeepNorm在训练过程中梯度的变化训练策略分词器GLM-130B基于icetk做修改实现文本分词icetk是一个支持图像和文本的分词器，使用25G中英双语，基于sentencepiece训练得到，词本大小为150000GLM-130B忽略了图像tokens（前20000），添加了[MASK]、[gMASK]、&lt;sop&gt;、&lt;eop&gt;、&lt;eos&gt;等哨兵token预训练数据GLM-130B在预训练中使用了2类数据1）服务于自监督blank infilling（占所有预训练数据的95%）数据来源包含中英文语料，两者数量均衡  Pile【15】英文语料，1.2T  WuDao【16】中文语料，1T  来自互联网的中文语料，250G数据的30%被作为[MASK]类型的spans，每个span由连续的tokens组成，其长度分布遵循泊松分布数据的70%被作为[gMASK]类型的spans，以每句话为单位，其末尾被[gMASK]掩码掉，不同的span长度遵循均匀分布2）服务于多任务instruction的预训练（占所有预训练数据的5%）T5【10】认为在预训练中进行多任务学习比之后做fine-tuning更有效GLM也构建了instruction数据集，包含了NLU、NLG、信息提取任务，训练数据只占整个数据集的5%，以避免破坏模型的通用能力训练参数及策略训练参数：  序列长度2048。对[gMASK]上下文窗口长度2048，对[MASK]上下文窗口长度512，每次用4个样本拼接  batch size。在前2.5%的样本训练时从192增长至4224  优化器。使用AdamW，b1、b2分别为0.9,0.95，weight decay 为0.1  学习率。前0.5%样本从10-7到8*10-5，之后开始降低GLM-130B使用DeepSpeed框架【17】实现的PipeDream-Flush方法做训练在训练过程中采用混合精度：除了优化器状态和权重参数使用fp32，其他均使用fp16GLM-130B分析了训练过程中的不稳定（震荡）问题，并提出了EGS方法损失震荡的问题定位1）如果使用Pre-LN，在深的层中数值会变得很大GLM-130B使用Post-LN+DeepNorm解决2）注意力得分超过了fp16能表达的范围BLOOM-176B使用BF16代替fp16，但会多占用15%的显存，并且这种数据格式不能被所有GPU架构支持对此GLM-130B提出了EGSEGS(Embedding Layer Gradient Shrink)简单来说就是对word embedding做平滑\\[word\\_embedding=word\\_embedding*\\alpha+word\\_embedding*(1-\\alpha) \\tag{5}\\]其中$\\alpha=0.1$图10. 左：不同层的梯度变化；右：使用EGS对训练误差走势的影响文献【1】Du Z, Qian Y, Liu X, et al. GLM: General language model pretraining with autoregressive blank infilling[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 320-335.【2】Zeng A, Liu X, Du Z, et al. Glm-130b: An open bilingual pre-trained model[J]. arXiv preprint arXiv:2210.02414, 2022.【3】Dong L, Yang N, Wang W, et al. Unified language model pre-training for natural language understanding and generation[J]. Advances in neural information processing systems, 2019, 32.【4】Bao H, Dong L, Wei F, et al. Unilmv2: Pseudo-masked language models for unified language model pre-training[C]//International conference on machine learning. PMLR, 2020: 642-652.【5】Shoeybi M, Patwary M, Puri R, et al. Megatron-lm: Training multi-billion parameter language models using model parallelism[J]. arXiv preprint arXiv:1909.08053, 2019.【6】Hendrycks D, Gimpel K. Gaussian error linear units (gelus)[J]. arXiv preprint arXiv:1606.08415, 2016.【7】Schick T, Schütze H. Exploiting cloze questions for few shot text classification and natural language inference[J]. arXiv preprint arXiv:2001.07676, 2020.【8】Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.【9】Yang Z, Dai Z, Yang Y, et al. Xlnet: Generalized autoregressive pretraining for language understanding[J]. Advances in neural information processing systems, 2019, 32.【10】Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. The Journal of Machine Learning Research, 2020, 21(1): 5485-5551.【11】Dong L, Yang N, Wang W, et al. Unified language model pre-training for natural language understanding and generation[J]. Advances in neural information processing systems, 2019, 32.【12】Liu Y, Ott M, Goyal N, et al. Roberta: A robustly optimized bert pretraining approach[J]. arXiv preprint arXiv:1907.11692, 2019.【13】Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021.【14】Wang H, Ma S, Dong L, et al. Deepnet: Scaling transformers to 1,000 layers[J]. arXiv preprint arXiv:2203.00555, 2022.【15】Gao L, Biderman S, Black S, et al. The pile: An 800gb dataset of diverse text for language modeling[J]. arXiv preprint arXiv:2101.00027, 2020.【16】Yuan S, Zhao H, Du Z, et al. Wudaocorpora: A super large-scale chinese corpora for pre-training language models[J]. AI Open, 2021, 2: 65-68.【17】Rasley J, Rajbhandari S, Ruwase O, et al. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters[C]//Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. 2020: 3505-3506."
  },
  
  {
    "title": "【PLM】（3）自编码自回归 & GPT-1、2、3模型",
    "url": "/posts/PLM-3-%E8%87%AA%E7%BC%96%E7%A0%81%E8%87%AA%E5%9B%9E%E5%BD%92-&-GPT-1-2-3%E6%A8%A1%E5%9E%8B/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2023-05-17 00:00:00 +0000",
    





    
    "snippet": "GPT和BERT都是基于transformer架构的模型，但分别走了不同的技术路线  自编码：BERT使用transformer的编码模块，可以实现双向注意力  自回归：GPT使用transformer的解码模块，只能从左到右实现单向注意力图1. BERT和GPT的发展时间线            模型      技术路线      参数量（最大）      数据集大小      分词方法 ...",
    "content": "GPT和BERT都是基于transformer架构的模型，但分别走了不同的技术路线  自编码：BERT使用transformer的编码模块，可以实现双向注意力  自回归：GPT使用transformer的解码模块，只能从左到右实现单向注意力图1. BERT和GPT的发展时间线            模型      技术路线      参数量（最大）      数据集大小      分词方法      亮点                  GPT-1      解码、单向注意力      1亿      5GB      BPE      提出预训练+微调的路线，在12个子任务中均取得9项SOTA              BERT      编码、双向注意力      3.4亿      33亿个词      WordPiece      精度超越GPT-1，在GLUE所有子任务均取得SOTA              GPT-2      解码、单向注意力      15亿      40GB      BBPE      精度没有显著超越BERT，卖点在zero-shot              GPT-3      解码、单向注意力      1750亿      45TB      -      能够生成真假难辨的文章      推荐阅读  《Hugging Face中GPT2模型应用代码》  《BPE 算法原理及使用指南【深入浅出】》  《浅谈Byte-Level BPE》  《GPT/GPT2/GPT3/ChatGPT梳理》  《预训练语言模型之GPT-1，GPT-2和GPT-3》  《[论文解读]北大综述In-context Learning学习分享》自编码与自回归自编码和自回归在模型结构上和学习方式上均有差别，最终导致两者适合的下游任务不同当然现在也有以T5、BART等为代表的模型，同时结合自编码与自回归学习方式的差异自编码以BERT为代表，在训练中随机对某些位置做MASK，目标函数是让模型准确预测被掩盖的token这种模式下模型以MASK为中心，向左和向右考虑其上下文做出推理，因此称自编码是双向的自编码的模式一般需要对训练数据做标注，适用的任务为NLU，本质上都是分类任务自编码可以完整覆盖上下文，在NLU任务上表现更好，但存在两个缺点：  由于其预训练使用了[MASK]符号，而在针对下游任务的fine-tuning时该符号永远不会出现，因此会造成一定的预训练-微调gap  自编码在训练时基于一个独立假设：在给定了unmasked tokens时，所有待预测的masked tokens是相互独立的，意味着丢掉了masked tokens之间的联系（因为在预测某个masked的token时，模型只能参考未被mask的内容）自回归以GPT为代表，在训练中根据前面的token，预测下一个token这种模式下模型只能参考左边已经生成的数据（参考式1），因此称自回归是单向的自回归的模式可以对语料做shift实现自监督训练，适用的任务为NLG模型结构的差异自回归模型的注意力层，query中的第$i$个token只能与$j&lt;=i$位置的key做计算以Baichuan7b官方代码为例def forward(...):    ...    if attention_mask is None:        attention_mask = torch.ones(            (batch_size, seq_length_with_past), dtype=torch.bool, ce=inputs_embeds.device        )    # 生成一个下三角掩码矩阵 len_q x len_k    # 1 0 0 0 0    # 1 1 0 0 0    # 1 1 1 0 0    attention_mask = self._prepare_decoder_attention_mask(        attention_mask, (batch_size, seq_length), inputs_embeds, _key_values_length    )def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):    # create causal mask    # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]    combined_attention_mask = None    if input_shape[-1] &gt; 1:        combined_attention_mask = _make_causal_mask(            input_shape,            inputs_embeds.dtype,            device=inputs_embeds.device,            past_key_values_length=past_key_values_length,        )    if attention_mask is not None:        # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]        expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.e, tgt_len=input_shape[-1]).to(            inputs_embeds.device        )        combined_attention_mask = (            expanded_attn_mask if combined_attention_mask is None else nded_attn_mask + combined_attention_mask        )    return combined_attention_maskclass Attention(nn.Module):    def forward(...):        ...        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)        ...        if attention_mask is not None:            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):                raise ValueError(                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"                )            # 通过掩码，仅保留下三角的得分            attn_weights = attn_weights + attention_mask            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))GPT-1在GPT-1【1】提出之前，传统的NLP模型一般使用大量的标注数据针对具体任务做有监督训练，这会有2个问题：  需要大量标注数据，数据成本高  只能适用该具体任务，很难泛化到其他任务中GPT-1的思想是预训练+微调：先通过无标签数据学习一个生成式的语言模型，希望其能掌握NLP中的通用知识，然后使用少量带标签数据针对下游任务做微调需要说明的是BERT也沿用了GPT-1的思路，但其预训练的是transformer编码器部分，不是生成式的语言模型网络结构图2. 左：GPT-1网络结构；右：应用到不同下游任务的转换方式如图2左所示，GPT-1使用了12层的transformer解码模块，其中的多头注意力计算使用了掩码图3. 左：双向自注意力；右：从左至右单向自注意力如图3右所示，对于位置为2的query，只能与位置1和2的key与value计算权重，得到输出，超过位置2的信息被mask掉了也就是说掩码自注意力使得系统只能通过已有的信息预测未来，这比BERT的完形填空要难（可能也是模型大小没达到GPT-3之前效果没有BERT好的原因）掩码自注意力在Transformers中的代码实现可以参考GPT2Attention# 生成下三角为True的矩阵[K,K]# max_positions为一个句子的最大长度self.bias = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.uint8)).view(                1, 1, max_positions, max_positions)# 生成应用到value的权重矩阵[K,K]# 其中(i,j)表示query中第i个token和key中第j个token的相似度# 如果 i&lt;j，即query的位置在key位置之前，则对应self.bias的上三角区域，应该要被mask掉attn_weights = torch.matmul(query, key.transpose(-1, -2))...# 这里query_length == key_length表示当前句子长度（也就是当前query在句子中的位置）query_length, key_length = query.size(-2), key.size(-2)causal_mask = self.bias[:, :, key_length - query_length : key_length, :key_length].to(torch.bool)mask_value = torch.finfo(attn_weights.dtype).min# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)# 将causal_mask中为True的位置用attn_weights相同位置赋值，False的位置填充为负无穷大attn_weights = torch.where(causal_mask, attn_weights, mask_value)分词方法GPT-1使用的BPE是基于子词的分词的一种，被广泛使用的还有WordPiece和Unnigram Language Model古典分词古典分词或者按标点符号或者按语法规则实现分词，参考《BPE 算法原理及使用指南【深入浅出】》图4. 古典分词方法示例可以看到，古典分词方法主要有3个缺点：  对于未登录词（OOV, Out Of Vocabulary），模型将无法处理  词表中的低频词在模型中很难得到充分训练  语言中的多形态问题，如look、looks、looked；old、older、oldest意思是一样的，如果视为不同token，将导致词表过大，增加训练难度当然还有一种最极端的方式：将词拆分为单个字符，虽然能解决OOV问题和大词表问题，但粒度太细，训练成本很高基于子词的分词基于子词的分词方法（Subword Tokenization）将一个词切分成更小的子词，其中的子词可以用来构造其他词\"unfortunately\" = \"un\" + \"for\" + \"tun\" + \"ate\" + \"ly\"可见这样既可以降低词表大小，对相似词也能更好的处理具体到GPT-1使用的字节对编码（BPE, Byte Pair Encoding）,是一种数据压缩算法，算法简单有效详细分析参考《BPE详解》无监督预训练假设上文已有n个tokens $U={u_1,…,u_n}$，那么对于语言模型$\\Theta$，其预测下一个token的目标函数为\\[L_1(U)=\\sum_ilogP(u_i|u_{i-k},...,u_{i-1};\\Theta) \\tag{1}\\]在实现上，训练语句和lable保持一致，计算loss时将label往后shift一个token            inputs      [CLS]      你      好                  label      你      好      next token      即对于输入[CLS]，希望模型输出你，对于输入你，希望模型输出好# Shift so that tokens &lt; n predict nshift_logits = lm_logits[..., :-1, :].contiguous()shift_labels = labels[..., 1:].contiguous()# Flatten the tokensloss_fct = CrossEntropyLoss()loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))有监督微调如图2右所示，GPT-1对文本分类、文本蕴含、文本相似、多项选择4种NLP任务转换为分类任务其中文本相似任务使用的加法是想实现a+b=b+a的效果此外GPT-1在数据中添加了起始、分割、结束3种特殊标记所有任务转换为分类任务后，就可以通过在网络后面加fc+softmax轻松实现下游任务这里由于新添加的参数非常少，因此确保了基于小样本的监督学习策略是有效的GPT-2GPT-2【2】更像是对BERT挑战的回应  将模型大小提高到BERT的5倍（15亿参数）  将训练数据提高到40GB  依然坚持基于解码器做生成式模型的技术路线  分词方法从BPE升级为BBPE  抛弃预训练+微调的思想，提出zero-shot的思路网络结构GPT-2使用与GPT-1一致的结构，只在参数上做了部分修改  词表大小达到50257  最大句子长度从512提升到1024GPT-2测试了4种不同大小的网络            参数量      transformer模块数      词向量长度                  117M      12      768              345M      24      1024              762M      36      1280              1542M      48      1600      GPT-2的测试表明，随着模型大小和数据量的增大，其潜能还有进一步开发的空间Zero-ShotGPT-2认为  只要模型和训练数据足够大，模型可以掌握全部的语言知识  有监督任务（微调）其实只是无监督任务（预训练）的子集，它的意义在于告诉模型真实意图（情感分类、文本蕴含。。。）基于此，GPT-2提出Zero-Shot: 在预训练完成后不再使用下游任务的标注数据来进行微调，而是直接进行预测那么如何弥补任务意图和无监督训练之间的鸿沟呢？通过在预测时给提示# 翻译任务的预测数据可以组织成(translate to french, english text, french text)# 阅读理解任务的预测数据可以组织成(answer the question, document, question, answer)上述例子中的translate to french和answer the question就是任务提示(prompt)GPT-3GPT-3【3】保持与GPT-2相同的网络结构，但参数量达到疯狂的1750亿个，训练数据集更是达到45TB，训练费用高达1200万美元其带来的效果是：可以撰写通过图灵测试的文章、可以编写SQL语句或者JavaScript代码由于GPT-3已经非常大了，再对下游任务做fine-tuning即不经济也没必要，文章使用了in-context learning的方法实现预训练模型和下游任务的衔接In-context LearningIn-context Learning (ICL)不对预训练模型的权重做修改，只是在预测时加入几个示例让模型“学习规律”图5. In-context Learning举例图5来自北大研究团队的综述【4】对于情感分析任务，如果想要模型预测语句\"Good Mean\"是Positive还是Negative，首先按图中Template的格式组织若干示例，然后将要预测的语句也按该格式组织，将上述内容一次性输入预训练大模型，理想情况模型的输出就是Positive个人的理解是：ICL假定模型有一定推理和类比能力，因此构造上下文让模型找规律然后作答，所以这里的learning并不是更新参数的意思这里根据示例的多少，可以分为zero-shot、one-shot和few-shot图6. zero-shot/one-shot/few-shot/fine-tuning文献【1】Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.【2】Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.【3】Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[J]. Advances in neural information processing systems, 2020, 33: 1877-1901.【4】Dong Q, Li L, Dai D, et al. A Survey for In-context Learning[J]. arXiv preprint arXiv:2301.00234, 2022."
  },
  
  {
    "title": "【PLM】（2）BERT模型-代码",
    "url": "/posts/PLM-2-BERT%E6%A8%A1%E5%9E%8B-%E4%BB%A3%E7%A0%81/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2023-05-11 00:00:00 +0000",
    





    
    "snippet": "相关代码:  BERT官方代码使用tensorflow实现  transformers的实现同时支持tensorflow和pytorch，并且支持两种模型的转换  本文基于transformers实现的BERT+classify代码，支持训练和推理Transformers是HuggingFace维护的一个基于transformer为技术基础的算法库，目前已复现了BERT、BART、GPT、V...",
    "content": "相关代码:  BERT官方代码使用tensorflow实现  transformers的实现同时支持tensorflow和pytorch，并且支持两种模型的转换  本文基于transformers实现的BERT+classify代码，支持训练和推理Transformers是HuggingFace维护的一个基于transformer为技术基础的算法库，目前已复现了BERT、BART、GPT、ViT等大量算法，并且每种算法提供pyTorch和tensorflow两种实现本文将基于Transformers库对BERT做分析，并以意图分类为目标搭建一份支持训练和推理的代码推荐阅读:  《BERT源码详解（一）——HuggingFace Transformers最新版本源码解读》  《BERT源码详解（二）——HuggingFace Transformers最新版本源码解读》  《【Huggingface Transformers】保姆级使用教程—上》  《Bert在fine-tune时训练的5种技巧》BERT分词在Transformer中通过BertTokenizer实现了BERT的分词和字典ID映射功能class BertTokenizer(PreTrainedTokenizer):    def __init__(        self,        vocab_file,        do_lower_case=True,        do_basic_tokenize=True,        never_split=None,        unk_token=\"[UNK]\",        sep_token=\"[SEP]\",        pad_token=\"[PAD]\",        cls_token=\"[CLS]\",        mask_token=\"[MASK]\",        tokenize_chinese_chars=True,        strip_accents=None,        **kwargs    ):    ...    # 加载字典文件    self.vocab = load_vocab(vocab_file)    self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])    self.do_basic_tokenize = do_basic_tokenize    if do_basic_tokenize:        # 做完basic_tokenizer后再做wordpiece_tokenizer        self.basic_tokenizer = BasicTokenizer(            do_lower_case=do_lower_case,            never_split=never_split,            tokenize_chinese_chars=tokenize_chinese_chars,            strip_accents=strip_accents,        )    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)BertTokenizer包含两种分词器:1）BasicTokenizer  作为WordpieceTokenizer的可选预处理步骤  按空格、标点分割句子，并处理是否统一大小写，清理非法字符  对中文字符，通过加空格做按字分割2）WordpieceTokenizer  将词进一步分为子词，如tokenizer切为token和##izer，其中##表示接在上一个词后面  通过这种将词根与词缀分开的方式可以大大降低字典的大小  论文中BERT使用的是WordPiece字典BertTokenizer使用方法&gt;&gt;&gt; tokenizer = BertTokenizer(vocab_file, do_basic_tokenize=True)# 分词&gt;&gt;&gt; tokens = tokenizer.tokenize('关于BERT的分词器')&gt;&gt;&gt; tokens                ['关', '于', 'be', '##rt', '的', '分', '词', '器']# 直接映射为字典id（句子前后加[CLS]和[SEP]）&gt;&gt;&gt; ids = tokenizer.encode('关于BERT的分词器')  &gt;&gt;&gt; ids[101, 1068, 754, 8815, 8716, 4638, 1146, 6404, 1690, 102]# 根据分词列表返回字典id&gt;&gt;&gt; tokenizer.convert_tokens_to_ids(tokens)[1068, 754, 8815, 8716, 4638, 1146, 6404, 1690]# 根据字典id返回分词列表&gt;&gt;&gt; tokenizer.convert_ids_to_tokens(ids)      ['[CLS]', '关', '于', 'be', '##rt', '的', '分', '词', '器', '[SEP]']# 将分词列表拼接为一个句子(带空格)&gt;&gt;&gt; tokenizer.convert_tokens_to_string(tokens)'关 于 bert 的 分 词 器'# 将字典id列表映射回句子&gt;&gt;&gt; tokenizer.decode(ids)'[CLS] 关 于 bert 的 分 词 器 [SEP]'在实际应用中一般encode会用的比较多# 为了方便batch计算，需要确保encode的序列长度一致# 可以用truncation结合max_length控制输出的最大长度# 对于输出小于期望长度的情况，可以在后面补0（同时要用mask）&gt;&gt;&gt; ids = tokenizer.encode('关于BERT的分词器', add_special_tokens=True, truncation=True, max_length=5)&gt;&gt;&gt; ids[101, 1068, 754, 8815, 102]# 可选择是否add_special_tokens&gt;&gt;&gt; ids = tokenizer.encode('关于BERT的分词器', add_special_tokens=False, truncation=True, max_length=5)&gt;&gt;&gt; ids[1068, 754, 8815, 8716, 4638]示例:ids, mask = [0]*self._max_length, [0]*self._max_lengthids_ = self._tokenizer.encode(    sentence,     add_special_tokens=True,    truncation=True,     max_length=self._max_length)ids[:len(ids_)]  = ids_for ind in range(len(ids_)):    mask[ind] = 1return ids, maskBERT网络由类BertModel实现，网络可以简单分为3块: BertEmbeddings、BertEncoder和BertPoolerclass BertModel(BertPreTrainedModel):    def __init__(self, config, add_pooling_layer=True):        super().__init__(config)        self.config = config        # 实现句子到embedding序列的转换        self.embeddings = BertEmbeddings(config)        # encoder部分，由多个（默认11）个transformer模块串联组成        self.encoder = BertEncoder(config)        # 对输出embedding的[CLS]做fc+tanh        self.pooler = BertPooler(config) if add_pooling_layer else None        # 对所有weights做初始化        self.post_init()BertEmbeddings模块该模块首先通过分词器实现句子到id的映射，然后添加位置编码和句子编码（图1）图1. BERT的embedding输入class BertEmbeddings(nn.Module):    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"    def __init__(self, config):        super().__init__()        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)        ...        def forward(self,...):        ...        # embeddings + 句子id（句子对就是0,1）+ token id        embeddings = inputs_embeds + token_type_embeddings        if self.position_embedding_type == \"absolute\":            position_embeddings = self.position_embeddings(position_ids)            embeddings += position_embeddings        embeddings = self.LayerNorm(embeddings)        embeddings = self.dropout(embeddings)        # 输出 [batch,seq_length,hidden_dim]        return embeddingsBertEncoder模块该模块实现了多个transformer结构的串联class BertEncoder(nn.Module):    def __init__(self, config):        super().__init__()        self.config = config        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])        # 可以降低内存占用，默认关闭        self.gradient_checkpointing = False其中的gradient_checkpointing出自论文【1】，可以降低训练时的显存占用，在pytorch中的实现为torch.utils.checkpoint.checkpoint其中的BertLayer由attention结构和mlp结构组成class BertLayer(nn.Module):    def __init__(self, config):        super().__init__()        ...        # attention结构        self.attention = BertAttention(config)        # mlp结构        self.intermediate = BertIntermediate(config)        ...其中的BertIntermediate比较简单，就是一个全连接+激活的结构其中的BertAttention实现了自注意力层class BertAttention(nn.Module):    def __init__(self, config, position_embedding_type=None):        super().__init__()        # query、key、value计算        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)        # 自注意力输出与输入做resnet跳层连接        self.output = BertSelfOutput(config)        # 多头注意力剪枝(失活某些头)        self.pruned_heads = set()class BertSelfAttention(nn.Module):    def __init__(self, config, position_embedding_type=None):        super().__init__()        ...        self.num_attention_heads = config.num_attention_heads        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)        self.all_head_size = self.num_attention_heads * self.attention_head_size        self.query = nn.Linear(config.hidden_size, self.all_head_size)        self.key = nn.Linear(config.hidden_size, self.all_head_size)        self.value = nn.Linear(config.hidden_size, self.all_head_size)        ...需要特别说明的是BertSelfAttention内部对token做的mask操作（BertTokenizer阶段生成的mask由0/1组成，1表示有效）    if attention_mask is not None:        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)        # token的mask，已做变换: 0-&gt;负无穷大, 1-&gt; 0        # 后面的softmax计算exp时，负无穷大的结果是0        attention_scores = attention_scores + attention_maskclass BertSelfOutput(nn.Module):    def __init__(self, config):        super().__init__()        self.dense = nn.Linear(config.hidden_size, config.hidden_size)        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)        self.dropout = nn.Dropout(config.hidden_dropout_prob)    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -&gt; torch.Tensor:        hidden_states = self.dense(hidden_states)        hidden_states = self.dropout(hidden_states)        # resnet跳层连接        hidden_states = self.LayerNorm(hidden_states + input_tensor)        return hidden_statesBertPooler模块  该模块根据下游任务可选  对[CLS]对应的embedding做全连接+激活class BertPooler(nn.Module):    def __init__(self, config):        super().__init__()        self.dense = nn.Linear(config.hidden_size, config.hidden_size)        self.activation = nn.Tanh()    def forward(self, hidden_states: torch.Tensor) -&gt; torch.Tensor:        # We \"pool\" the model by simply taking the hidden state corresponding        # to the first token.        first_token_tensor = hidden_states[:, 0]         # 对应[CLS]        pooled_output = self.dense(first_token_tensor)        pooled_output = self.activation(pooled_output)        return pooled_outputBERT预训练BERT在预训练阶段包含了两个任务: Masked LM和NSP  Masked LM就是做完形填空，输入是一个句子  NSP输入是两个句子，要求判断其是否有逻辑关系在transformers中BERT的预训练由BertForPreTraining实现class BertForPreTraining(BertPreTrainedModel):    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\", r\"cls.predictions.decoder.weight\"]    def __init__(self, config):        super().__init__(config)        # BERT网络        self.bert = BertModel(config)        # 下游任务        self.cls = BertPreTrainingHeads(config)        # Initialize weights and apply final processing        self.post_init()其中BertPreTrainingHeads包含了上述两个子任务class BertPreTrainingHeads(nn.Module):    def __init__(self, config):        super().__init__()        # Masked LM任务        self.predictions = BertLMPredictionHead(config)        # NSP任务        self.seq_relationship = nn.Linear(config.hidden_size, 2)    def forward(self, sequence_output, pooled_output):        prediction_scores = self.predictions(sequence_output)        seq_relationship_score = self.seq_relationship(pooled_output)        return prediction_scores, seq_relationship_score注意在Mask LM任务中，对[MASK]位置的label取-100，因为要与torch.nn.CrossEntropyLoss中的ignore_index=-100保持一致BERT下游任务的fine-tuningtransformers中同时编写了4种类实现不同任务的fine-tuning  BertForSequenceClassification: 句子分类/回归，适用于GLUE任务集  BertForMultipleChoice: 多选分类，适用于SWAG任务集  BertForTokenClassification: 词元分类，适用于NER任务  BertForQuestionAnswering: QA问答，适用于SQuAD任务图2. transformers中部分类的继承关系训练细节1）优化器BERT使用的优化器为AdamW，出自ICLR2017的best paper【2】import transformersoptimizer = transformers.AdamW(model_parameters, lr=lr, correct_bias=True)博文《Bert在fine-tune时训练的5种技巧》测试了有无correct_bias的情况，似乎影响不大2）预训练权重的选择通常越接近输入的层学到的是越通用化的语义，如词性词法等；越接近输出的层学到的是下游任务相关的知识文章【3】研究了保留部分预训练权重，对另一部分权重做随机初始化的策略，在部分任务上获得了提升3）L2正则化weight_decayBERT官方代码中对bias、LayerNorm.bias和LayerNorm.weight不做正则化        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']        param_optimizer = list(model.named_parameters())        optimizer_grouped_parameters = [            {'params': [p for n,p in param_optimizer                         if not any(nd in n for nd in no_decay)],              'weight_decay':0.01            },            {'params': [p for n,p in param_optimizer                         if any(nd in n for nd in no_decay)],              'weight_decay':0.0            },        ]        optimizer = transformers.AdamW(            optimizer_grouped_parameters,  lr=lr, correct_bias=True        )4）冻结部分层在大模型中冻结部分层可以减少参数迭代计算，加快训练速度一般冻结靠近输入的低层不会对精度影响很大在PyTorch中可以利用params.requires_grad=False或with torch.no_grad():实现参数冻结5）warmup训练前期从一个较小的lr开始慢慢上升，之后再慢慢降低文章【4】认为这是因为训练开始时梯度比较大，如果lr比较大会造成剧烈变化，模型的优化变得不稳定此外不同于CV，NLP的lr一般比较小，如BERT的实验部分使用的lr处于e-5级别文献【1】Chen T, Xu B, Zhang C, et al. Training deep nets with sublinear memory cost[J]. arXiv preprint arXiv:1604.06174, 2016.【2】Loshchilov I, Hutter F. Fixing weight decay regularization in adam[J]. 2017.【3】Zhang T, Wu F, Katiyar A, et al. Revisiting few-sample BERT fine-tuning[J]. arXiv preprint arXiv:2006.05987, 2020.【4】Xiong R, Yang Y, He D, et al. On layer normalization in the transformer architecture[C]//International Conference on Machine Learning. PMLR, 2020: 10524-10533."
  },
  
  {
    "title": "【PLM】（1）BERT模型-原理",
    "url": "/posts/PLM-1-BERT%E6%A8%A1%E5%9E%8B-%E5%8E%9F%E7%90%86/",
    "categories": "LLM, PLM",
    "tags": "PLM",
    "date": "2023-05-08 00:00:00 +0000",
    





    
    "snippet": "BERT【1】是Google在2018年提出的，该模型将NLP的研究推向了一个新的高度  在GLUE的9个子任务均取得SOTA，总体精度提高到80.5%（促使难度更高的SuperGLUE被提出）  使得预训练+fine-tuning的方法在NLP中成为主流推荐阅读:  BERT 论文逐段精读【论文精读】  基于BERT预训练模型的SQuAD问答任务  基于BERT预训练模型的SWAG问答任务...",
    "content": "BERT【1】是Google在2018年提出的，该模型将NLP的研究推向了一个新的高度  在GLUE的9个子任务均取得SOTA，总体精度提高到80.5%（促使难度更高的SuperGLUE被提出）  使得预训练+fine-tuning的方法在NLP中成为主流推荐阅读:  BERT 论文逐段精读【论文精读】  基于BERT预训练模型的SQuAD问答任务  基于BERT预训练模型的SWAG问答任务预训练语言模型与下游任务的结合BERT将预训练语言模型与下游任务的结合策略分为两种：feature-based为下游任务设计特殊的网络结构，同时使用预训练语言模型（如ELMo）的输出作为该网络的额外输入特征这种方法将预训练模型作为补充特征来源，在训练时不更新预训练模型的参数fine-tuning针对下游任务在预训练模型后添加少量layer（比如针对分类任务添加fc+softmax）,代表方法为GPT【2】在训练时更新全部网络的参数可以看到：  feature-based方法重点仍在为下游任务定制的模型上  fine-tuning假定预训练模型掌握所有知识，只需要特定的方法将其表达成下游任务需要的形式BERT沿用的是fine-tuning的思路，而这也是其能凭借一个模型解决多种NLP子任务的原因模型结构标准的语言模型是单向（从左至右）的：Transformer中的自注意力层只关注当前token之前的内容，模型根据当前时刻已获取的所有信息推测下一时刻的信息但BERT认为这种方式限制了模型的能力，如句子级别的任务并不一定需要从左到右；词元级别的任务也可以一次看完所有内容再判断BERT的设计基于之前的两项工作：ELMo【3】和GPT【2】  ELMo是双向的设计，但使用了比较老的RNN结构  GPT是单向设计，但使用了性能更好的Transformer结构由此BERT提出使用Transformer+双向的设计思路（此时的BERT还想象不到未来的自己会被GPT-3反杀）：  网络结构上，只使用transformer【4】中的encoder模块  预训练任务上，包含Cloze任务（token级别）和下一个句子预测任务（句子级别）图1. transformer结构，左：encoder，右：decoder如下表所示，BERT使用  $L$表示Transformer的模块数量（即图1中的N）  $H$表示隐藏层的维度  $A$表示自注意力头的数量基于不同的L、H、A设定设计了两个模型            模型名称      L      H      A      参数量                  $BERT_{BASE}$      12      768      12      110M              $BERT_{LARGE}$      24      1024      16      340M      其中$BERT_{BASE}$约1亿参数量，与GPT相当，用来做性能对比；$BERT_{LARGE}$约3.4亿参数，用来刷榜参数量的计算参考图1左，可以将BERT网络分为2个部分：输入embedding、transformer模块输入embedding该部分将输入的token转换为embedding向量，该向量的维度即为$H$BERT使用的token字典来自WordPiece【5】,包含30K个tokens因此该部分的参数量为$30k\\times H$transformer模块一个transformer模块包含Attention模块和MLP模块1）Attention模块其中的query、key、value的自注意力计算是矩阵乘法，没有可学习的参数但多头注意力需要  将输入embedding首先通过全连接到query、key和value（embedding的维度/3）  注意力计算后再做一次全连接恢复到embedding的维度因此一个Attention模块的可学习参数为$3\\times (H\\times H/3)+H\\times H=4H^2$2）MLP模块该模块包含2个全连接，第一个将Attention的输出embedding维度扩大到4倍，第二个再恢复为原始维度因此一个MLP模块的可学习参数量为$H\\times 4H+4H\\times H=8H^2$最后，对于一个包含了$L$层的transformer模块，可学习参数量为$12H^2\\times L$综上，可得一个BERT的参数量计算公式为\\[Parm_{BERT}=30k\\times H+12H^2\\times L \\tag{1}\\]BERT预训练为了使BERT建立双向理解和句子间关系理解的能力，BERT使用2个无监督任务来做预训练  BERT在预训练时使用无监督数据，在fine-tuning时再使用与下游任务相关的带label数据做有监督训练  BERT认为大数据量的无监督训练效果是好于小数据量的有监督训练的  BERT推荐使用文档级别的语料，而不是毫无逻辑关系的语句作为训练语料为了使BERT同时能兼容以下两种任务，其对输入做了特别的设计图2. BERT的输入如图2所示，BERT使用[CLS]表示首句的类别，使用[SEP]表示句子的结束，这样就可以将两个句子组成一个输入序列为了区分不同句子，BERT在token位置编码（Position Embeddings）的基础上额外添加了句子级的位置编码（Segment Embeddings）图2中的$E_A=0$，$E_B=1$Masked LM为了使得模型具有双向能力，BERT提出了Masked LM任务MLM与之前提出的Cloze其实是一回事，就是将句子中的某些token遮住，让模型做完形填空，预测该位置的token具体操作上，BERT将被遮住的token用[MASK]代替考虑到下游任务做fine-tuning时不会存在包含[MASK]的数据，为了提高fine-tuning时的效果，在基于MLM做预训练时采用如下策略：  每个句子15%的位置被选为mask位置  对每个mask位置，80%的概率使用[MASK]代替，10%的概率换成其他token（相当于加噪声），10%的概率不做改变BERT在附录中比较了不同比例的消融实验效果图3. 不同mask比例对精度的影响NSPNSP(Next Sentence Prediction)旨在训练BERT模型对句子间关系的理解能力，这种能力在诸如QA和NLI等下游任务中是被需要的具体来说，BERT会随机选择句子对，确保50%的概率下B句是A句的下一句（IsNext），另外50%的概率则不是（NotNext）图4. NSP对精度的影响图4中$BERT_BASE$是使用了MLM和NSP的精度，后面3种都没有使用NSPBERT 下游任务fine-tuning在fine-tuning时BERT的输入如图2所示，输出根据具体任务做很小的调整，fine-tuning过程中BERT的所有参数都会被改变BERT认为相较于预训练，fine-tuning的代价很低，论文中所有的实验使用TPU最多1小时，使用GPU也就几个小时（个人认为这里作者想表达的意思是，fine-tuning时很少量的数据就能获得很高的精度，毕竟fine-tuning的计算量是不变的，代价降低只能归因于数据量上了）GLUEGLUE都可以看做分类任务BERT将图2中输入的[CLS]作为类别输出特征（维度为$H$），然后接一个$H\\times K$的全连接层+log_softmax实现分类输出（其中$K$为类别数）在fine-tuning过程中，BERT选择的部分参数为  batch_size: 32  epoch: 3  learning rate: [5e-5, 4e-5, 3e-5, 2e-5]图5. BERT在GLUE的fine-tuning效果SQuADSQuAD是斯坦福通过众包构建的一个机器阅读理解数据集，目前有版本1.1和2.0SQuAD提供一个上下文描述和一个问题  1.1版本下该问题的答案包含在上下文描述中，要求模型从中找出来  2.0版本增加了50000条没有答案的问题，要求模型学会对没有答案的问题不做回答BERT将其转化为一个token分类问题：对上下文描述中的每个token做分类，看其是否为答案开头、结尾BERT在fine-tuning中选择的部分参数为  batch_size: 32  epoch: 3  learning rate: 5e-5以下内容参考《基于BERT预训练模型的SQuAD问答任务》模型构建图6. BERT解决SQuAD问题针对上下文过长（比如超过BERT默认的512）的情况，BERT采用了滑动窗口的方式将其拆成几个子句图7. 滑动窗口处理上下文过长如图7所示，[CLS]保持不变，将上下文通过窗口滑动切成4个子段（100表示其归属的样本ID），然后分别对每个子段做预测1）对每个子段的预测结果取TOP K如图7(2)所示，取K=4，7:0.41表示该子段位置为7的token（这里对应的是[SEP]）是起始位置的概率为0.412）根据位置做过滤基于两条规则  位置ID应该位于上下文范围内，如对第一个子段，应有$7 &lt; id &lt;17$  结尾ID应大于所有起始ID中最小的那个最终的筛选结果如图7(3)所示（第2行的8:0.2应该被删掉）3）分别对每个子段过滤后的起始和结尾做组合，并按概率排列图8. 组合筛选后的结果组合的概率为起始预测概率和结尾预测概率之和如图8所示，最终选择第4个子段的[9,13]序列为输出答案SWAGSWAG(Situations With Adversarial Generations)给定一个问题或描述，要求模型从给定的4个选项中预测最合理的一个The people are in robes. TheyA) wearing colorful costumes.# 正确选项B) are doing karate moves on the floor.C) shake hands on their hips.  D) do a flip to the bag.SWAG也可以转为一个分类问题，参考《基于BERT预训练模型的SWAG问答任务》图9. SWAG转换为分类问题如图9，保持[CLS]不变，将选项分别组织成4个子段，经过BERT后对4个输出做分类BERT在fine-tuning中选择的部分参数为  batch_size: 16  epoch: 3  learning rate: 2e-5文献【1】Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.【2】Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.【3】Peters M , Neumann M , Iyyer M , et al. Deep Contextualized Word Representations[J]. 2018.【4】Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.【5】Wu Y, Schuster M, Chen Z, et al. Google’s neural machine translation system: Bridging the gap between human and machine translation[J]. arXiv preprint arXiv:1609.08144, 2016."
  },
  
  {
    "title": "GLUE&SuperGLUE基准分析",
    "url": "/posts/GLUE&SuperGLUE%E5%9F%BA%E5%87%86%E5%88%86%E6%9E%90/",
    "categories": "NLP",
    "tags": "",
    "date": "2023-04-26 00:00:00 +0000",
    





    
    "snippet": "推荐参考资料:  《【HugBert04】GLUE：BERT类模型的通用评估基准》GLUE基准通用语言理解评估(General Language Understanding Evaluation, GLUE)于2018年由纽约大学、华盛顿大学和DeepMind的研究者共同提出【1】，官网GLUE主要目标是鼓励开发出能在任务间共享通用的语言知识的模型，BERT正是通过狂刷GLUE9项任务的记录...",
    "content": "推荐参考资料:  《【HugBert04】GLUE：BERT类模型的通用评估基准》GLUE基准通用语言理解评估(General Language Understanding Evaluation, GLUE)于2018年由纽约大学、华盛顿大学和DeepMind的研究者共同提出【1】，官网GLUE主要目标是鼓励开发出能在任务间共享通用的语言知识的模型，BERT正是通过狂刷GLUE9项任务的记录开启一个新的时代GLUE集合了针对自然语言理解NLU的训练、评估和分析所需的资源  涵盖9个nlu任务，这些任务都建立在已有的数据集上，并考虑了不同数据集大小、文本类型和难度  设计了一个诊断数据集，用于评估和分析模型性能  建立了一个公共排行榜9类子任务图1. GLUE子任务如上图，可以将9类任务分为3大类：  单句任务：CoLA和SST-2，输入一个句子，输出某种分类判断  句子相似任务：MRPC、STS-B、QQP，输入一个句子对，输出两个句子的相似度  推断任务：MNLI、QNLI、RTE、WNLI，输入一个句子对，输出后句相对于前句语义关系的一个推断其中只有STS-B是回归任务，其余都是分类任务CoLA任务语言可接受性语料库(The Corpus of Linguistic Acceptability, CoLA)【2】，是一个二分类任务，判断一个句子在语法上是否正确，官网1）数据组织CoLA由来自23种语言学出版物的10657个句子组成，并由原作者对其可接受性（语法正确性）进行了专业注释CoLA将所有数据分为train、dev和test，前两项包含9594个句子，test数据不公开在数据组织上，CoLA将其中的17种出版物来源定义为in-domain数据集，剩下的6种定义为out-of-domain数据集，其中in-domain数据集包含train、dev和test，out-of-domain只包含dev和test此外CoLA还提供了经过分词处理的版本  raw/in_domain_train.tsv (8551 lines)  raw/in_domain_dev.tsv (527 lines)  raw/out_of_domain_dev.tsv (516 lines)  tokenized/in_domain_train.tsv (8551 lines)  tokenized/in_domain_dev.tsv (527 lines)  tokenized/out_of_domain_dev.tsv (516 lines)2）数据格式每行数据通过tab分为4列            数据来源编码      语法可接受性label      由原作者标记的可接受性      语句内容                  gj04      1             We yelled ourselves hoarse.              gj04      0      *      We yelled ourselves.              gj04      1             Bill urinated out the window.              cj99      0      ??      I can well imagine the more him eating, the fatter him getting.      其中’?’和’#’标记表示作者对此模棱两可，CoLA将这种情况均定义为不可接受SST-2任务Stanford Sentiment Treebank(SST-2)【3】是一个针对情感分析的二分类任务，由于是二分类因此称为SST-21）数据组织SST-2数据来自烂番茄电影评论，一共11856个语句├── original│   ├── README.txt│   ├── SOStr.txt│   ├── STree.txt│   ├── datasetSentences.txt│   ├── datasetSplit.txt│   ├── dictionary.txt│   ├── original_rt_snippets.txt│   └── sentiment_labels.txt├── dev.tsv├── test.tsv└── train.tsv其中train.tsv、dev.tsv和test.tsv是整理好的数据，original是原始数据  original_rt_snippets.txt：是直接从烂番茄扒的数据，一共10605个片段，每个片段可能包含多个句子  datasetSentences.txt：使用stanford parser对original_rt_snippets.txt做处理的结果2）数据格式每行数据通过tab分为2列，0表示negative，1表示positive            sentence      label                  hide new secretions from the parental units      0              with his usual intelligence and subtlety      1      MRPC任务Microsoft Research Paraphrase Corpus(MRPC)是微软提出的一种二分类任务，要求判断两个句子在语义上是否等价1）数据组织MRPC发布于2005年，数据来自各个新闻网站，一共5801条，由至少两个人给每对语句做是否语义相似的判断，如果两个人意见不一致则引入第三人  msr_paraphrase_data.txt：原始数据，一行一个语句，包含ID、作者、URL、抓取日期  msr_paraphrase_test.txt ：测试数据，一行两个语句，包含是否相似的label(0/1)、ID1、ID2、语句1、语句2  msr_paraphrase_train.txt：训练数据，一行两个语句，包含是否相似的label(0/1)、ID1、ID2、语句1、语句22）数据格式            Quality      #1 ID      #2 ID      #1 String      #2 String                  1      702876      702977      Amrozi accused his brother, whom he called “the witness”, of deliberately distorting his evidence.      Referring to him as only “the witness”, Amrozi accused his brother of deliberately distorting his evidence.              0      2108705      2108831      Yucaipa owned Dominick’s before selling the chain to Safeway in 1998 for $2.5 billion.      Yucaipa bought Dominick’s in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.      STS-B任务Semantic Textual Similarity Benchmark(STS-B)，官网任务要求判断两个句子在语义上是否等价，但与MRPC不同的是，其要求模型给出相似性打分（1-5分），因此这是一个回归任务1）数据组织STS-B数据来自于新闻、字幕、论坛，一共8628对语句，将其分为train、dev和test                   train      dev      test      总计                  新闻      3299      500      500      4299              字幕      2000      625      625      3250              论坛      450      375      254      1079              总计      5749      1500      1379      8628      数据集文件分布如下├── original│   ├── sts-dev.tsv│   ├── sts-test.tsv│   └── sts-train.tsv├── readme.txt├── LICENSE.txt├── dev.tsv├── test.tsv└── train.tsv2）数据格式每行一对语句，用tab分割            index      genre      filename      year      old_index      source1      source2      sentence1      sentence2      score                  0      main-captions      MSRvid      2012test      0001      none      none      A plane is taking off.      An air plane is taking off.      5.000              1      main-captions      MSRvid      2012test      0004      none      none      A man is playing a large flute.      A man is playing a flute.      3.800      QQP任务Quora Question Pairs(QQP)数据来自社区问答网站Quora任务要求判断给定的两个问句是否在问同一件事情，是一个二分类问题1）数据组织QQP包含超过40W个问题对，使用0/1标注其是否属于一个问题├── dev.tsv├── test.tsv└── train.tsv2）数据格式每行一对语句，用tab分割            id      qid1      qid2      question1      question2      is_duplicate                  402555      536040      536041      How do I control my horny emotions?      How do you control your horniness?      1              360472      364011      490273      What causes stool color to change to yellow?      What can cause stool to come out as little balls?      0      MNLI任务Multi-Genre Natural Language Inference (多流派自然语言推理，MultiNLI)【4】，官网MNLI是一个文本蕴含任务：给定两个句子，一个句子称为前提(premise)，一个句子称为假设(hypothesis)，要求判断在给定前提的情况下，假设是否成立（成立entailment、相反contradiction、无法推断/中性neutral），因此是一个三分类问题1）数据组织MNLI从多种领域整理了433K个语句对├── original│   ├── multinli_1.0_train.jsonl│   ├── multinli_1.0_train.txt│   ├── multinli_1.0_dev_mismatched.txt│   ├── multinli_1.0_dev_mismatched.jsonl│   ├── multinli_1.0_dev_matched.txt│   └── multinli_1.0_dev_matched.jsonl├── train.tsv├── dev_matched.tsv├── dev_mismatched.tsv├── test_matched.tsv└── test_mismatched.tsv同时还提供了4种模型的精度作为baseline            Model      Matched Test Acc      Mismatched Test Acc                  Most Frequent Class      36.5%      35.6%              CBOW      65.2%      64.6%              BiLSTM      67.5%      67.1%              ESIM      72.4%      71.9%      2）数据格式每行一对语句，用tab分割            index      promptID      pairID      genre      sentence1_binary_parse      sentence2_binary_parse      sentence1_parse      sentence2_parse      sentence1      sentence2      label1      gold_label                  0      31193      31193n      government      ( ( Conceptually ( cream skimming ) ) ( ( has ( ( ( two ( basic dimensions ) ) - ) ( ( product and ) geography ) ) ) . ) )      ( ( ( Product and ) geography ) ( ( are ( what ( make ( cream ( skimming work ) ) ) ) ) . ) )      (ROOT (S (NP (JJ Conceptually) (NN cream) (NN skimming)) (VP (VBZ has) (NP (NP (CD two) (JJ basic) (NNS dimensions)) (: -) (NP (NN product) (CC and) (NN geography)))) (. .)))      (ROOT (S (NP (NN Product) (CC and) (NN geography)) (VP (VBP are) (SBAR (WHNP (WP what)) (S (VP (VBP make) (NP (NP (NN cream)) (VP (VBG skimming) (NP (NN work)))))))) (. .)))      Conceptually cream skimming has two basic dimensions - product and geography.      Product and geography are what make cream skimming work.      neutral      neutral              2      134793      134793e      fiction      ( ( One ( of ( our number ) ) ) ( ( will ( ( ( carry out ) ( your instructions ) ) minutely ) ) . ) )      ( ( ( A member ) ( of ( my team ) ) ) ( ( will ( ( execute ( your orders ) ) ( with ( immense precision ) ) ) ) . ) )      (ROOT (S (NP (NP (CD One)) (PP (IN of) (NP (PRP$ our) (NN number)))) (VP (MD will) (VP (VB carry) (PRT (RP out)) (NP (PRP$ your) (NNS instructions)) (ADVP (RB minutely)))) (. .)))      (ROOT (S (NP (NP (DT A) (NN member)) (PP (IN of) (NP (PRP$ my) (NN team)))) (VP (MD will) (VP (VB execute) (NP (PRP$ your) (NNS orders)) (PP (IN with) (NP (JJ immense) (NN precision))))) (. .)))      One of our number will carry out your instructions minutely.      A member of my team will execute your orders with immense precision.      entailment      entailment      QNLI任务Question Natural Language Inference(QNLI)，数据来源于斯坦福问答数据集SQuAD原SQuAD任务是给定一个问题，从一篇wikipedia的文章标识出可能存在的答案QNLI将该问题转为为句子对的分类问题：问题句不变，但从文章自动抽句子作为答案，要求判断该答案能否回答该问题（能entailment，不能not_entailment）1）数据组织QNLI包含大约10万对QA数据├── dev.tsv├── test.tsv└── train.tsv2）数据格式每行一对语句，用tab分割index\tquestion\tsentence\tlabel1\tWhich missile batteries often have individual launchers several kilometres from one another?\tWhen MANPADS is operated by specialists, batteries may have several dozen teams deploying separately in small sections; self-propelled air defence guns may deploy in pairs.\tnot_entailment2\tWhat two things does Popper argue Tarski's theory involves in an evaluation of truth?\tHe bases this interpretation on the fact that examples such as the one described above refer to two things: assertions and the facts to which they refer.\tentailmentRTE任务Recognizing Textual Entailment(RTE)是一个文本蕴含任务，官网任务给定2个句子，要求判断后者的语义是否能由前者推出（能entailment，不能not_entailment）1）数据组织RTE包含大约5K对语句├── dev.tsv├── test.tsv└── train.tsv2）数据格式每行一对语句，用tab分割            index      sentence1      sentence2      label                  0      No Weapons of Mass Destruction Found in Iraq Yet.      Weapons of Mass Destruction Found in Iraq.      not_entailment              1      A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.      Pope Benedict XVI is the new leader of the Roman Catholic Church.      entailment      WNLI任务Winograd Schema Challenge(WNLI)，官网任务要求模型能正确判断句子中代词指代的内容GLUE人工的将可能的指代对象组织成句子，和原句放在一起，由模型判断该句子的指代是否正确，由此将其转化为一个二分类任务原句：I stuck a pin through a carrot. When I pulled the pin out, it had a hole.人工组织语句：The carrot had a hole.原句中的it指代的是the carrot，因此应该分类为11）数据组织WNLI大约包含800对语句├── dev.tsv├── test.tsv└── train.tsv2）数据格式每行一对语句，用tab分割            index      sentence1      sentence2      label                  0      I stuck a pin through a carrot. When I pulled the pin out, it had a hole.      The carrot had a hole.      1              1      John couldn’t see the stage with Billy in front of him because he is so short.      John is so short.      1              3      Steve follows Fred’s example in everything.      Steve influences him hugely.      0      诊断数据集GLUE基于MNLI任务精心构建了一个诊断数据集，希望通过执行蕴含任务让开发者深入了解模型在做什么、抓住了什么现象，以及它的局限性在哪里诊断数据集按考察的方向分为4个主类（词汇语义、谓词论元结构、逻辑、知识），每个主类又被细分为若干子类，帮助开发者定位模型缺陷图2. 诊断数据集考察方向诊断数据集中每对语句都会做正向反向排列，其数据格式为            Lexical Semantics      Predicate-Argument Structure      Logic      Knowledge      Domain      Premise      Hypothesis      Label                                Negation             Artificial      The cat sat on the mat.      The cat did not sit on the mat.      contradiction                            Negation             Artificial      The cat did not sit on the mat.      The cat sat on the mat.      contradiction      评估指标大部分子任务的评估指标是精度和F1指标其中CoLA采用Matthews相关系数STS-B采用Pearson和Spearman相关系数相关性描述两个变量之间的统计关系：  正相关：一个变量随另一个变量的增大/减小而增大/减小  负相关：一个变量随另一个变量的增大/减小而减小/增大  不相关：两个变量没有统计关系线性相关：变化幅度(梯度)为常数单调关系：变化幅度(梯度)为正Matthews相关系数Matthews相关系数被用来衡量二分类和多分类任务，可以解决因为类别不平衡造成的precision、recall、f1统计偏差参考《Matthews Correlation Coefficient is The Best Classification Metric You’ve Never Heard Of》考虑一个猫狗分类问题，狗图片有20张，其中18张被分类器正确分类，猫有4张，其中1张被正确分类现在分别以狗为positive、猫为positive统计混淆矩阵图3. 混淆矩阵分别计算两种情况的precision和recall：\\[\\begin{aligned}P_{dog}=\\frac{TP}{TP+FP}=0.86,\\ &amp;P_{cat}=\\frac{TP}{TP+FP}=0.33\\\\R_{dog}=\\frac{TP}{TP+TN}=0.9,\\ &amp;R_{cat}=\\frac{TP}{TP+TN}=0.25\\end{aligned} \\tag{1}\\]可以看到，同一个分类器，因为positive的定义不同，统计的PR有很大差别（其根本原因是P和R都没有考虑FN）现在考虑Matthews相关系数计算公式\\[MCC=\\frac{TP*TN-FP*FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}} \\tag{2}\\]其中  MCC的取值范围为[-1,1]；  1表示完全正相关，是一个完美的分类器  -1表示完全负相关，也是一个完美的分类器（反转分类器即可）可以看到在对于上例，有$MCC_{dog}=MCC_{cat}=0.17$，两种情况的统计值一致，分类器效果不理想Pearson和Spearman相关系数Pearson和Spearman相关系数用于衡量预测值和标签值之间的相关性  Pearson适用于两个变量之间的线性关系，Spearman适用于单调关系  Pearson处理变量的原始值，Spearman处理数据排序值  一般来说Spearman比Pearson更准确\\[\\begin{aligned}Pearson&amp;=\\frac{Cov(X,Y)}{\\sigma_X\\sigma_Y}\\\\&amp;=\\frac{\\sum_{i=1}^n(x_i-\\hat{x})(y_i-\\hat{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\hat{x})^2}\\sqrt{\\sum_{i=1}^n(y_i-\\hat{y})^2}}\\end{aligned} \\tag{3}\\]对于Spearman，首先将式3中的变量X和Y从小到大排序，取其秩次使用式3计算Baselines&amp;排行榜GLUE提供了若干Baselines，包含GloVe、CoVe和ELMo三种模型GLUE还开放了一个排行榜，对每个模型从各个子任务进行评分图4. GLUE排行榜SuperGLUEGLUE提出的一年时间里，模型算法的进化非常快，其表现已接近非专家的人类表现了因此提出一个难度更高的NLU任务集SuperGLUE【5】，官网图5. 模型在GLUE的得分  GLUE的综合得分：MT-DNN和XLNet-Large已经超越人类基准(1.0)  子任务得分：QNLI、MRPC、QQP已经超过人类基准但这并不意味着机器已经掌握了英语。比如对于WNLI中某些句子的代词指代，机器的表现还与随机乱猜差不多SuperGLUE的成果包括  涵盖7个nlu子任务（其中2项继承自GLUE）  发布了一套用于预训练、多任务学习和迁移学习的模块化建模工具包（基于PyTorch和AllenNLP）  建立了一个新的排行榜，修改了管理规则使之更公平8类子任务SuperGLUE对所有子任务数据的格式做了统一：  从组织上，每个任务的数据由train.jsonl、dev.jsonl和test.jsonl组成  从结构上，每条数据都是json格式图6. SuperGLUE子任务CB任务CommitmentBank（CB）与MNLI一样是一个文本蕴含任务给定两个句子，一个句子称为前提(premise)，一个句子称为假设(hypothesis)要求判断在给定前提的情况下，假设是否成立（成立entailment、相反contradiction、无法推断/中性neutral），是一个三分类问题CB数据来自华尔街日报、英国国家语料库的小说、Switchboard，大约包含500对语句SuperGLUE使用了CB的一个子集，将所有数据分为train、dev和test数据格式每行数据为一个字典，包含premise、hypothesis、label和idx其中label取值为entailment、neutral和contradiction{  \"premise\": \"It was a complex language. Not written down but handed down. One might say it was peeled down.\",   \"hypothesis\": \"the language was peeled down\",   \"label\": \"entailment\",   \"idx\": 0}COPA任务Choice of Plausible Alternatives（COPA）【6】是一个因果推理任务：给定一个前提句子和两个可能的选项，要求选择与前提句子更可信的那项COPA数据主要来自博客和摄影相关的百科全书，大约包含1000条语句数据格式每行数据为一个字典，包含premise、chocice1、choice2、question、label和idx其中label取值为0或1，question取值为cause或effect{  \"premise\": \"My body cast a shadow over the grass.\",   \"choice1\": \"The sun was rising.\",   \"choice2\": \"The grass was cut.\",   \"question\": \"cause\",   \"label\": 0,   \"idx\": 0}MultiRC任务Multi-Sentence Reading Comprehension（MultiRC）是一个上下文阅读理解任务：给定一个段落文本和若干个问题，每个问题给定若干回答，要求依次在每个问题内选择正确的回答其设计考量主要有：  每个问题都可以有多个可能的正确答案。因此每个QA必须独立，模型针对某个Q在预设的多个A里选择  问题的设计方式使得每个问题的解答都需要从多个上下文句子中提取信息  相比于基于范围的抽取型问答，MultiRC更匹配其他SuperGLUE任务的任务格式(API)数据格式每行数据为一个字典，包含idx、version、passage、questions其中passage是段落文本，questions是若干QAs{  \"idx\": 2,   \"version\": 1.1,   \"passage\": {    \"text\": \"I'm here to tell you the story of a robot named Carl. .... \", \t\"questions\": [\t  {\t    \"question\": \"What did Carl need before going to the lab\", \t\t\"answers\": [\t\t  {\"text\": \"Tire\", \"idx\": 189, \"label\": 1}, \t\t  {\"text\": \"Tire and sun gatherer\", \"idx\": 190, \"label\": 1}, \t\t  {\"text\": \"10 days\", \"idx\": 191, \"label\": 0}, \t\t  {\"text\": \"Only a tire\", \"idx\": 192, \"label\": 0}, \t\t  {\"text\": \"Only a sun gatherer\", \"idx\": 193, \"label\": 0}, \t\t  {\"text\": \"A tire as well as sun gatherer\", \"idx\": 194, \"label\": 1}, \t\t  {\"text\": \"Sun gatherer\", \"idx\": 195, \"label\": 1}\t\t  ], \t\t\"idx\": 30\t  }, \t  {\t    \"question\": \"Does Mr. X install Carl's new parts?\", \t\t  \"answers\": [\t\t    {\"text\": \"Not as all\", \"idx\": 196, \"label\": 1}, \t\t    {\"text\": \"Maybe\", \"idx\": 197, \"label\": 0}\t\t  ], \t\t\"idx\": 31\t  }, \t  ...\t]  }}RTE任务Recognizing Textual Entailment(RTE)是一个文本蕴含任务任务给定2个句子，要求判断后者的语义是否能由前者推出（能entailment，不能not_entailment）RTE是从GLUE继承下来的任务，是从迁移学习中获益最多的RTE的准确度从GLUE发布时的56%提升到目前的85%，但与人类水平还差8个百分点数据格式每行数据为一个字典，包含premise、hypothesis、label、idx{  \"premise\": \"No Weapons of Mass Destruction Found in Iraq Yet.\",   \"hypothesis\": \"Weapons of Mass Destruction Found in Iraq.\",   \"label\": \"not_entailment\",   \"idx\": 0}WiC任务The Word-in-Context Dataset （WiC）WiC针对的是词义消岐任务：给定2个句子和同时出现在句子中的多义词，要求判断其含义是否相同数据格式每行数据为一个字典，包含word、sentence1、sentence2、idx、label、start1、start2、end1、end2、version{  \"word\": \"place\",   \"sentence1\": \"Do you want to come over to my place later?\",   \"sentence2\": \"A political system with no place for the less prominent groups.\",   \"idx\": 0,   \"label\": false,   \"start1\": 31,   \"start2\": 27,   \"end1\": 36,   \"end2\": 32,   \"version\": 1.1}WSC任务Winograd Schema Challenge（WSC）任务要求模型能正确判断句子中代词指代的内容WSC继承自GLUE中的WNLI，将需要对比的代词和指代内容提取出来，模型对其做二分类数据格式每行数据为一个字典，包含text、target、sentence2、idx、label{  \"text\": \"Mark told Pete many lies about himself, which Pete included in his book. He should have been more skeptical.\",    \"target\": {      \"span2_index\": 13, \"span1_index\": 0, \"span1_text\": \"Mark\", \"span2_text\": \"He\"    },   \"idx\": 0,   \"label\": false}BoolQ任务BoolQ是一个文本蕴含任务，其数据源是自然产生而非机器生成BoolQ给定一个yes/no问题（推断）、一段文本（假设），要求基于文本判断该问题回答的二分类数据格式每行数据为一个字典，包含question、passage、idx、label{  \"question\": \"calcium carbide cac2 is the raw material for the production of acetylene\",   \"passage\": \"Calcium carbide -- Calcium carbide is a chemical compound with the ...\",   \"idx\": 13,   \"label\": true}ReCoRD任务Reading Comprehension with Commonsense Reasoning Dataset（ReCoRD）【7】，基于常识推理数据集的阅读理解ReCoRD是一个多选题的QA任务：给定一段文本和一个与之相关的Cloze式问题（其中一个实体被masked），要求根据文本预测被masked的实体数据格式每行数据为一个字典，包含source、passage、entities、qas其中  passage是相关文本  entities是其中的实体（模型从中选择作为答案）  qas中的query是Cloze式问题，’@placeholder’是被masked的地方，answers是模型给出的答案{  \"source\": \"Daily mail\",   \"passage\": {    \"text\": \"By Mail On Sunday Reporter PUBLISHED: 16:04 EST, 11 May 2013 | UPDATED: 01:58 EST, 13 May 2013 ...\", \t\"entities\": [\t  {\"start\": 3, \"end\": 16}, \t  {\"start\": 237, \"end\": 241}, \t  {\"start\": 262, \"end\": 268}, \t  {\"start\": 331, \"end\": 360}, \t  {\"start\": 418, \"end\": 423}, \t  {\"start\": 446, \"end\": 453}, \t  {\"start\": 465, \"end\": 481}, \t  {\"start\": 554, \"end\": 566}, \t  {\"start\": 656, \"end\": 663}, \t  {\"start\": 690, \"end\": 706}\t]  },   \"qas\": [    {\t  \"query\": \"of Royal-watchers, as well as the @placeholder\\u2019s own staff, some of whom could\", \t  \"answers\": [\t    {\"start\": 262, \"end\": 268, \"text\": \"Charles\"}, \t\t  {\"start\": 418, \"end\": 423, \"text\": \"Prince\"}\t  ], \t  \"idx\": 38\t}  ],   \"idx\": 23}诊断数据集SuperGLUE设计了两个诊断数据集：AX-b和AX-gAX-b数据集Broadcoverage Diagnostics（AX-b）继承自GLUE的诊断数据集，仅修改了格式{  \"idx\": \"0\",   \"label\": \"not_entailment\",   \"sentence1\": \"The cat sat on the mat.\",   \"sentence2\": \"The cat did not sit on the mat.\",   \"logic\": \"Negation\"}{  \"idx\": \"1\",   \"label\": \"not_entailment\",   \"sentence1\": \"The cat did not sit on the mat.\",   \"sentence2\": \"The cat sat on the mat.\",   \"logic\": \"Negation\"}AX-g数据集Winogender Schema Diagnostics（AX-g）来自一个关注模型性别偏见的任务【8】AX-g每对句子除了性别代词，其余都一样，要求模型对其分别做相同的蕴含推理（对于premise，判断hypothesis是否合理：entailment/not_entailment），以此诊断模型是否有性别偏见{  \"hypothesis\":\"The accountant sought help filing taxes.\",  \"idx\":0,  \"label\":\"not_entailment\",  \"pair_id\":551646,  \"premise\":\"The taxpayer met with the accountant to get help filing his taxes.\"}{  \"hypothesis\":\"The accountant sought help filing taxes.\",  \"idx\":1,  \"label\":\"not_entailment\",  \"pair_id\":551648,  \"premise\":\"The taxpayer met with the accountant to get help filing her taxes.\"}评估指标从评估角度可以将各子任务分为  COPA、WiC、RTE、WSC、BoolQ和CB都是分类任务（其中CB是三分类，其余都是二分类）  MultiRC和ReCoRD是多选任务其中多选任务的评估有两个思路：  转化为分类问题：表6中的$F1_a$就是将每个answer可视为一个二分类问题，计算其F1，然后取均值  计算精确匹配度：表6中的EM就是计算预测匹配到正确答案的百分比SuperGLUE以BERT为基础模型计算BaseLine图7. SuperGLUE的baseline模块化工具包SuperGLUE还提出了一个模块化的工具包jiant【9】  基于PyTorch开发  支持多任务学习、迁移学习、50+NLU任务，支持GLUE、SuperGLUE和XTREME基准测试文献【1】Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark and analysis platform for natural language understanding[J]. arXiv preprint arXiv:1804.07461, 2018.【2】Warstadt A, Singh A, Bowman S R. Neural network acceptability judgments[J]. Transactions of the Association for Computational Linguistics, 2019, 7: 625-641.【3】Socher R, Perelygin A, Wu J, et al. Recursive deep models for semantic compositionality over a sentiment treebank[C]//Proceedings of the 2013 conference on empirical methods in natural language processing. 2013: 1631-1642.【4】Williams A, Nangia N, Bowman S R. A broad-coverage challenge corpus for sentence understanding through inference[J]. arXiv preprint arXiv:1704.05426, 2017.【5】Wang A, Pruksachatkun Y, Nangia N, et al. Superglue: A stickier benchmark for general-purpose language understanding systems[J]. Advances in neural information processing systems, 2019, 32.【6】Roemmele M, Bejan C A, Gordon A S. Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning[C]//AAAI spring symposium: logical formalizations of commonsense reasoning. 2011: 90-95.【7】Zhang S, Liu X, Liu J, et al. Record: Bridging the gap between human and machine commonsense reading comprehension[J]. arXiv preprint arXiv:1810.12885, 2018.【8】Rudinger R, Naradowsky J, Leonard B, et al. Gender bias in coreference resolution[J]. arXiv preprint arXiv:1804.09301, 2018.【9】Pruksachatkun Y, Yeres P, Liu H, et al. jiant: A software toolkit for research on general-purpose text understanding models[J]. arXiv preprint arXiv:2003.02249, 2020."
  },
  
  {
    "title": "预训练大模型的Prompt Tuning",
    "url": "/posts/%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84Prompt-Tuning/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-04-23 00:00:00 +0000",
    





    
    "snippet": "本文主要参考刘鹏飞2021年Prompt Tuning的综述【1】推荐阅读：  CMU 刘鹏飞：NLP的第四范式  一文轻松入门Prompt(附代码)  Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章  大模型Prompt Tuning技术分享1. 什么是Prompt  Prompt是预训练大模型PLM与人类之间的桥梁，可以使PLM更准确理解人类的...",
    "content": "本文主要参考刘鹏飞2021年Prompt Tuning的综述【1】推荐阅读：  CMU 刘鹏飞：NLP的第四范式  一文轻松入门Prompt(附代码)  Fine-tune之后的NLP新范式：Prompt越来越火，CMU华人博士后出了篇综述文章  大模型Prompt Tuning技术分享1. 什么是Prompt  Prompt是预训练大模型PLM与人类之间的桥梁，可以使PLM更准确理解人类的表达  Prompt通过在输入中添加额外信息，从而更好的使用PLM中的知识1.1 研究范式刘鹏飞将近代NLP技术的发展总结为4种范式：1）非神经网络时代的完全监督学习特征工程：需要人工设计特征2）基于神经网络的完全监督学习结构工程：省掉了特征工程，但需要人工设计网络结构3）预训练+精调范式目标函数工程：研究额外的目标函数到预训练语言模型上，使其适配下游任务从BERT开始，Pre-train + fine-tuning的研究范式迅速席卷NLP领域这种方式使用下游任务的数据对预训练模型做微调，在CV领域已被广泛应用。但大模型对硬件和训练数据的要求很高，训练成本很高4）预训练+提示+预测范式Prompt挖掘工程：不追求改动预训练语言模型，希望通过对合适Prompt的利用重新定义下游任务建模的方式从GPT-3开始，Pre-train + Prompt + Predict的范式受到了极大关注。文章【1】就是对该范式的综述这种方式认为大模型已蕴含了足够的知识，因此对于下游任务，只需要构造某种提示(Prompt)，引导大模型输出下游任务所需的结果即可图1. 预训练+精调范式 VS 预训练+提示+预测范式图1是对后两种范式的对比：  Fine-tuning: 是预训练模型“迁就”各种下游任务。通过引入辅助任务loss对预训练模型做再训练，使其适配下游任务  Prompting: 是各种下游任务“迁就”预训练模型。通过对不同任务做重构，使其适配预训练大模型1.2 一个例子假设下游任务为情感分类（输出一个语句是positive的还是negative的）1）对于预训练+精调范式的范式模型层面需要在大模型的基础上添加2维linear layer+softmax实现二分类输出训练层面需要首先设计损失函数，然后使用该任务对应的数据集对修改的模型做fine-tuning2）对于预训练+提示+预测范式的范式可以首先构造一个模板：Input: x=I Love this movieTemplate: [x] Overall, it was a [z] movie.模板中的[x]位置赋值为输入，让大模型填充其中的[z]（大模型比如BERT的训练方式就是做完形填空）。模型可能会填入’fantastic’、’boring’等词语，最后将这些词语映射到positivate和negativate，就完成了下游任务的输出。2. Prompt有哪些方法图2. Prompt的分类2.1 从模板形状分类模板形状可以理解为示例中[z]的位置1）Cloze填充文本字符串空白的完形填空，如示例的形式，[z]处于模板的中间位置适用于使用掩码mask形式进行训练的大模型，如BERT2）Prefix延续字符串前缀，[z]处于模板的末尾适用于与生成任务或使用标准自回归的大模型（从左至右的方式），如GPT2.2 从模板生成方式1）手工模板工程如示例根据下游任务的需要人工设计相应的模板这种方法很直接，但需要大量实验和丰富的经验，很难发现最佳的Prompt此外模型对Prompt非常敏感，不同的模板得到的效果差别很大2）自动化模板学习自动化搜索的Prompt将该工作交给训练，让模型找到最好的表示这类方法根据生成的Prompt是token-level的还是vector-level的可以进一步分成Discrete和Continuous两种其中continuous将Prompt表示成向量，不再关注显性语义上的表现2.3 多重Prompt方法上述方法主要集中与为输入构建单个Prompt，但大量研究表明，multi-Prompt可以进一步提升Prompt的效果目前主要有以下5类multi-Prompt方法图3. 多重Prompt方法3. Answer工程从上文中的例子可以看到，Prompt方法不仅需要研究如何生成提示模板，还需要将基于提示的大模型输出与下游任务label做映射（如’fantastic’ -&gt; positivate）图4. answer工程的算法分类4. 预训练模型的选择给定下游任务或者Prompt，如何选择合适的预训练模型模型分类如下所示，一般来说有以下对应关系可以参考  Cloze类型的Prompt（如NLU类的下游任务）适用于Masked LM  Prefix类型的Prompt适用于Left-to-Right LM  生成类的下游任务适用于Encoder-Decoder LM4.1 预训练大模型的分类按网络结构一般可将预训练大模型分类为3种1) Left-to-Right LM以GPT【15】、GPT-3【4】为代表是一类自回归LM，通过链式规则从左至右计算单词序列$x$的概率，以此预测即将出现的单词\\[\\begin{aligned}P(x)&amp;=P(x_1)P(x_2|x_1)...P(x_n|x_1,...x_{n-1})\\\\x&amp;=x_1,x_2,...x_n\\end{aligned} \\tag{1}\\]2) Masked LM以BERT【21】、RoBERTa【22】为代表自回归语言模型虽然功能强大，但只能从左到右计算，对于如分类等下游任务，这种方式并不是必须的Masked Language Model(MLM)是一种基于上下文对masked位置做预测（完形填空）的模型，即：\\[P(x_i|x_1,x_2,...x_{i-1}x_{i+1},...x_n) \\tag{2}\\]3) Encoder-Decoder LM如UniLM【18】、T5【19】、BART【20】翻译、摘要等文本生成任务的输入和输出都是文本序列，因此需要模型具有编码和解码能力：  首先对输入做编码  然后使用自回归（从左到右）对输出做解码目前有两类主流模型用于解决该问题：Prefix LM(如UniLM【18】)、Encoder-decoder LM(如BART【20】、T5【19】)图5. 预训练语言模型的分类5. Prompt如何训练5.1 从训练数据层面可以将基于Prompt的训练分成以下三种1）Zero-shot不使用任何关于下游任务的数据，如1.2所示的例子，此时如果搭配人工设计的Prompt模板可以无需任何训练2）Few-shot使用少量（如100组）针对下游任务的数据做训练3）Full-data使用大量（如10K）针对下游任务的数据做训练5.2 从参数更新维度如图6所示，在训练时对于PLM，可以冻结其参数更新，也可以允许更新；对于Prompt，可以使用无参数的形式，也可以选择对其参数做冻结等策略图6. Prompt训练策略按上述两部分的组合，目前主要有以下几种训练策略            策略名称      策略说明      应用场景      代表方法                  Promptless Fine-tuning      PLM参数可调，无Prompt      有足够多的训练样本      ELMo【27】、BERT【21】、BART【20】              Tuning-free Prompting      PLM参数冻结，Prompt无参数      训练样本较少      GPT-3【4】、LAMA【2】、AutoPrompt【28】              Fixed-LM Prompt Tuning      PLM参数冻结，Prompt参数可调      训练样本较少      Prefix-Tuning【10】、Prompt-Tuning【29】              Fixed-prompt LM Tuning      PLM参数可调，Prompt参数冻结      有足够多的训练样本      PET-TC【17】、PET-Gen【5】、LM-BFF【30】              Prompt+LM Fine-tuning      PLM参数可调，Prompt参数可调      有足够多的训练样本      PADA【31】、P-Tuning【12】、PTR【32】      Prompt有哪些局限该部分内容来自P-Tuning【12】的作者刘潇1）缺少规模通用性作者发现P-Tuning能够达到fine-tuning的效果需要满足以下条件  10B左右或更高的参数量  特殊的初始化技巧（label initrialization）  LM adaptation那么对于1B-10B的中小规模模型，该如何使用Prompt？2）缺少任务通用性对于简单任务（如分类任务），Prompt很好设计，效果也不错但对于困难任务（如ner、阅读理解等nlu任务），Prompt该如何设计？此外即使对于分类任务，如果类别并没有特定语义，该如何做answer工程？作者基于以上问题，提出了P-TuningV2【33】参考文献【1】Liu P, Yuan W, Fu J, et al. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing[J]. ACM Computing Surveys, 2023, 55(9): 1-35.【2】Petroni F ,  Rocktschel T ,  Lewis P , et al. Language Models as Knowledge Bases?[J].  2019.【3】Cui L ,  Wu Y ,  Liu J , et al. Template-Based Named Entity Recognition Using BART[J].  2021.【4】Brown T B ,  Mann B ,  Ryder N , et al. Language Models are Few-Shot Learners[J].  2020.【5】Schick T , H Schütze. Few-Shot Text Generation with Pattern-Exploiting Training[J].  2020.【6】Jiang Z ,  Xu F F ,  Araki J , et al. How Can We Know What Language Models Know?[J].  2019.【7】Ben-David E ,  Oved N ,  Reichart R . PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains[J].  2021.【8】Wallace E ,  Feng S ,  Kandpal N , et al. Universal Adversarial Triggers for Attacking and Analyzing NLP:, 10.18653/v1/D19-1221[P]. 2019.【9】Feldman J ,  Davison J ,  Rush A M . Commonsense Knowledge Mining from Pretrained Models[J]. arXiv, 2019.【10】Li X L ,  Liang P . Prefix-Tuning: Optimizing Continuous Prompts for Generation[J].  2021.【11】Tsimpoukelli M ,  Menick J ,  Cabi S , et al. Multimodal Few-Shot Learning with Frozen Language Models[J].  2021.【12】Liu X ,  Zheng Y ,  Du Z , et al. GPT Understands, Too[J].  2021.【13】Schick T , H Schütze. It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners[J].  2020.【14】Jiang Z ,  Anastasopoulos A ,  Araki J , et al. Multilingual Factual Knowledge Retrieval from Pretrained Language Models[J].  2020.【15】Alec Radford and Karthik Narasimhan. 2018. Improving language understanding by generative pre-training.In arXiv.【16】Hambardzumyan K ,  Khachatrian H ,  May J . WARP: Word-level Adversarial ReProgramming[J]. arXiv e-prints, 2020.【17】Schick T , H Schütze. Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference[C]// Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2021.【18】Dong L ,  Yang N ,  Wang W , et al. Unified Language Model Pre-training for Natural Language Understanding and Generation:, 2019.【19】 Raffel C ,  Shazeer N ,  Roberts A , et al. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer[J].  2020(140).【20】 Lewis M ,  Liu Y ,  Goyal N , et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension[C]// Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2020.【21】Devlin J ,  Chang M W ,  Lee K , et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[J].  2018.【22】Liu Y ,  Ott M ,  Goyal N , et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach[J].  2019.【23】Yuan W ,  Neubig G ,  Liu P . BARTScore: Evaluating Generated Text as Text Generation[J].  2021.【24】Mishra S ,  Khashabi D ,  Baral C , et al. Cross-Task Generalization via Natural Language Crowdsourcing Instructions[J]. arXiv e-prints, 2021.【25】Lu Y ,  Bartolo M ,  Moore A , et al. Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity[J].  2021.【26】Han X, Zhao W, Ding N, et al. Ptr: Prompt tuning with rules for text classification[J]. AI Open, 2022, 3: 182-192.【27】Peters M , Neumann M , Iyyer M , et al. Deep Contextualized Word Representations[J]. 2018.【28】Shin T ,  Razeghi Y ,  Logan I , et al. AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts[J].  2020.【29】Lester B ,  Al-Rfou R ,  Constant N . The Power of Scale for Parameter-Efficient Prompt Tuning[J].  2021.【30】Gao T ,  Fisch A ,  Chen D . Making Pre-trained Language Models Better Few-shot Learners[J].  2020.【31】Ben-David E ,  Oved N ,  Reichart R . PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains[J].  2021.【32】Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021. Ptr: Prompt tuning with rules for text classification.【33】Liu X, Ji K, Fu Y, et al. P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks[J]. arXiv preprint arXiv:2110.07602, 2021."
  },
  
  {
    "title": "ViT图像分类",
    "url": "/posts/ViT%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB/",
    "categories": "深度学习",
    "tags": "",
    "date": "2023-03-23 00:00:00 +0000",
    





    
    "snippet": "2020年google提出ViT（Vision Transformer）【1】在图像领域ViT其实并不是第一个使用注意力机制的方法，但这些方法没有改变CNN总体结构，只是替换了其中的部分模块ViT的意义在于：证明了摈弃CNN的结构，使用和NLP完全一致的纯Transformer结构也能获得相同甚至更好的效果ViT的提出开启了CV的一个新时代，同时对多模态的研究有极大促进作用图1. Paper...",
    "content": "2020年google提出ViT（Vision Transformer）【1】在图像领域ViT其实并不是第一个使用注意力机制的方法，但这些方法没有改变CNN总体结构，只是替换了其中的部分模块ViT的意义在于：证明了摈弃CNN的结构，使用和NLP完全一致的纯Transformer结构也能获得相同甚至更好的效果ViT的提出开启了CV的一个新时代，同时对多模态的研究有极大促进作用图1. PapersWithCode模型排名如图1，在PapersWithCode的图像分类任务上，排名靠前的全部都是基于transformer的模型推荐参考资料  《Transformer论文逐段精读【论文精读】》  《ViT论文逐段精读【论文精读】》Transformer背景在NLP中目前的主流方法是：使用自注意力机制尤其是Transformer搭建模型，首先在大型数据集上做预训练，然后针对具体任务做fine-tune一个让人惊讶的发现是，随着模型和数据的增长，目前还没有发现性能饱和的情况注意力机制自注意力机制是Transformer的核心文章attention is all you need【2】提出了一种简单的实现Scaled Dot-Product Attention图2. 自注意力机制图2中的key维度为$n\\times d_k$，value维度为$n\\times d_v$，query维度为$m\\times d_k$其中$n$和$m$代表元素个数，$d_k$和$d_v$代表每个元素的特征维度需要确保key的特征维度与query保持一致，key的元素数量（序列长度）需要与value保持一致这种设计确保了同一个模型可以接受不同长度的输入（query）自注意力的过程就是：对于某个query，计算其与key的内积得到$m\\times n$的权重矩阵，然后基于该权重对value做加权和，输出$m\\times d_v$的结果  内积其实就是计算query与key的相似度，权重矩阵中$(i,j)$的值就是query中第$i$个元素与key中第$j$个元素的相似度  输出矩阵中$(i,j)$的值就是query中第$i$个元素的输出向量  每一个输出都是整个value的加权和，因此其感受野是整个key图3. 自注意力计算过程\\[Attention(Q,K,V)=softmax(\\frac{QK^T}{\\sqrt{d_k}})V \\tag{1}\\]式1是文章【2】Scaled Dot-Product Attention的计算公式  $softmax$是为了将结果归一化（$n$个元素，每个元素分别做归一化）  除以$\\sqrt{d_k}$是因为当$d_k$比较大时（论文中取512），$softmax(QK)$的分布会偏向0或1，导致梯度很小，不利于训练另外在文章【2】的实际应用中，将同一个输入复制3次分别作为Q、K、V，因此称为自注意力机制多头注意力机制多头注意力机制（图2右）首先通过线性层将Q、K、V投影到多个低维空间，在每个空间中做Scaled Dot-Product Attention后再做concat，最后再通过一个线性层投影回原来的空间维度：假设输入维度为$d_{model}$，注意力层为$h$，那么每层（头）的输出维度为$d_{model}/h$，这样就可以确保多头注意力concat后的输出维度依然为$d_{mnodel}$从式1可以看到，其实单个Scaled Dot-Product Attention并没有可以学习的参数，多头注意力中的多个投影矩阵就是可学习的参数关于Mask图2左在做softmax之前还做了个mask操作回顾自注意力过程，每一个输出，其感受野都是整个key但对于NLP中的解码阶段，第$i$个输出只与key的前$i-1$个元素相关，因此需要一个mask将$i$后面的权重置0图4. Transformer模型结构图4所述的Transformer结构包含了encoder部分（左）和decoder部分（右）  其中encoder部分包含了1个多头自注意力模块，其query、key、value是输入的3份拷贝  其中decoder部分包含了1个带mask的多头自注意力模块和1个多头自注意力模块，其中模块3的key和value来自encoder的输出，query来自模块2的输出正弦位置编码Transformer需要额外的位置编码来描述位置信息《Transformer中的位置编码》总结了目前主流的位置编码方案本文采用了正弦位置编码（图4 Positional Encoding）\\[\\begin{aligned}PE_{(pos,2i)}&amp;=sin(pos/10000^{2i/d_{model}})\\\\PE_{(pos,2i+1)}&amp;=cos(pos/10000^{2i/d_{model}})\\end{aligned} \\tag{2}\\]Batch Norm &amp; Layer Norm概念对比图5. 红框：batchnorm；绿框：layernorm如图5所示，假设batch=4，每个样本的特征是1维，长度为6  BatchNorm: 对该batch内所有样本，依次计算每个特征维度上的均值和方差，对该维度的特征值做归一化（如红框所示）  LayerNorm: 依次对每个样本计算其均值和方差，对该特征向量做归一化（如绿框所示）其中BatchNorm最终还需学习一个对全部样本的每个特征维度上的均值和方差，这样才能在推理阶段做归一化而LayerNorm因为是对样本自身计算均值和方差，因此不需要学习全局的参数LayerNorm的意义在NLP中，一个样本是一条语句，包含若干token，而每个token又要通过一个特征向量表示因此一个样本的维度为$n\\times k$，其中$n$为token数量图6. NLP中的LayerNorm图6所示1个batch中有5个样本（句子），每个样本的长度不一  如果使用BatchNorm，因为样本长度的差异，会给计算带来较大误差，不同batch的均值和方差抖动会很大；另外在推理过程中，全局均值和方差的估计也无法适应未知长度的样本  而LayerNorm因为只针对每个样本计算均值和方差，可以无视样本长度差异带来的影响ViT的网络设计要将Transformer应用在图像领域，首先要解决的是2d图像数据到1d序列数据的形式转换一个容易想到的方法就是：从上到下从左到右对像素点做重排比如对于$224\\times 224\\times 3$的图像，重排后就是$1\\times 150528$然而目前的硬件只能处理最多上千位的序列长度，如文章【2】中默认的序列长度$n=512$，因此上述方案是不现实的为了降低序列长度，ViT之前的代表性方法有以下2种：1）CNN + Transformer使用CNN（比如ResNet）对图像做特征提取和降维，然后输入给Transformer（CVPR2018【3】）2）局部自注意力文章【4】(NIPS2019)将图像分成一个个局部窗口，每个窗口用Transformer文章【5】(ECCV2020)分别沿H和W做Transformer，以此降低序列长度以上这些方法虽然理论上效率高，但因为使用的不是通用的方法，没有做硬件加速优化，因此将其组成大模型还是比较困难的图7. ViT的网络设计ViT 对输入的调整ViT首先将图像切成若干个二维窗口，将每个窗口的像素reshape为一维向量，将其类比为一个token的向量表示原文将图像切成$16\\times 16$的二维窗口，这就是文章标题的来源: AN IMAGE IS WORTH 16X16 WORDS''' 将图像切分为一个个patch，每个patch相当于一个token的向量表示    将该向量表示先做LayerNorm，再通过线性层改变维度，最后再做一次LayerNorm'''# 确定一共有多少个patch，每个patch拉成一个序列的长度num_patches = (image_size[0]//patch_size[0]) * (image_size[1]//patch_size[1])patch_dim = channels*patch_size[0]*patch_size[1]# 输出为[barch,num_patches,len_features]self.to_patch_embedding = nn.Sequential(    Rearrange('b c (h p1) (w p2) -&gt; b (h w) (p1 p2 c)', p1=patch_size[0], p2=patch_size[1]),    nn.LayerNorm(patch_dim),    nn.Linear(patch_dim, dim),    nn.LayerNorm(dim)    )输入Patch加入类别标签和位置标签ViT使用一个与patch维度一致的向量表示类别，将其与patch在1维做concat（这样”token”数量就为$16\\times 16+1$）然后再将位置标签与之相加，增加位置信息，输出Embedded Patchesself.pos_embedding = nn.Parameter(torch.randn(1, num_patches+1, dim))self.cls_token = nn.Parameter(torch.randn(1,1,dim))...cls_tokens = repeat(self.cls_token, '1 1 d -&gt; b 1 d', b=b)x = torch.cat((cls_tokens, x), dim=1)x += self.pos_embedding[:, :(n+1)]x = self.dropout(x)多头注意力结构如图7右所示，将输入Embedded Patches首先复制3次，分别作为KQVclass Attention(nn.Module):    def __init__(self, dim, heads=8, dim_head=64, dropout=0):        super().__init__()        inner_dim = dim_head*heads        project_out = not (heads==1 and dim_head==dim)        self.heads = heads        self.scale = dim_head ** -0.5        self.attend = nn.Softmax(dim=-1)        self.dropout = nn.Dropout(dropout)        self.to_qkv = nn.Linear(dim, inner_dim*3, bias=False)        self.to_out = nn.Sequential(            nn.Linear(inner_dim, dim),            nn.Dropout(dropout)        ) if project_out else nn.InstanceNorm1d()    def forward(self, x):        # 复制3次作为qkv, 按heads数量做切分        qkv = self.to_qkv(x).chunk(3, dim=-1)        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&gt; b h n d', h=self.heads), qkv)        # attention计算（式1）        dots = torch.matmul(q, k.transpose(-1,-2)) * self.scale        attn = self.attend(dots)        attn = self.dropout(attn)        out = torch.matmul(attn, v)        out = rearrange(out, 'b h n d -&gt; b n (h d)')        return self.to_out(out)分类输出Transformer出来的尺寸为$batch\\times (1+patches)\\times dim$可以选择对$(1+patches)$取平均作为类别标签，也可以只取其中第1个（输入处理时作为位置标签添加的）最后再经过一个线性层将特征维度规约到$num_classes$self.mlp_head = nn.Sequential(    nn.LayerNorm(dim),    nn.Linear(dim, num_classes))...x = self.transformer(x)x = x.mean(dim=1) if self.pool == 'mean' else x[:,0]x = self.mlp_head(x)实验及讨论位置编码的消融实验作者尝试了无位置编码、1D、2D和相对位置编码，发现无位置编码效果较差，后3种最终的效果差不多图8. 位置编码的消融实验类别标签的对比实验在分类输出时  可以选择对$(1+patches)$取平均作为类别标签  也可以只取其中第1个（Bert采用的方法）作者经过实验发现两者对精度并没有什么影响（因为在attention过程中每个元素的信息都会考虑，因此最后用哪个来表征类别就没什么区别了）图9. 类别标签的对比实验图中GAP即为采用平均的方法，需要注意的是两类方法的$lr$是有差别的，GAP取$3e-4$的效果就会差不少，因此调参很重要与CNN的对比实验图10. 与CNN的对比实验图10左是在不同大小数据集上预训练，在ImageNet上的TOP1精度  可以看到当预训练数据集较小（ImageNet）时，ViT的精度比不上BiT（基于ResNet）  当采用中大型数据集做预训练时，ViT开始追上并超越BiT图10右的横坐标是计算量FLOP，纵坐标是精度，可以看到  相同计算量下ViT的精度更高  Hybrid是指用CNN提取特征，Transformer做分类。在计算量较低时这种方式精度更高，但随着计算量提升就没有明显优势了以上两组实验说明：1）对于小型数据集，还是推荐使用CNN；但对于中大型数据集，Transformer效果更好2）Tansformer的计算复杂度比CNN更低ViT的短板1）相对于CNN结构，ViT更难训练，且需要更大的数据集作者提出了一个归纳偏置的概念：  CNN的设计充分考虑的图像数据的先验信息：局部相似性、平移不变性，并且也没有改变元素之间的相对位置关系  但Transformer的结构只能通过训练去学习这些信息，即使是位置编码，在网络初始化的时候也是随机赋值的2）fine-tuning的适配性理论上分辨率越高最终的精度越高，但ViT模型中patch的数量和每个patch的维度都已固定，提高分辨率就意味着这些维度都要变文章对此采用插值的方法，但可以想象如果分辨率差别很大肯定会掉点代码官方代码实现了多篇文章提出的方法这里参考第三方pytorch实现，重新整理，添加了训练和评估代码文献【1】Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.【2】Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.【3】Wang X, Girshick R, Gupta A, et al. Non-local neural networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7794-7803.【4】Ramachandran P, Parmar N, Vaswani A, et al. Stand-alone self-attention in vision models[J]. Advances in neural information processing systems, 2019, 32.【5】Wang H, Zhu Y, Green B, et al. Axial-deeplab: Stand-alone axial-attention for panoptic segmentation[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV. Cham: Springer International Publishing, 2020: 108-126."
  },
  
  {
    "title": "Transformer中的位置编码",
    "url": "/posts/Transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/",
    "categories": "LLM",
    "tags": "",
    "date": "2023-03-22 00:00:00 +0000",
    





    
    "snippet": "CNN和RNN都包含了位置信息，但使用Attention机制的Transformer从结构上丢失了位置信息，因此需要额外的位置编码本文整理自苏剑林的博文《让研究人员绞尽脑汁的Transformer位置编码》1. 绝对位置编码令$x_k$是输入的第$k$个向量，绝对编码就是在其基础上加上位置向量$p_k$变成$x_k+p_k$其中$p_k$只依赖于位置$k$为什么用加法？这其实除了具有几何意义...",
    "content": "CNN和RNN都包含了位置信息，但使用Attention机制的Transformer从结构上丢失了位置信息，因此需要额外的位置编码本文整理自苏剑林的博文《让研究人员绞尽脑汁的Transformer位置编码》1. 绝对位置编码令$x_k$是输入的第$k$个向量，绝对编码就是在其基础上加上位置向量$p_k$变成$x_k+p_k$其中$p_k$只依赖于位置$k$为什么用加法？这其实除了具有几何意义外，对于深度学习来说都没有什么区别（回顾网络结构中相加和concat的处理）事实上也有文章研究使用乘法组合位置编码1.1 训练式该类方法最早由ConvS2S（2017）【1】提出，BERT（2018）【2】也使用了该方案方法将位置编码当做可训练参数比如令最大长度为$n$，编码维度为$m$，就初始化一个$n\\times m$的矩阵作为位置编码，让它随着训练更新该方案简单有效，但存在外推性的缺点：最多只能处理长度为$n$的句子当然苏剑林在文章《层次分解位置编码，让BERT可以处理超长文本》中提出了一种解决办法能有效规避这个问题1.2 三角式也称为Sinusoidal正弦位置编码，是《Attention is All You Need》（2017）【3】提出的方法其提出显式的计算方法，给定位置pos和向量维度，可以唯一确定一个位置编码\\[\\begin{aligned}PE_{(pos,2i)}&amp;=sin(pos/10000^{2i/d_{model}})\\\\PE_{(pos,2i+1)}&amp;=cos(pos/10000^{2i/d_{model}})\\end{aligned} \\tag{1}\\]其中$pos$是输入的位置，$i$是维度，位置编码的维度与输入向量维度$d_{model}$一致，这样才能最后将位置编码与输入向量相加比如对于$d_{model}=128$，位置$pos=3$的输入，按式1，其对应的位置编码向量为\\[\\left[sin\\left(\\frac{3}{10000^{0/128}}\\right),cos\\left(\\frac{3}{10000^{1/128}}\\right),...,cos\\left(\\frac{3}{10000^{127/128}}\\right)\\right]\\]图1. tl：pos=0；tr：pos=1；bl：pos=2；br：pos=3图1为$d_{model}=128$，$pos$分别取0,1,2,3时正弦位置编码的波形图相比训练式，该方法有两个特点：1）不再需要训练，此外因为生成规则是确定的，理论上有一定外推性2）提供了表达相对位置信息的可能性\\[\\begin{aligned}sin(\\alpha+\\beta)&amp;=sin\\alpha cos\\beta+cos\\alpha sin\\beta\\\\cos(\\alpha+\\beta)&amp;=cos\\alpha cos\\beta-sin\\alpha sin\\beta\\end{aligned} \\tag{2}\\]这说明位置$\\alpha+\\beta$的向量可以表示成位置$\\alpha$和$\\beta$的向量组合另外个人看来使用sinusoidal位置编码有点使用载波做调制的意思1.3 递归式由于串行的结构设计，RNN不需要额外的位置编码因此另一种思路是：输入-&gt;RNN-&gt;Transformer，通过RNN学习一种绝对位置编码ICML2020的论文【4】提出了FLOATER方案，用微分方程$dp_t/dt=h(p_t,t)$的方式建模位置编码其中函数$h(p_t,t)$可以通过神经网络来建模，因此也称为神经微分方程理论上正弦位置编码是FLOATER的一个特解很明显，递归式中RNN的应用牺牲了一定的并行性2. 相对位置编码绝对位置编码对不同位置计算编码，但没有考虑不同位置间的关系考虑以下两个句子：  有个东西在吃鱼  有条鱼在吃东西显然“东西”的含义跟其与“鱼”的相对位置有关  对于绝对位置编码，只能期望模型通过训练能够学习到位置之间的关系  相对位置编码在计算注意力权重（通过query和key计算应用在value的权重）的时候就引入了两个token之间的相对位置信息考虑一般形式的带绝对位置编码的Attention\\[\\begin{aligned}q_i&amp;=(x_i+p_i)W_Q\\\\k_j&amp;=(x_j+p_j)W_K\\\\v_j&amp;=(x_j+p_j)W_V\\\\a_{i,j}&amp;=softmax(q_ik_j^T)\\\\o_i&amp;=\\sum_ja_{i,j}v_j\\end{aligned} \\tag{3}\\]注意key和value的维度一致，因此query与key的相对位置就是query与value的相对位置（$p_j$）1）权重计算阶段对其中的$q_ik_j^T$展开，有\\[\\begin{aligned}q_ik_j^T&amp;=(x_i+p_i)W_QW_K^T(x_j+p_j)^T\\\\&amp;=(x_iW_Q+p_iW_Q)(W_K^Tx_j^T+W_K^Tp_j^T)\\\\&amp;=x_iW_QW_K^Tx_j^T+x_iW_QW_K^Tp_j^T+p_iW_QW_K^Tx_j^T+p_iW_QW_K^Tp_j^T\\end{aligned} \\tag{4}\\]上式4项展开可以分别理解为“输入-输入”、“输入-位置”、“位置-输入”、“位置-位置”2）加权输出阶段对其中的$o_i$展开，有\\[\\begin{aligned}o_i&amp;=\\sum_ja_{i,j}v_j\\\\&amp;=\\sum_ja_{i,j}(x_jW_V+p_jW_V)\\end{aligned} \\tag{5}\\]2.1 经典式以google2018的论文【5】为代表1）权重计算阶段该方法删除了式4中“位置-输入”、”位置-位置“两项，并将“输入-位置”中的$W_K^Tp_j^T$合并为一个$R_{i,j}^K$，如式6所示\\[q_ik_j^T=x_iW_Q(x_jW_K+R_{i,j}^K)^T \\tag{6}\\]这样query到key的相对位置信息在$R_{i,j}^K$学习2）加权输出阶段将式5中的$p_jW_V$合并为$R_{i,j}^V$，如式7所示\\[o_i=\\sum_ja_{i,j}(x_jW_V+R_{i,j}^V) \\tag{7}\\]这样query到value的相对位置信息在$R_{i,j}^V$中学习位置截断此外为了规避外推性的问题，方法对$R_{i,j}^K$和$R_{i,j}^K$设定了固定长度$2k+1$比如对于$k=2$，有\\[R_{i,j}^K=[w_{-2},w_{-1},w_0,w_1,w_2]\\]其中$w_{-2}$表示query位置$i$与key位置$j$的差$i-j=-2$对于位置差大于$\\pm 2$的情况使用截断，即\\[\\begin{aligned}R_{i,j}^K&amp;=w_{clip(j-i,k)}^K\\\\R_{i,j}^V&amp;=w_{clip(j-i,k)}^V\\\\clip(x,k)&amp;=max(-k,min(k,x))\\end{aligned} \\tag{8}\\]2.2 XLNET式该方法出自论文Transformer-XL（2019）【6】1）权重计算阶段将式4中的$p_j$替换为$R_{i-j}$（使用了正弦位置编码方案，不需学习），将$p_i$替换为两个可学习的向量$u$、$v$2）加权输出阶段将式5中$v_j$（query到value）的偏置$p_j$去掉，即$v_j=x_jW_V$\\[\\begin{aligned}q_ik_j^T&amp;=x_iW_QW_K^Tx_j^T+x_iW_QW_K^TR_{i-j}^T+uW_QW_K^Tx_j^T+vW_QW_K^TR_{i-j}^T\\\\o_i&amp;=\\sum_ja_{i,j}x_jW_V\\end{aligned} \\tag{9}\\]似乎从Transformer-XL开始，相对位置编码都只考虑query到key的了2.3 T5式该方法出自论文（2020）【7】，微软ICLR2021的论文【8】提出的TUPE位置编码也使用了类似思路该方法认为输入信息与位置信息应该是独立的，不应该有过多交互1）权重计算阶段删除式3中的“输入-位置”、“位置-输入”又因为”位置-位置“对应的$p_iW_QW_K^Tp_j^T$其实是一个只依赖于$(i,j)$的标量，因此将其简化为可学习参数$\\beta_{i,j}$2）加权输出阶段将式5中$v_j$（query到value）的偏置$p_j$去掉，即$v_j=x_jW_V$\\[\\begin{aligned}q_ik_j^T&amp;=x_iW_QW_K^Tx_j^T+\\beta_{i,j}\\\\o_i&amp;=\\sum_ja_{i,j}x_jW_V\\end{aligned} \\tag{10}\\]此外不同于截断的思路，T5在$\\beta_{i,j}$中对相对位置做“分桶”处理基本准则是邻近的位置使用不同位置编码，距离远的共用相同位置编码            i-j      0      1      2      3      4      5      6      7      8      9      10      11      12      13      …                  f(i-j)      0      1      2      3      4      5      6      7      8      8      8      8      9      9      …      2.4 DeBERTa式出自微软ICLR2021论文DeBERTa【9】该方法主要改进就是在位置编码上，在SuperGLUE上成绩超过了T5该方法认为大多数任务可能只需要相对位置信息，但有些场景可能也需要绝对位置信息，因此将模型分成encoder和decoder两部分，encoder部分使用相对位置编码，decoder部分使用绝对位置编码1）权重计算阶段DeBERTa删除式3的最后一项“位置-位置”将“输入-位置”、“位置-输入”两项换为相对位置编码2）加权输出阶段将式5中$v_j$（query到value）的偏置$p_j$去掉，即$v_j=x_jW_V$\\[\\begin{aligned}q_ik_j^T&amp;=x_iW_QW_K^Tx_j^T+x_iW_QW_K^TR_{i,j}^T+R_{j,i}W_QW_K^T\\\\o_i&amp;=\\sum_ja_{i,j}x_jW_V\\end{aligned} \\tag{11}\\]3. 其他位置编码3.1 CNN式对于CNN捕捉位置编码能力的解释目前有2种：1）卷积核的各向异性关于各向异性参考博文中没有展开解释但本人更倾向于认为多层的CNN虽然改变了特征图的尺寸，但没有改变其相对位置关系2）Zero Padding该解释出自2020年的论文【9】在CNN中为了保证特征维度的一致性，一般需要做zero padding，而该操作可能提供给当前位置与边界的相对距离信息3.2 复数式出自2019年的论文【11】方法对每个词$j$使用三组与位置无关的词向量表示\\[\\begin{aligned}r_j&amp;=[r_{j,1},r_{j,2},...,r_{j,d}]\\\\w_j&amp;=[w_{j,1},w_{j,2},...,w_{j,d}]\\\\\\theta_j&amp;=[\\theta_{j,1},\\theta_{j,2},...,\\theta_{j,d}]\\end{aligned} \\tag{12}\\]其位置编码为\\[\\left[r_{j,1}e^{i\\left(w_{j,1}^{k+\\theta_{j,1}}\\right)},r_{j,2}e^{i\\left(w_{j,2}^{k+\\theta_{j,2}}\\right)},...,r_{j,d}e^{i\\left(w_{j,d}^{k+\\theta_{j,d}}\\right)}\\right] \\tag{13}\\]其中$j$表示词，$k$表示该词的位置，$i$是虚数单位值得说明的是【11】的作者Benyou Wang将模型也做了复数化改造，实现了复数版的Transformer、Fasttext、LSTM、CNN等3.3 旋转位置编码RoPE这是博文作者苏剑林在2021年发表的Roformer【12】中使用的一种融合式位置编码由于绝对位置编码具有实现简单、计算速度高等优点，而相对位置编码考虑了相对位置关系，实际性能更好因此RoPE设计思想是：使用绝对位置编码的方式实现相对位置编码其基本思路是首先给每个token添加绝对位置信息，然后对其做内积，方法借助复数计算证明了内积中包含相对位置信息\\[Re[f(q,m)f^*(k,n)]=g(q,k,m-n) \\tag{14}\\]上式中$q$和$k$为不同的token，$m$和$n$为其绝对位置具体分析参考作者博文《Transformer升级之路：2、博采众长的旋转式位置编码》文献【1】Gehring J, Auli M, Grangier D, et al. Convolutional sequence to sequence learning[C]//International conference on machine learning. PMLR, 2017: 1243-1252.【2】Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018.【3】Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.【4】Liu X, Yu H F, Dhillon I, et al. Learning to encode position for transformer with continuous dynamical model[C]//International conference on machine learning. PMLR, 2020: 6327-6335.【5】Shaw P, Uszkoreit J, Vaswani A. Self-attention with relative position representations[J]. arXiv preprint arXiv:1803.02155, 2018.【6】Dai Z, Yang Z, Yang Y, et al. Transformer-xl: Attentive language models beyond a fixed-length context[J]. arXiv preprint arXiv:1901.02860, 2019.【7】Raffel C, Shazeer N, Roberts A, et al. Exploring the limits of transfer learning with a unified text-to-text transformer[J]. The Journal of Machine Learning Research, 2020, 21(1): 5485-5551.【8】Ke G, He D, Liu T Y. Rethinking positional encoding in language pre-training[J]. arXiv preprint arXiv:2006.15595, 2020.【9】He P, Liu X, Gao J, et al. Deberta: Decoding-enhanced bert with disentangled attention[J]. arXiv preprint arXiv:2006.03654, 2020.【10】Islam M A, Jia S, Bruce N D B. How much position information do convolutional neural networks encode?[J]. arXiv preprint arXiv:2001.08248, 2020.【11】Wang B, Zhao D, Lioma C, et al. Encoding word order in complex embeddings[J]. arXiv preprint arXiv:1912.12333, 2019.【12】Su J, Lu Y, Pan S, et al. Roformer: Enhanced transformer with rotary position embedding[J]. arXiv preprint arXiv:2104.09864, 2021."
  },
  
  {
    "title": "【对话系统】Rasa Core对话管理-决策组件、默认Actions、Fallback机制（7）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa-Core%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86-%E5%86%B3%E7%AD%96%E7%BB%84%E4%BB%B6-%E9%BB%98%E8%AE%A4Actions-Fallback%E6%9C%BA%E5%88%B6-7/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-20 00:00:00 +0000",
    





    
    "snippet": "在domain.yml中可以设置对话特性session_config:  session_expiration_time: 60           # 本轮对话处于没有输入输出状态持续超过多少分钟则失效                                        # 设为0则表示永不超时  carry_over_slots_to_new_session: true # 新...",
    "content": "在domain.yml中可以设置对话特性session_config:  session_expiration_time: 60           # 本轮对话处于没有输入输出状态持续超过多少分钟则失效                                        # 设为0则表示永不超时  carry_over_slots_to_new_session: true # 新的对话会不会继承旧对话的slot对话开始有3种方式：  用户首次连接rasa开始对话  在一段时间的inactive后又开始对话  在一段时间的inactive后检测到session_start意图对话管理决策算法在config.yml的policies下定义对话管理决策的算法组件，输出bot下一步的actionpolicies:  # 检查当前对话是否与任意story相匹配，若匹配则根据story预测动作  - name: MemoizationPolicy  # 使用机器学习预测下一步动作  - name: TEDPolicy    max_history: 5    epochs: 100    constrain_similarities: true  # 基于规则预测动作，动作预测得分太低则用utter_ask_rephrase  - name: RulePolicy    core_fallback_threshold: 0.1    core_fallback_action_name: \"utter_ask_rephrase\"  可以同时设置多种（默认最大10种）决策组件，rasa选择置信度最高的action决策  当多个组件输出的置信度一致时，选择优先级更高的作为最终决策，如果优先级也一致，则随机选择其中一种输出  不推荐同时设置多个优先级一样的决策组件决策组件从算法原理上可以分成基于机器学习的和基于规则的关于决策组件更详细内容可参考官方文档Policies基于机器学习的决策组件基于机器学习的决策组件通过对stories.yml中定义的story的学习实现决策结果的输出，会扩充已有的故事线表格中数字越大表示该组件的优先级越高      决策算法名称    优先级    主要参数        TEDPolicy    1    epochs: fine-tunning的迭代次数，默认为1        max_history: 算法考虑的历史对话数量，默认为None即考虑所有历史对话        constrain_similarities: 设置为True则使用交叉熵损失，使模型泛化能力更好        use_gpu: 设置为False则强制使用CPU做训练        UnexpecTEDPolicy    2    epochs、max_history、use_gpu与上文一致        ignore_intents_list: 被忽略的意图list        tolerance: 取值范围[0,1]，取0表示训练时所有的负样本都会触发        MemoizationPolicy    3    max_history与上文一致        AugmentedMemoizationPolicy    3    max_history与上文一致  1）UnexpecTEDIntentPolicyUnexpecTEDIntentPolicy是一个辅助组件，其只能输出一个特殊的默认动作action_unlikely_intent，因此需要配合其他决策组件一起使用UnexpecTEDIntentPolicy会随机挑选story中的片段，与随机挑选的意图intent合并，形成负样本，在对话过程中如果发现进入负样本story，则触发action_unlikely_intent通过这种方式UnexpecTEDIntentPolicy可以为story的完善提供参考2）AugmentedMemoizationPolicyAugmentedMemoizationPolicy在MemoizationPolicy的基础上增加了遗忘机制，可以删除特定数量的历史对话基于规则的决策组件基于规则的决策组件根据rule.yml中定义的规则匹配实现决策结果的输出，不会对其做扩充      决策算法名称    优先级    主要参数        RulePolicy    6    core_fallback_threshold: 默认0.3，输出置信度小于该阈值则调用core_fallback_action_name        core_fallback_action_name: 默认action_default_fallback，指定输出置信度小于阈值时的action        enable_fallback_prediction: 默认true        check_for_contradictions: 默认true，此时会在训练之前检查slot和active loops在所有rules中是否明确或冲突        restrict_rules: 默认true，此时会限定仅针对一个用户做回复，不建议改为false  RulePolicy会考虑所有历史对话实现决策输出训练数据增强仅TEDPolicy支持数据增强的训练可以使用rasa train --augmentation 50指定数据增强因子为50，这样在训练开始前会产生最多50*10=500个增强后的故事默认Actionsrasa内置了默认的动作，详细说明见官方文档Default Actions      action名称    执行操作        action_listen    等待用户的下一个输入        action_restart    重置所有对话历史，包括所有的slot，由回复utter_restart触发        action_session_start    开启新的对话，重置tracker，但默认不会清除slot        action_default_fallback    当决策器输出的置信度小于阈值，默认发送utter_defalut动作        action_deactive_loop    deactive表单，并且重置请求过的slots        action_two_stage_fallback    用于NLU输出置信度低的情况        action_default_ask_affirmation    用于NLU输出置信度低的情况，向用户确认意图，可以根据场景自定义          action_default_ask_rephrase    用于NLU输出置信度低的情况，向用户确认意图，当用户否认时执行的动作         action_back    撤销上一轮对话         action_unlikely_intent    返回的是一个意图，由UnexpecTEDIntentPolicy触发，可以通过编写rule指定该意图的动作，也可以重载action_unlikely_intent，在函数内触发下一步动作         action_extract_slots    在用户输入后，决策器进行预测前执行，按指定规则提取并验证slot值   Two-Stage Fallback该部分详细资料参考官方文档fallback-handoff当NLU输出的置信度较低时，可以通过Two-Stage Fallback机制向用户进一步询问和确定意图，其基本流程为1）rasa向用户确认意图（该意图是NLU分类器得到的最大置信度的那个意图）2）如果用户肯定了意图，则按该意图开始后面的对话；如果用户否认，则请求用户再表达一次3）用户再次表达的语句如果意图置信度高，则按该意图开始后面的对话；否则再次向用户确认意图4）如果用户肯定了意图，则按该意图开始后面的对话；如果用户否认，则触发action_default_fallback可以看到在Two-Stage Fallback机制中，最差的情况用户需要两次确认意图，重复表达两次机制实现方法1）config.yml：在NLU pipeline中添加FallbackClassifier分类器，在policies中添加RulePolicy预测器pipeline:  ...  # 对意图分类器的结果再做一次筛选，如果小于指定阈值则认为意图识别失败  - name: FallbackClassifier    threshold: 0.7            # 最大的意图得分小于该阈值    ambiguity_threshold: 0.1  # 最大的两个意图得分之差需要大于该值policies:  # 基于规则预测动作  - name: RulePolicy2）domain.yml：定义回复模板utter_default和utter_ask_rephraseresponses:  # 用户最后一次否认后自动调用的模板  utter_default:    # action_default_fallback会自动调用该回答    - text: \"我是您的用药小顾问，请问有什么可以帮您？\"  # 要求用户重复表达时自动调用的模板  utter_ask_rephrase:    - text: \"不好意思我不太懂您的意思，能说明确一点吗？\"3）rules.yml：定义对应rule- rule: Implementation of the Two-Stage-Fallback  steps:  # 当意图分类器的置信度小于0.7时会返回nlu_fallback意图  # 但action_two_stage_fallback每次会根据(意图分类器输出的最高置信度对应的意图)做询问  - intent: nlu_fallback  - action: action_two_stage_fallback  - active_loop: action_two_stage_fallback图1. Two-Stage-Fallback对话机制改进的思路从图1可以看到该机制会使用猜测的意图自动组成buttom语句：Did you mean 'intent_name'?在中文场合下这有点违和，并且intent_name是开发者指定的，用户未必能理解，因此需要对action_two_stage_fallback做改进：1）将询问语句换成中文2）对每个intent_name设置对应的用户易懂的映射具体实现方式有2种：1）在actions.py中重写函数，同时在domain.yml中声明该action# actions.pyclass ActionTwoStageFallback(LoopAction):    def name(self):        return \"action_two_stage_fallback\"    async def run(        self,        dispatcher: CollectingDispatcher,        tracker: Tracker,        domain,    ):        ...        return []# domain.yml# 在actions中声明后,rasa首先会在actions.py中找对应的实现并注册# 否则会使用默认的实现actions:  - action_two_stage_fallback2）改写源代码rasa中对action_two_stage_fallback的实现在rasa/core/actions/two_stage_fallback.py中class TwoStageFallbackAction(LoopAction):    def __init__(self, action_endpoint: Optional[EndpointConfig] = None) -&gt; None:        self._action_endpoint = action_endpoint    def name(self) -&gt; Text:        return ACTION_TWO_STAGE_FALLBACK_NAME    ...该方法的实现还是比较复杂的，因此直接修改源代码可能更方便"
  },
  
  {
    "title": "【对话系统】Rasa Core对话管理-回复Response、Action Server、事件Event（6）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa-Core%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86-%E5%9B%9E%E5%A4%8DResponse-Action-Server-%E4%BA%8B%E4%BB%B6Event-6/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-17 00:00:00 +0000",
    





    
    "snippet": "对话管理训练数据Response回复Response定义了机器人对用户的回复信息，该信息是预先定义好的，由rasa根据决策结果挑选执行  Response通常在domain.yml中定义，也可以放在一个单独的文件responses.yml中  Response支持富文本信息，包括text、image和buttons，同时支持custom自定义信息  Response名称必须以utter_开头...",
    "content": "对话管理训练数据Response回复Response定义了机器人对用户的回复信息，该信息是预先定义好的，由rasa根据决策结果挑选执行  Response通常在domain.yml中定义，也可以放在一个单独的文件responses.yml中  Response支持富文本信息，包括text、image和buttons，同时支持custom自定义信息  Response名称必须以utter_开头参考官方文档Responsesresponses:  # 文本信息  utter_greet:    # rasa会从同一个类别中的多条信息随机挑选一个    - text: \"您好，请问您哪里不舒服？\"    - text: \"您好，我是您的用药小顾问，请问有什么可以帮您？\"  utter_goodbye:    - text: \"再见\"  # 图片链接  utter_cheer_up:    - text: \"Here is something to cheer you up:\"      image: \"https://i.imgur.com/nGF1K8f.jpg\"  # 按钮信息  utter_greet:    - text: \"Hey! Would you like to purchase motor or home insurance?\"      buttons:        - title: \"Motor insurance\"          payload: '/informinsurance'        - title: \"Home insurance\"          payload: '/informinsurance'buttons信息按钮信息buttons包含两个key：title、payload：  title：按钮上面的文字  payload：当用户点击该按钮，返回给rasa的信息payload的格式为/intent_nameentity_name，其中的/会被正则解释器RegexInterpreter捕捉rasa收到payload的信息后会修改nlu预测的意图和实体，从而影响对话过程图1. 按钮信息自定义信息格式根据与rasa连接的对话前端的特性，可以使用custom编写该前端支持的信息格式# 以json格式发送给slack前端responses:  utter_take_bet:  - custom:      blocks:      - type: section        text:          text: \"Make a bet on when the world will end:\"          type: mrkdwn        accessory:          type: datepicker          initial_date: '2019-05-21'          placeholder:            type: plain_text            text: Select a date在Response中使用变量可以在response的语句中使用变量（槽slot），执行时rasa会使用该变量的值做更新responses:  utter_greet:    # name是一个slot的名称    - text: \"Hey, {name}. How are you?\"指定Response对应的channelchannel是指rasa与前端连接的通道，在credentials.yml中实现配置###### 当前使用的channel是scoketio ######facebook:#  verify: \"&lt;verify&gt;\"#  secret: \"&lt;your secret&gt;\"#  page-access-token: \"&lt;your page access token&gt;\"#slack:#  slack_token: \"&lt;your slack token&gt;\"#  slack_channel: \"&lt;the slack channel&gt;\"#  slack_signing_secret: \"&lt;your slack signing secret&gt;\"socketio: user_message_evt: user_uttered bot_message_evt: bot_uttered session_persistence: false可以指定某条response所述的channelresponses:  utter_ask_game:  # 这条信息只针对slack通道，其他通道不能收到  - text: \"Which game would you like to play on Slack?\"    channel: \"slack\"  # 这条信息对所有通道都有效  - text: \"Which game would you like to play?\"每种类型的回复中要确保至少有一条信息是不指定通道的，否则在类似rasa shell和rasa interactive等场景就无法正常工作了给Response指定限制条件可以通过指定某些槽slot的状态限制该response能否被选中slots:  logged_in:    type: bool    influence_conversation: False    mappings:    - type: custom  name:    type: text    influence_conversation: False    mappings:    - type: customresponses:  utter_greet:    - condition:        - type: slot          name: logged_in          value: true      text: \"Hey, {name}. Nice to see you again! How are you?\"    - text: \"Welcome. How is your day going?\"使用condition选项，在其内部设置type、name(slot的名称)、value(slot的值)需要注意value: true和value: 'true'的区别，前者将其作为bool类型，后者将其作为string类型Response的内部优先级rasa在确保能正常输出的前提下尽可能挑选更精准的response：1）首先查找是否有满足channel限制和condition限制的response2）如果不满足上述条件，则查找仅满足channel限制的response3）如果不满足上述条件，则查找仅满足condition限制的response4）如果不满足上述条件，则查找无限制的response在上述的每一步，如果有1条，则输出，如果有多条，则随机输出其中1条自定义actionResponse的方式只能实现模板化的回复，无法适用更复杂的场景（比如查询数据库生成回复，或者执行天气查询动作，甚至打开电灯等）rasa通过rasa-sdk向开发者开放了自定义action的能力，开发者需要  在actions/actions.py中实现自定义action  在domain.yml的actions中声明自定义action的名称（必须以action_开头）rasa core运行的同时，还需通过rasa run actions开启action server，当需要执行自定义action时向action服务器发送POST请求（信息格式为json），后者返回执行完毕的结果（json格式）给到core自定义action不仅可以用于针对意图的自定义动作，还可以在表单form场景下验证、提取slot值，动态增删slot，自定义表单提问动作，这部分内容参考【对话系统】Rasa Core对话管理-表单form（4）自定义动作的输入与输出对于针对意图的自定义动作，action server接收rasa core发送的json格式信息，返回以json格式封装的events和response信息来影响对话进程详细资料参考官方文档action-server#### rasa core发送给action server的json格式信息# 包含 预测的下一步action名称、会话ID、tracker信息、domain信息{  \"next_action\": \"string\",        # actions.py中应该有该action的实现  \"sender_id\": \"string\",          # 客户端(channel)id，actions.py可以根据不同id做不同反应  \"tracker\": {    \"conversation_id\": \"default\",    \"slots\": {},                  # domain.yml中定义的所有slot以及它们此时的值    \"latest_message\": {},         # 最近一次的对话信息    \"latest_event_time\": 1537645578.314389,  # 最近一个被tracker记录的event的时间戳    \"followup_action\": \"string\",    \"paused\": false,              # 该会话是否处于暂停状态    \"events\": [],                 # 该会话产生的所有的events    \"latest_input_channel\": \"rest\", # 最近一次用户信息的来源channel    \"active_loop\": {},            # 当前处于激活状态的表单form    \"latest_action\": {}           # 上一次执行的action  },  \"domain\": {    # domain.yml中的静态信息，一般actions.py不会使用    \"config\": {},    \"session_config\": {},    \"intents\": [],    \"entities\": [],    \"slots\": {},    \"responses\": {},    \"actions\": [],    \"forms\": {},    \"e2e_actions\": []  },  \"version\": \"version\"  # rasa core的版本}#### action server返回给rasa core的json格式信息# 需要包含events和responses{  # action server产生的事件event，通过这些event可以影响对话进程  \"events\": [            {                # 事件类型为slot，事件内容是给名称为temperature的slot赋值30                \"event\": \"slot\",                \"timestamp\": null,                \"name\": \"temperature\",                \"value\": \"30\"            }        ],  # action server产生的回复信息，可以为富文本形式，也可以为特定channel兼容的格式  \"responses\": [    {\"text\": \"This is your weather forecast!\"}    ]}自定义动作的实现自定义动作通过继承类Action，改写其name和run函数实现from typing import Text, Dict, Any, Listfrom rasa_sdk import Actionfrom rasa_sdk.events import SlotSetclass ActionCheckRestaurants(Action):   def name(self) -&gt; Text:     ''' 返回action的名称，需要与domain.yml中声明的一致         action server通过调用该函数注册所有自定义action     '''      return \"action_check_restaurants\"   async def run(self,           dispatcher: CollectingDispatcher,           tracker: Tracker,           domain: Dict[Text, Any]) -&gt; List[Dict[Text, Any]]:      ''' 自定义action的入口函数          输入： dispatcher - 生成回复信息                 tracker    - 当前用户的跟踪器，用来获取slot值、历史信息、当前激活的表单等                 domain     - 静态的domain.yml内容，一般不使用          输出： List[Dict[Text, Any]] - 返回的envents                 async是python中协程函数的关键字，配合await实现单线程内的并发      '''      # 可以返回自定义富文本信息(text、image、buttons、accachment等)      dispatcher.utter_message(text=\"looking for context...\")      # 可以返回domain.yml中定义好的utter_信息      dispatcher.utter_message(response=\"utter_greet\")      # 获取slot值      cuisine = tracker.get_slot('cuisine')      q = \"select * from restaurants where cuisine='{0}' limit 1\".format(cuisine)      result = db.query(q)      # 使用SlotSet产生slot事件      return [SlotSet(\"matches\", result if result is not None else [])]在终端通过命令开启action server### rasa core和action server在同一台服务器rasa run actions                      # 默认查找actions/目录rasa run actions --actions ACTION_PKG # 指定actions package所在的目录### 指定action core监听的地址 SANIC_HOST=192.168.69.150 rasa run actions事件Eventrasa中的对话可以表示为一系列的事件events，自定义action也是通过event来影响对话的事件Event可能通过动作Action产生（此时该事件由rasa自动产生），也可能由用户自定义产生      Event类型    事件描述    产生方式        slot    给某个slot赋值    可由rasa自动产生，也可通过用户调用SlotSet产生        reset_slots    将所有的slots重置为null    只能通过用户调用AllSlotsReset产生        reminder    为某个意图设置自动触发时间    只能通过用户调用ReminderScheduled产生        cancel_reminder    取消某个意图的自动触发    只能通过用户调用ReminderCancelled产生        pause    暂停对话，rasa不再回复用户信息    只能通过用户调用ConversationPaused产生        resume    恢复被暂停的对话，rasa只对之后的用户信息做回复    只能通过用户调用ConversationResumed产生        followup    绕过预测器，强制执行指定action    只能通过用户调用FollowupAction产生        rewind    还原上一条用户信息的所有副作用，并删除上一条用户事件    只能通过用户调用UserUtteranceReverted产生        undo    还原上一条bot动作的所有副作用，并删除上一条bot动作    只能通过用户调用ActionReverted产生        restart    重置tracker，之前所有的对话信息包括该restart事件全部删除    只能通过用户调用Restarted产生        session_started    开启一个新的对话，会迁移所有的SlotSet事件到新的对话中    第一次开始对话或者上轮对话失效后重新恢复对话时自动产生，也可以通过用户调用SessionStarted产生        user    当用户发送一条信息给bot时产生    自动产生，一般不会由用户调用UserUttered产生        bot    当bot发送一条信息给用户时产生    bot返回response时产生，一般不会由用户调用BotUttered产生        action    当bot执行任意一个action时产生    bot执行action时产生，一般不会由用户调用ActionExecuted产生  ## slot类型事件{    \"event\": \"slot\",    \"name\": \"departure_airport\",  # slot名称    \"value\": \"BER\"                # 填入的值}## reminder类型事件{  \"event\": \"reminder\",  \"intent\": \"my_intent\",                                  # 针对的意图名称  \"entities\": {\"entity1\": \"value1\", \"entity2\": \"value2\"}, # 需要发送给该意图的实体  \"date_time\": \"2018-09-03T11:41:10.128172\",              # 意图触发时间  \"name\": \"my_reminder\",                                  # 该事件的id  \"kill_on_user_msg\": true,                               # 用户信息是否会中断该事件}## cancel_reminder类型事件{  \"event\": \"cancel_reminder\",  \"name\": \"my_reminder\",                          # 该事件的id  \"intent\": \"my_intent\",                          # 针对的意图名称  \"entities\": [        {\"entity\": \"entity1\", \"value\": \"value1\"}, # 需要发送给该意图的实体        {\"entity\": \"entity2\", \"value\": \"value2\"},    ],  \"date_time\": \"2018-09-03T11:41:10.128172\",      # 事件触发时间}## followup类型事件{    \"event\": \"followup\",    \"name\": \"my_action\"  # 将被强制执行的action名称}## user类型事件{    \"event\": \"user\",    \"text\": \"Hey\",            # 用户信息内容    \"parse_data\": {           # NLU对信息的输出        \"intent\": {            \"name\": \"greet\",            \"confidence\": 0.9        },        \"entities\": []    },    \"metadata\": {},           # 用户信息中附带的meta数据}## action类型事件{    \"event\": \"action\",    \"name\": \"my_action\"       # 被执行的action名称}"
  },
  
  {
    "title": "【对话系统】Rasa Core对话管理-故事story、规则rule（5）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa-Core%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86-%E6%95%85%E4%BA%8Bstory-%E8%A7%84%E5%88%99rule-5/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-16 00:00:00 +0000",
    





    
    "snippet": "对话管理训练数据意图intent、实体entity、槽slot和表单forms是对话管理中的基本元素，在此基础上对话管理还需要有控制对话走向的能力rasa中通过Stories和Rules为对话走向决策算法提供训练数据：  Stories为基于机器学习的决策算法提供训练数据，决策算法根据历史对话可以产生未在story配置文件中设计好的对话走向  Rules为基于规则的决策算法提供训练数据，一旦...",
    "content": "对话管理训练数据意图intent、实体entity、槽slot和表单forms是对话管理中的基本元素，在此基础上对话管理还需要有控制对话走向的能力rasa中通过Stories和Rules为对话走向决策算法提供训练数据：  Stories为基于机器学习的决策算法提供训练数据，决策算法根据历史对话可以产生未在story配置文件中设计好的对话走向  Rules为基于规则的决策算法提供训练数据，一旦决策算法发现用户意图与某个rule匹配，则强制执行rule配置文件中设计好的对话走向故事StoriesStories数据一般单独在data/stories.yml中完成配置stories:- story: Greet the user  metadata:    author: Somebody    key: value  steps:  # list of steps  - intent: greet  - action: utter_greetStories的组成元素有  story：故事的名称，可以随便设置，不参与训练  metadata：该故事的相关信息，可写可不写，不参与训练  steps：故事线的走向描述，参与训练其中steps可以由以下元素组成  intent：用户意图，由NLU输出  action：机器人需要执行的动作，在domain.yml中声明，可以为response也可以为action  form：表单，其实也可理解为机器人需要执行的动作  slot_was_set事件：指定slot是否被填充为指定值，可以作为后面动作执行的条件  or表达式：只要满足or下的某个条件，就可以执行后面的动作  checkpoint：是一个标记，可以实现多个story的连接intent元素决策算法根据用户意图和实体值来预测下一步动作因此可以进一步在intent内指定实体值（是可选的），确保story更明确此外参考《【对话系统】Rasa Core对话管理-意图、实体、槽（3）》，可以在意图数据声明中通过use_entities和ignore_entities影响决策算法stories:- story: user message structure  steps:    - intent: intent_name  # Required      entities:  # Optional      - entity_name: entity_value    - action: action_nameaction元素该元素定义当前流程机器人应该执行什么动作机器人可以执行的动作只有2种：1）Response在domain.yml中预设的回复模板，模板名称需要以utter_开头# domain.ymlresponses:  utter_greet:    - text: \"您好，请问您哪里不舒服？\"    - text: \"您好，我是您的用药小顾问，请问有什么可以帮您？\"# stories.ymlstories:- story: greet  steps:    - intent: greet    - action: utter_greetrasa会从utter_greet中随机选择一个执行response更详细的分析见【对话系统】Rasa Core对话管理-回复Response、Action Server、事件Event（6）2）Custom actions在action.py中自定义的动作，需要在domain.yml中定义，动作名称需要以action_开头# domain.ymlactions:  - action_response_query# stories.yml- story: query disease happy way  steps:    - intent: query_knowledge    - action: action_response_query自定义动作更详细的分析见【对话系统】Rasa Core对话管理-回复Response、Action Server、事件Event（6）form元素表单forms是机器人执行的一种特殊的动作，可以根据逻辑关系自动引导用户提供表单中需要的slot值stories:- story: story with a form  steps:  - intent: find_restaurant  - action: restaurant_form                # 激活form  - active_loop: restaurant_form           # 是一个条件，只有form已经被激活才往下执行  - active_loop: null                      # 是一个条件，只有form已经结束才往下执行  - action: utter_restaurant_found表单forms更详细的分析见【对话系统】Rasa Core对话管理-表单form（4）slot_was_set元素slot_was_set与action_loop一样也是一个条件语句，只有当其指定的slot满足条件时才往下执行stories:- story: story with a slot  steps:  - intent: celebrate_bot  # 只有当槽feedback_value的值为positive时才往下执行  - slot_was_set:    - feedback_value: positive  # 只有当槽feedback_value已经被赋值才往下执行  - slot_was_set:    - feedback_value  # 只有当槽feedback_value还没有被赋值才往下执行  - slot_was_set:    - feedback_value: null  - action: utter_yay注意，在story中使用slot_was_set需要确保：1）slot在声明的时候没有将influence_conversation设为false（默认为true，参考《【对话系统】Rasa Core对话管理-意图、实体、槽（3）》的slot部分）2）同时创建一个该slot取其他值的story（测试发现，基于机器学习的预测器并不会严格遵循slot_was_set的规则，因此需要增加反面数据训练）or表达式元素如果只要满足多种意图或多种slot_was_set中的一种就可以往下执行时，可以使用or表达式stories:- story: story with OR  steps:  - intent: signup_newsletter  - action: utter_ask_confirm  # 意图为affirm或thanks都可以往下执行  - or:    - intent: affirm    - intent: thanks  - action: action_signup_newsletter- story: another or  steps:  - intent: greet  - action: utter_greet  - intent: tell_name  # 槽name的值为joe或bob都可以往下执行  - or:    - slot_was_set:        - name: joe    - slot_was_set:        - name: bob  # ... next actions不要滥用or表达式，否则会使得对话系统难以维护（跟if语句太多是一个道理）checkpoint元素checkpoint放在一个story的开始或结束，被相同checkpoint标记的story会被自动收尾相连此外置于story开始位置的checkpoint还可以设置slot_was_set条件# 会自动形成story_with_a_checkpoint_1+story_with_a_checkpoint_2的storystories:- story: story_with_a_checkpoint_1  steps:  - intent: greet  - action: utter_greet  - checkpoint: greet_checkpoint- story: story_with_a_checkpoint_2  steps:  - checkpoint: greet_checkpoint  - intent: book_flight  - action: action_book_flight# checkpoint内设置了slot_was_set条件- story: story_with_a_checkpoint_3  steps:  - checkpoint: greet_checkpoint    slot_was_set:      - holiday_name: thanksgiving  - intent: greet  - action: utter_greet_thanksgiving不要滥用checkpoint，否则会使得对话系统难以维护规则RulesRules数据一般单独在data/rules.yml中完成配置rules:- rule: Say `hello` whenever the user sends a message with intent `greet`  steps:  - intent: greet  - action: utter_greetRules的组成元素有  rule：规则的名称，可以随便设置，不参与训练  steps：规则的走向描述，参与训练其中steps可以由以下元素组成  intent：用户意图，由NLU输出  action：机器人需要执行的动作，在domain.yml中声明，可以为response也可以为actionrule在训练时不会对路径做扩展，只要对话中的意图匹配到了某个rule则会按设计的路线执行（此时预测器输出的置信度为1，因此会忽略基于机器学习的story）rule设置准入条件rule可以通过conversation_start和condition选项设置准入条件1） conversation_start是否在对话开始时才被允许匹配该rulerules:- rule: Say `hello` when the user starts a conversation with intent `greet`  conversation_start: true  steps:  - intent: greet  - action: utter_greet2）condition匹配该rule的条件，有2个选项可以设置：slot_was_set和active_looprules:- rule: Only say `hello` if the user provided a name  # 只有当user_provided_name的值为true才能匹配  condition:  - slot_was_set:    - user_provided_name: true  steps:  - intent: greet  - action: utter_greet- rule: continue_form_and_query_disease  # 只有当表单处于激活状态才能匹配  condition:    - active_loop: patient_form  steps:    - intent: continue_form    - action: patient_form    - active_loop: patient_form    - active_loop : null    - action: action_response_queryrule设置结束后动作rule同时还可以设置结束后的动作：rasa执行完rule最后一个action后默认会自动进入action_listen状态，等待用户的输入如果希望其继续执行其他action（官方文档没有明说，个人理解是决策器对上次意图输出action按置信度排序的下一个），可以设置wait_for_user_input为falserules:- rule: Rule which will not wait for user message once it was applied  steps:  - intent: greet  - action: utter_greet  wait_for_user_input: falserule和forms的冲突当一个Forms被激活时，Relus中的所有rule会被忽视，除非1）表单中所有的slot均被赋值2）表单赋值过程中被其他意图打断# 在story中，表单问询过程被question_why意图打断# 向用户解释后问用户是否愿意继续，用户拒绝- story: query disease question unhappy way  steps:    - intent: query_knowledge    - slot_was_set:      - object_type: null    - action: patient_form    - active_loop: patient_form    - intent: question_why    - action: utter_explain_form    - intent: stop_form    - action: action_deactivate_loop    - active_loop : null    - action: utter_goodbye# 在rule中，设置准入条件是表单激活，用户选择同意继续- rule: continue_form_and_query_disease  condition:    - active_loop: patient_form  steps:    - intent: continue_form    - action: patient_form    - active_loop: patient_form    - active_loop : null    - action: action_response_query"
  },
  
  {
    "title": "【对话系统】Rasa Core对话管理-表单form（4）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa-Core%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86-%E8%A1%A8%E5%8D%95form-4/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-15 00:00:00 +0000",
    





    
    "snippet": "对话管理训练数据对话信息变量表单form表单form用于向用户收集信息，在订餐厅、查询数据库、订飞机票等场景中很有用处表单form的核心是使用自动化流程引导用户为预设的槽slot赋值，当所有槽赋值完毕则自动关闭更详细内容参考官方文档Forms使用表单的一般步骤1）在config.yml中添加RulePolicy策略表单form需要用到rule预测器policies:- name: RuleP...",
    "content": "对话管理训练数据对话信息变量表单form表单form用于向用户收集信息，在订餐厅、查询数据库、订飞机票等场景中很有用处表单form的核心是使用自动化流程引导用户为预设的槽slot赋值，当所有槽赋值完毕则自动关闭更详细内容参考官方文档Forms使用表单的一般步骤1）在config.yml中添加RulePolicy策略表单form需要用到rule预测器policies:- name: RulePolicy2）在domain.yml中添加forms声明forms:  patient_form:    required_slots:      - disease      - age      - gender在required_slots下填入该表单需要填充的所有槽slot3）在domain.yml中为表单中的槽添加声明和实体entities:  - disease  - age  - genderslots:  age:    # 年龄是个浮点类型，当表单patient_form激活时可以从实体中自动填充    type: float    mappings:      - type: from_entity        entity: age        conditions:          - active_loop: patient_form  gender:    # 性别是个枚举类型，当表单patient_form激活时可以从实体中自动填充    type: categorical    values:      - male      - female    mappings:      - type: from_entity        entity: gender        conditions:          - active_loop: patient_form  disease:    type: text     mappings:      - type: from_entity        entity: disease4）在domain.yml中添加表单涉及槽的问询语句responses:  utter_ask_age:    - text: 请问患者年龄是多大呢？  utter_ask_gender:    - text: 请问患者是男性还是女性？  utter_ask_disease:    - text: 请问患者哪里不舒服？当表单激活后，会按required_slots中的声明顺序依次检查槽，如果该槽为空则自动调用上述语句，让用户提供相应信息为了实现自动调用，需要确保语句名称形式为utter_ask_&lt;slot_name&gt;或utter_ask_&lt;form_name&gt;_&lt;slot_name&gt;也可以自定义action函数供表单自动调用，详见“自定义表单提问动作”5）在nlu.yml中添加上述问询语句的回复模板需要在NLU训练数据中添加包含上述slot的意图语句，确保rasa能成功提取表单需要的槽- intent: info_age  examples: |    - 我今年[18](age)岁了    - [8](age)岁    - [5](age)    - 今年[50](age)岁了    - 今年[66](age)- intent: info_gender  examples: |    - [男](gender)    - [男人](gender)    - [男](gender)的    - 是[男](gender)的    - [女](gender)    - [女人](gender)    - [女](gender)的    - 是[女](gender)的# 使用正则表达式提取年龄实体- regex: age  examples: |    - \\d{1,2,3}# 使用同义词对性别称呼做标准化表示- synonym: male  examples: |    - 男    - 男人- synonym: female  examples: |    - 女    - 女人6）编写rule或story- story: query disease  steps:    - intent: query_knowledge    # 激活表单    - action: patient_form    # 表单向用户收集信息    - active_loop: patient_form    # 当所有槽都被赋值，表单自动关闭    - active_loop : null    # 表单关闭后执行其他流程    - action: action_response_query表单的鲁棒性提升有时候用户不会按期望回答表单的问询比如当表单问“请问患者年龄是多大呢？”用户可能会反问“为什么要问我年龄？”，此时一个鲁棒的对话系统需要暂停收集信息，向用户解释原因，如果用户接受则继续收集，否则关闭表单可以通过增加story或rule来增加上述功能# 用户在表单收集过程中反问，解释后选择拒绝- story: query disease question unhappy way  steps:    - intent: query_knowledge    - action: patient_form    - active_loop: patient_form    - intent: question_why    - action: utter_explain_form    - intent: stop_form    - action: action_deactivate_loop    # rasa自带的action，实现表单的关闭，但相关的slot并不会清除    - active_loop : null    - action: utter_goodbye# 用户在表单收集过程中反问，解释后选择继续- story: query disease question happy way  steps:    - intent: query_knowledge    - action: patient_form    - active_loop: patient_form    - intent: question_why    - action: utter_explain_form    - intent: continue_form    - action: patient_form    - active_loop: patient_form    - active_loop : null    - action: action_response_query当然其中新加的意图question_why、continue_form、stop_form和动作utter_explain_form也需要做定义意图continue_form的语句一般是“好的”，“OK”等等，在对话结束时用户有可能随手回复这类语句，这样有可能造成预测器根据上述story猜测下一步是patient_form为了避免这种情况，可以将上述query disease question happy way改写为rule，同时添加表单已激活的条件rules:  # 检测到continue_form意图时，只有当表单为激活状态才执行继续表单的动作  - rule: continue_form_and_query_disease  condition:    - active_loop: patient_form  steps:    - intent: continue_form    - action: patient_form    - active_loop: patient_form    - active_loop : null    - action: action_response_query对表单内容做验证和自定义提取在表单填写的过程中，rasa只会检查槽slot是否被填入内容，因此有时候需要自定义方法验证填入的内容是否满足场景需求1）在domain.yml中添加名称为validate_&lt;form_name&gt;的action2）在action.py中继承FormValidationAction实现自定义验证方法'''    对表单RestaurantForm的槽cuisine做进一步验证    如果不在支持的菜品范围内就删除'''from typing import Text, List, Any, Dictfrom rasa_sdk import Tracker, FormValidationActionfrom rasa_sdk.executor import CollectingDispatcherfrom rasa_sdk.types import DomainDictclass ValidateRestaurantForm(FormValidationAction):    def name(self) -&gt; Text:        return \"validate_restaurant_form\"    @staticmethod    def cuisine_db() -&gt; List[Text]:        \"\"\"Database of supported cuisines\"\"\"        return [\"caribbean\", \"chinese\", \"french\"]    # 只有当表单问询cuisine，rasa根据用户回复的语句提取了该slot后才进入该函数    def validate_cuisine(        self,        slot_value: Any,        dispatcher: CollectingDispatcher,        tracker: Tracker,        domain: DomainDict,    ) -&gt; Dict[Text, Any]:        \"\"\"Validate cuisine value.\"\"\"        if slot_value.lower() in self.cuisine_db():            # validation succeeded, set the value of the \"cuisine\" slot to value            return {\"cuisine\": slot_value}        else:            # validation failed, set this slot to None so that the            # user will be asked for the slot again            return {\"cuisine\": None}此外在表单填写过程中，如果rasa自带的slot_mapping方式无法提取场景所需数据，也可以自定义mapping方法1）在domain.yml中添加名称为validate_&lt;form_name&gt;的action2）确保需要自定义提取的slot，其mappings-type为custom3）在action.py中继承FormValidationAction实现自定义提取方法from typing import Dict, Text, List, Optional, Anyfrom rasa_sdk import Trackerfrom rasa_sdk.executor import CollectingDispatcherfrom rasa_sdk.forms import FormValidationActionclass ValidateRestaurantForm(FormValidationAction):    def name(self) -&gt; Text:        return \"validate_restaurant_form\"    # 表单收集过程中每局对话之后都会进入该函数    async def extract_outdoor_seating(        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict    ) -&gt; Dict[Text, Any]:        text_of_last_user_message = tracker.latest_message.get(\"text\")        sit_outside = \"outdoor\" in text_of_last_user_message        return {\"outdoor_seating\": sit_outside}  槽slot的验证和自定义提取是在同一个自定义的类中实现，验证方法是validate_&lt;slot_name&gt;，提取方法是extract_&lt;slot_name&gt;  槽slot的自定义验证函数是：在用户回答了该slot的语句，并且通过设定好的mapping-slot方法自动填充后进入，在该方法内对填充后的值做检查和调整  槽slot的自定义提取函数是：在表单被激活的整个过程中，无论机器人还是用户的每句话产生，都会进入每一个自定义的提取函数，如果某个slot是非custom的mapping方法，则该slot已经被自动填充表单行为的动态改变默认情况下表单会根据required_slots内定义的槽slot按顺序调用utter_ask_&lt;slot_name&gt;要求用户提供slot信息但有时根据当前slot的值可能需要额外增加其他slot或者删除已经定义的slot比如在订餐厅场景中，当用户回答要订室外的桌子，那么机器人可能会需要进一步知道用户要订荫蔽处还是太阳下的位置（这个slot如果提前写在required_slots内显然不合理）from typing import Text, List, Optionalfrom rasa_sdk.forms import FormValidationActionclass ValidateRestaurantForm(FormValidationAction):    def name(self) -&gt; Text:        return \"validate_restaurant_form\"    async def required_slots(        self,        domain_slots: List[Text],        dispatcher: \"CollectingDispatcher\",        tracker: \"Tracker\",        domain: \"DomainDict\",    ) -&gt; List[Text]:        additional_slots = [\"outdoor_seating\"]        if tracker.slots.get(\"outdoor_seating\") is True:            # If the user wants to sit outside, ask            # if they want to sit in the shade or in the sun.            additional_slots.append(\"shade_or_sun\")        return additional_slots + domain_slots根据场景需求，也可以使用该方法动态删除某个在required_slots内定义的slotfrom typing import Text, List, Optionalfrom rasa_sdk.forms import FormValidationActionclass ValidateBookingForm(FormValidationAction):    def name(self) -&gt; Text:        return \"validate_booking_form\"    async def required_slots(        self,        domain_slots: List[Text],        dispatcher: \"CollectingDispatcher\",        tracker: \"Tracker\",        domain: \"DomainDict\",    ) -&gt; List[Text]:        # 因为required_slots是并行运行的，因此需要先copy再删除        # 如果直接在domain_slots上修改可能出现未知问题        updated_slots = domain_slots.copy()        if tracker.slots.get(\"existing_customer\") is True:            # If the user is an existing customer,            # do not request the `email_address` slot            updated_slots.remove(\"email_address\")        return updated_slots  动态调整表单的required_slots方法跟槽slot的验证和自定义提取在同一个自定义类中实现，重载函数required_slots  函数required_slots与槽slot的自定义提取函数的调用逻辑一致：在表单被激活的整个过程中，无论机器人还是用户的每句话产生，都会进入  额外增加的slot，也需要跟required_slots中的其他slot一样，提前在domain.py中定义，只是不需要写进forms的required_slots内requested_slotrequested_slot是rasa自动产生的一个slot，表单收集过程中会自动赋值为最近填充的slot名称比如上次对话填充了名称为cuisine的slot，那么tracker.slots会是{'requested_slot':'cuisine',...}在实际应用中，可以借助requested_slot设计不同的故事逻辑stories:- story: explain cuisine slot  steps:  - intent: request_restaurant  - action: restaurant_form  - active_loop: restaurant  # 如果requested_slot的值为cuisine才执行下面的故事  - slot_was_set:    - requested_slot: cuisine  - intent: explain  - action: utter_explain_cuisine  - action: restaurant_form  - active_loop: null自定义表单提问动作如前面“使用表单的一般步骤”中的第4步所示，可以在domain.yml中定义形如utter_ask_&lt;slot_name&gt;或utter_ask_&lt;form_name&gt;_&lt;slot_name&gt;的语句，当表单需要收集slot_name的信息时会自动调用该语句问询对于更复杂的情况，也可以在action.py中编写形如action_ask_&lt;slot_name&gt;或action_ask_&lt;form_name&gt;_&lt;slot_name&gt;的自定义函数，供表单自动调用from typing import Dict, Text, Listfrom rasa_sdk import Trackerfrom rasa_sdk.events import EventTypefrom rasa_sdk.executor import CollectingDispatcherfrom rasa_sdk import Actionclass AskForSlotAction(Action):    def name(self) -&gt; Text:        return \"action_ask_cuisine\"    def run(        self, dispatcher: CollectingDispatcher, tracker: Tracker, domain: Dict    ) -&gt; List[EventType]:        dispatcher.utter_message(text=\"What cuisine?\")        return []"
  },
  
  {
    "title": "【对话系统】Rasa Core对话管理-意图intent、实体entity、槽slot（3）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa-Core%E5%AF%B9%E8%AF%9D%E7%AE%A1%E7%90%86-%E6%84%8F%E5%9B%BEintent-%E5%AE%9E%E4%BD%93entity-%E6%A7%BDslot-3/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-14 00:00:00 +0000",
    





    
    "snippet": "Rasa Core是Rasa中负责对话管理的部分，负责记录对话过程并选择下一个动作Rasa Core以NLU输出的意图和实体为输入，使用词槽slot和表单form为内部变量保存对话信息，根据预测组件输出预设的动作action或回复response  Rasa Core在domain.yml中声明意图、实体、保存对话信息的变量、预设动作及回复  在config.yml中设置动作及回复的预测组件...",
    "content": "Rasa Core是Rasa中负责对话管理的部分，负责记录对话过程并选择下一个动作Rasa Core以NLU输出的意图和实体为输入，使用词槽slot和表单form为内部变量保存对话信息，根据预测组件输出预设的动作action或回复response  Rasa Core在domain.yml中声明意图、实体、保存对话信息的变量、预设动作及回复  在config.yml中设置动作及回复的预测组件对话管理训练数据如上文所述，该部分数据均在domain.yml文件中（也可以记录在多个文件，训练的时候指定domain的目录）# domain数据在多个文件中的情况rasa train --domain path_to_domain_directory详细的介绍参考官方文档Domain意图数据声明在intents关键字下声明NLU训练数据（nlu.yml）中涉及到的所有意图名称intents:  - greet  - goodbye  - query_knowledge对某个意图，可以选择忽略某些实体intents:  - greet1:      # 忽略该意图中的所有实体      use_entities: []  - greet2:      # 仅使用该意图中的部分实体      use_entities:        - name        - first_name  - farewell:      # 忽略该意图中的部分实体      ignore_entities:        - location        - age        - last_name实体数据声明在entities关键字下声明NLU训练数据（nlu.yml）中涉及到的所有实体名称entities:  - disease  - object_type  - attribute如果训练数据中的实体还包含角色roles和分组groups，则这里也需要做相应声明entities:   - city:                   roles:     # 训练数据中为city这个实体添加了“出发地”和“目的地”两种角色         - from         - to   - size:                   groups:    # 训练数据中提到了两个东西的尺寸size，对其做分组，第1组表示第1个东西的尺寸         - 1         - 2默认情况提取的实体会影响预测组件的输出  如果仅对特定意图，需要屏蔽某实体的影响，可以像上一节在意图声明中用use_entities和ignore_entities  如果需要对所有意图屏蔽某实体的影响，可以在实体声明中用influence_conversationentities:- location:    influence_conversation: false对话信息变量rasa中的槽slot相当于机器人的记忆，其使用字典结构记录对话过程中的信息表单form是另一种记录对话过程信息的变量，适用于需要用户提供多种信息才能给出答案的情况槽slot在slots关键字下声明对话过程中需要的槽一个槽声明的基本元素包括槽值类型type和其自动赋值方式mappingsslots:  slot_name:                       # slot的名称    type: text                     # slot存放值的类型    influence_conversation: false  # 不影响预测组件的输出(仅保存信息)    mappings:                      # slot的自动填充方式    - type: from_entity      entity: entity_name每个槽可以通过influence_conversation设置其是否影响预测组件的输出，当其设置为true时对应的槽是否被赋值会影响预测组件输出类型为Any的槽不支持设置influence_conversation比如对于问询天气的意图  如果槽city已经被赋值，预测组件直接输出查询天气并回复的action  如果槽city没有被赋值，预测组件输出询问城市的action此外可以通过initial_value为槽设定初始值slots:  num_fallbacks:    type: float    initial_value: 0   # 设置初始值为0    mappings:    - type: custom      槽类型    应用场景    说明        text    文本类型数据            bool    true/false两种数据             categorical    枚举量    同时需定义所有枚举量          float    浮点数    同时需指定数值范围，填充时会根据该范围做自动截断          list    列表    比如当实体为购物清单时         any    任意类型数据    比如当数据为字典形式时         自定义类型    自定义类型    需要重载Slot类实现  slots:  # categorical类型需要通过关键字values设置所有枚举量  risk_level:    type: categorical    values:      - low      - medium      - high    mappings:    - type: custom  # float类型需要指定min_value和max_value，自动填充时会强制将值截断到该范围  temperature:    type: float    min_value: -100.0    max_value:  100.0    mappings:    - type: custom自定义槽类型1）重载Slot类实现函数feature_dimensionality和as_feature### 自定义槽类型示例from rasa.shared.core.slots import Slotclass NumberOfPeopleSlot(Slot):    def feature_dimensionality(self):        return 2    def as_feature(self):        r = [0.0] * self.feature_dimensionality()        if self.value:            if self.value &lt;= 6:                r[0] = 1.0            else:                r[1] = 1.0        return r2）在domain.yml中使用自定义的槽类型假定上述py文件保存路径为addons/my_custom_slots.py└── rasa_bot    ├── addons    │   ├── __init__.py    │   └── my_custom_slots.py    ├── config.yml    ├── credentials.yml    ├── data    ├── domain.yml    ├── endpoints.yml那么domain.yml的声明为slots:  people:    type: addons.my_custom_slots.NumberOfPeopleSlot    influence_conversation: true    mappings:    - type: custom槽的自动填充mappingsslot使用mappings关键字设置其自动填充方式一个slot可以同时设置多种填充方式，rasa会按先后顺序尝试，一旦填充成功则忽略后面的方式      填充方式    应用场景    说明        from_entity    从抽取得到的实体得到    需要指定关联的实体名称        from_text    保存最近一次用户的语句            from_intent    只要符合设定的intent，则赋值预设的值    需要指定预设的值        from_trigger_intent    当表格form被激活时，只要符合设定的intent，则赋值预设的值    需要指定预设的值        custom    自定义的填充方法    需要指定触发的action    上述所有填充方式均可通过intent和not_intent做进一步限定，不满足意图条件的不被填充  from_entity还可以通过role和group做限定，不满足指定角色和分组的不被填充slots:  # from_entity填充方式  slot_name1:    type: any    mappings:    - type: from_entity      entity: entity_name      role: role_name              # 该实体需要同时是role_name      group: group name            # 该实体需要同时是group_name      intent: intent_name          # 该实体需要同时属于意图intent_name      not_intent: excluded_intent  # 该实体需要同时不属于意图intent_name    # 同时设置了多种填充方式，当上面的方式无法填充时使用该方式    - type: from_entity      entity: entity_name  # from_intent填充方式需要指定填充内容my_value  slot_name2:    type: any    mappings:    - type: from_intent      value: my_value      intent: intent_name      not_intent: excluded_intent    # from_trigger_intent填充方式需要指定填充内容my_value  slot_name3:    type: any    mappings:    - type: from_trigger_intent      value: my_value      intent: intent_name      not_intent: excluded_intent  # 自定义的填充方法，当指定动作action_calculate_day_of_week执行后填充该slot  day_of_week:    type: text    mappings:    - type: custom      action: action_calculate_day_of_week自动填充的条件限定1）如上节所述，可以通过intent、not_intent等方式限制slot的自动填充2）同时可以通过关键字conditions限制slot的填充3）全局范围的config定义可以控制slot的自动填充4）另外from_entity本身也有隐含的过滤机制首先讨论from_entity的隐含过滤机制当表格form被激活时  或者from_entity的关联entity_name恰好是form当前步骤所需（填充表格是一个过程，每次填一项）  或者from_entity的关联entity_name仅满足form所需的某一项slots:  departure_city:    type: text    mappings:    - type: from_entity      entity: city      role: from    - type: from_entity      entity: city  arrival_city:    type: text    mappings:    - type: from_entity      entity: city      role: to    - type: from_entity      entity: city  arrival_date:    type: any    mappings:    - type: from_entity      entity: dateforms:  your_form:    required_slots:    - departure_city    - arrival_city    - arrival_date如上例，假定your_form表格处于激活状态，当前实体抽取结果中包含一个city，但没有角色信息，那么表格所需的departure_city和arrival_city都可被填充，但这很显然是不合理的因此上述的两个限定条件可以举例为：  如果上一次对话是问“你希望目的地是哪里”，那么当前步骤其实就是填充表格arrival_city，即使city没有角色信息也应该赋值  除了上述情况，系统发现表格中同时有两个slot可以被填充同一个值，意味着缺少属性信息，则不能赋值然后讨论conditions的过滤机制conditions可以用来限定当指定表格激活时才允许自动填充slots:  slot_name:    type: text    mappings:    - type: from_text      intent: intent_name      conditions:      # 只有当your_form处于激活状态，并且表格正在询问slot_name时才自动填充      - active_loop: your_form        requested_slot: slot_name      # 只有当another_form处于激活状态才自动填充      - active_loop: another_form最后讨论全局范围的config定义在domain.yml中可以使用config控制slot的自动填充# 该选项默认为true# 当设置为false后，会阻止from_entity的自动填充config:  store_entities_as_slots: false"
  },
  
  {
    "title": "【对话系统】NLU训练数据及相关组件（2）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-NLU%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%8F%8A%E7%9B%B8%E5%85%B3%E7%BB%84%E4%BB%B6-2/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-13 00:00:00 +0000",
    





    
    "snippet": "Rasa在config.yml文件中通过pipeline确定自然语言理解（NLU）的算法组成；在data/目录下保存NLU+DM的训练数据data/目录下的yml文件主要包含nlu、stories和rules三种训练数据，每个文件均可同时包含这些类别的数据，但同一个文件中上述类别只允许出现一次version: \"3.1\"nlu:- intent: greet  examples: |    ...",
    "content": "Rasa在config.yml文件中通过pipeline确定自然语言理解（NLU）的算法组成；在data/目录下保存NLU+DM的训练数据data/目录下的yml文件主要包含nlu、stories和rules三种训练数据，每个文件均可同时包含这些类别的数据，但同一个文件中上述类别只允许出现一次version: \"3.1\"nlu:- intent: greet  examples: |    - Hey    - Hi    - hey there [Sara](name)- intent: faq/language  examples: |    - What language do you speak?    - Do you only handle english?stories:- story: greet and faq  steps:  - intent: greet  - action: utter_greet  - intent: faq  - action: utter_faqrules:- rule: Greet user  steps:  - intent: greet  - action: utter_greet注：examples后面的|是yml的语法标记，实现对下面内容的每行自动追加换行符，并且保留字符串中的特殊字符（如”,’等）为了组织结构清晰，nlu、stories和rules三种训练数据一般分别保存在不同文件内├── data│   ├── nlu.yml│   ├── rules.yml│   └── stories.ymlNLU训练数据nlu训练的目标是从用户语句中提取结构化的信息nlu训练数据包含意图（intent）、实体（entities）、同义词（synonyms）、正则表达式（regex）和查询表（lookup tables）version: \"3.1\"nlu:- intent: greet  examples: |    - hey    - hello    - hi    - 你好- intent: goodbye  examples: |    - bye    - goodbye    - 拜拜    - 再见    - 拜    - 退出    - 结束    - 好的    - 谢谢- intent: query_knowledge  examples: |    - 你都了解哪些[疾病](object_type)的知识？    - [感冒](disease)了怎么办？    - 我[感冒](disease)了    - [感冒](disease)吃什么[药](attribute)？    - [肚子痛](disease)了怎么办？    - 我[肚子痛](disease)的厉害    - [肚子痛](disease)吃什么[药](attribute)？- synonym: medicien  examples: |    - 药- synonym: diseases  examples: |    - 疾病    - 症状- synonym: cold  examples: |    - 感冒- synonym: stomach_pain  examples: |    - 肚子痛意图数据意图数据就是一个语句，分类器需要根据该语句确定用户想要做什么意图数据使用intent关键字标记，使用examples组织同一个意图的用户语句；比如对于打招呼的意图，用户可能的语句有hey、hello、你好等等可以使用/对意图做分组建立多级意图（retrieval intent），因此意图名称不能使用`/`符号nlu:- intent: chitchat/ask_name  examples: |    - What is your name?    - May I know your name?- intent: chitchat/ask_weather  examples: |    - What's the weather like today?    - Does it look sunny outside today?实体数据实体数据是语句中的关键元素，实体数据在意图数据中做标记，是对意图的作用对象的补充如对于上述例子中的“感冒吃什么药”，意图分类为查询医学知识库，但并不知道查询数据库中什么对象的什么属性因此训练数据中将“感冒”作为disease实体类别中的一种标记出来，将“药”作为attribute实体类别中的一种标记出来当然domain.yml中相应的需要对训练数据中的所有实体做声明entities:  - disease  - object_type  - attribute实际上实体标记可以使用花括号包含更丰富的信息：- I want to fly from [Berlin]{\"entity\": \"city\", \"role\": \"departure\"} to [San Francisco]{\"entity\": \"city\", \"role\": \"destination\"}.- Give me a [small]{\"entity\": \"size\", \"group\": \"1\"} pizza with [mushrooms]{\"entity\": \"topping\", \"group\": \"1\"} anda [large]{\"entity\": \"size\", \"group\": \"2\"} [pepperoni]{\"entity\": \"topping\", \"group\": \"2\"}如上，一个实体标记可以包含实体类别entity、角色定义role和分组group同义词数据对同一个类别的实体可能有不同的表达方法，如对于“番茄”，其他可能的表达有“西红柿”、“洋柿子”、“火柿子”那么可以将这些说法都映射为一个“标准”的表达“番茄”- synonym: 番茄  examples: |    - 西红柿    - 洋柿子    - 火柿子  同义词是在实体抽取之后将其映射到标准词的，只是为了后续能统一处理，并不能改善实体抽取效果  在实体数据中对每个同义词均需要有至少2个examples做训练（如包含“西红柿”的句子、包含“洋柿子”的句子等）正则表达式数据正则表达式的关键字是regex，可以用于提升意图分类效果和实体抽取效果rasa中可以使用RegexFeaturizer和RegexEntityExtractor提取特征和匹配实体提高意图分类效果此时正则表达式的作用是匹配语句中特定的字段，将其作为特征提供给意图分类器，提高分类器的分类精度nlu:- regex: help  examples: |    - \\bhelp\\b上述例子将匹配语句中的help，并使用RegexFeaturizer将其特征化，然后给到意图分类器  需要在config.yml的pipeline中添加RegexFeaturizer组件  正则化意图特征提取RegexFeaturizer并不直接预测意图，只是为分类器提供额外特征提高实体抽取效果比如对于数字字符串，可以使用正则表达式来辅助。具体来说有两种方式：1）使用RegexFeaturizer提取特征，提供给实体抽取组件目前支持RegexFeaturizer特征的实体抽取组件有：CRFEntityExtractor、DIETClassifier2）使用RegexEntityExtractor直接提取实体nlu:- regex: account_number  examples: |    - \\d{10,12}- intent: inform  examples: |    - my account number is [1234567891](account_number)    - This is my account number [1234567891](account_number)上述例子从语句中匹配10-12位的数字字符串，作为account_number类别的实体值查询表数据查询表（Lookup Tables）是一种特殊的正则化表达式，与上一节一样，使用RegexFeaturizer和RegexEntityExtractor提升意图分类效果和实体抽取效果对于一些可以归类的有限数据，如果它们不方便使用正则表达式，就可以使用查询表列举（查询表种的内容是大小写不敏感的）比如对于“国家”这个类别，无法使用正则表达式，但可以用查询表实现- lookup: country  examples: |    - Afghanistan    - Albania    - ...    - Zambia    - ZimbabweNLU训练组件NLU pipeline是一个基于有向无环图的通用框架，内部由组件连接构成  组件之间顺序关系非常重要，后面的组件可能需要前面组件的输出  有些组件是可以相互替换的，比如不同算法的分词器  有些组件是互斥的，比如分词结果不能同时由两个分词器提供  有些组件是可以同时使用的，比如提取特征的组件可以同时使用基于规则（如正则式）的组件和基于word-embedding的组件  有些组件同时支持多个功能，比如DIETClassifier既支持实体抽取，也支持意图识别图1. NLU pipeline下面为rasa中各个组件的功能情况，更详细内容参考官方文档Components语言模型组件语言模型组件是一个模型框架，通常这种框架内置了实体抽取、意图识别等算法，如果要用到这些算法就必须使用对应的模型框架如果pipeline中使用的是单独算法，不依赖某个特定模型框架，则不需要指定      组件类别    组件名称    组件上游依赖    备注        语言模型组件    MitieNLP    无    MITIE的其他组件均需要提前指定该语言模型框架        SpacyNLP    无    spaCy的其他组件均需要提前指定该语言模型框架  分词组件      组件类别    组件名称    组件上游依赖    备注        分词组件    WhitespacesTokenizer    无    按空格为英文语句分词        JiebaTokenizer    无    中文语句的分词，需要安装库(pip3 install jieba)        MitieTokenizer    MitieNLP    使用MITIE分词器，经过改造（参考官网）可以支持中文分词         SpacyTokenizer    SpacyNLP    使用spaCy分词器  特征提取组件特征提取组件获取用户输入和回复的特征，作为后续实体抽取和意图识别的输入      组件类别    组件名称    组件上游依赖    备注        特征提取组件    MitieFeaturizer    MitieNLP    返回致密特征（dense-features）        SpacyFeaturizer    SpacyNLP    返回致密特征（dense-features）        ConveRTFeaturizer    无    返回致密特征（dense-features），基于PolyAI的ConveRT模型，需要安装库(pip3 install rasa[convert])        LanguageModelFeaturizer    分词组件    返回致密特征（dense-features），基于HuggingFace的transformer库，需要下载对应语言（支持中文）的预训练模型        RegexFeaturizer    分词组件    返回稀疏特征（sparse-features），使用nlu训练数据定义的正则表达式做match，匹配成功的置1，否则置0        CountVectorsFeaturizer    分词组件    返回稀疏特征（sparse-features），使用词袋模型        LexicalSyntacticFeaturizer    分词组件    返回稀疏特征（sparse-features），提取用户输入的词法和语法特征，支撑实体抽取   实体抽取组件可以同时使用多个实体抽取（也称命名实体NER）组件，此时最好设置每种抽取组件针对不同的实体类型，否则可能会重复提取或者被覆盖      组件类别    组件名称    组件上游依赖    备注        实体抽取组件    MitieEntityExtractor    MitieNLP、分词组件    使用MITIE NER提取器，不依赖特征提取组件，不返回提取置信度        SpacyEntityExtractor    SpacyNLP    使用SpacyNLP内置的提取器，不支持再训练，不返回提取置信度        CRFEntityExtractor    分词组件、致密特征（dense-features）    基于条件随机场CRF，返回提取置信度        DucklingEntityExtractor    无    仅支持预定义的实体类别抽取（如日期、距离、时间等），返回提取置信度，需要运行Ducking服务器        DIETClassifier    无    同时支持实体抽取和意图识别，支持dense/sparse特征        RegexEntityExtractor    无    基于训练数据中的查询表或正则表达式做实体抽取        EntitySynonymMapper    实体抽取组件    基于训练数据中的同义词设置，将实体抽取组件的结果做标准化映射  意图分类组件      组件类别    组件名称    组件上游依赖    备注        意图分类组件    MitieIntentClassifier    MitieNLP、分词组件    使用MITIE的意图分类器（使用多类线性SVM实现），不依赖特征提取组件，返回分类置信度        LogisticRegressionClassifier    特征提取组件    使用逻辑回归分类器（基于scikit-learn的实现），支持dense/sparse特征，返回分类置信度        SklearnIntentClassifier    特征提取组件    使用sklearn意图分类器，仅支持dense特征，返回分类置信度        KeywordIntentClassifier    无    基于关键字匹配，训练数据中的一句话是一个关键字，返回分类置信度        DIETClassifier    特征提取组件    同时支持实体抽取和意图识别，支持dense/sparse特征，返回分类置信度        FallbackClassifier    意图分类组件    如果意图分类组件的输出置信度小于指定阈值，则分类为nlu_fallback意图        ResponseSelector    特征提取组件    当训练数据中有子类意图的情况，可以使用该组件对多级意图做一次性分类，支持dense/sparse特征  rasa将子类意图称为retrieval intents，使用/对意图再做一次分类nlu:- intent: chitchat/ask_name  examples: |    - What is your name?    - May I know your name?    - What do people call you?    - Do you have a name for yourself?- intent: chitchat/ask_weather  examples: |    - What's the weather like today?    - Does it look sunny outside today?    - Oh, do you mind checking the weather for me please?    - I like sunny days in Berlin.reponse的训练数据也需要做相应设置responses:  utter_chitchat/ask_name:  - text: Oh yeah, I am called the retrieval bot.  utter_chitchat/ask_weather:  - text: Oh, it does look sunny right now in Berlin.  ResponseSelector会根据分类结果直接选择其中某一个response，因此不再需要后续的对话管理决策器了  ResponseSelector一般用于简单的FQA对话NLU pipeline 示例因为pipeline中的组件没有使用某个语言模型框架的内置算法，因此无需添加语言模型框架pipeline:  # 分词  - name: JiebaTokenizer  # word embedding  - name: LanguageModelFeaturizer    model_name: bert    model_weights: /home/nick/data/models/bert-base-chinese  # intent classification + entity recognition  - name: DIETClassifier    epochs: 1000    learning_rate: 0.001  # 意图识别得分太低  - name: FallbackClassifier    threshold: 0.4    ambiguity_threshold: 0.1  - name: EntitySynonymMapper"
  },
  
  {
    "title": "【对话系统】Rasa架构及基本组成（1）",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90-1/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-02-10 00:00:00 +0000",
    





    
    "snippet": "本系列文章主要参考Rasa官方文档，适用Rasa版本为3.xRasa功能及产品组成Rasa是一款支持多轮会话的对话框架Rasa包含两种版本：Rasa Open Source和Rasa Platform  前者是开源版本，可以商业使用  后者是Rasa的商业解决方案，包含Rasa Pro和 Rasa X/Enterprise其中Rasa Pro在Rasa Open Source的基础上额外增加...",
    "content": "本系列文章主要参考Rasa官方文档，适用Rasa版本为3.xRasa功能及产品组成Rasa是一款支持多轮会话的对话框架Rasa包含两种版本：Rasa Open Source和Rasa Platform  前者是开源版本，可以商业使用  后者是Rasa的商业解决方案，包含Rasa Pro和 Rasa X/Enterprise其中Rasa Pro在Rasa Open Source的基础上额外增加了一些特征、API和一些满足企业在安全性、可观测性与规模方面的特定需求Rasa X/Enterprise可以提供低代码开发方式，帮助团队大规模审查和改善对话机器人，其需要与Rasa Pro捆绑使用图1. Rasa产品组成本系列文章仅针对Rasa Open Source做分析Rasa的功能模块组成从功能上来说，一个对话流程包括：自然语言理解（NLU）、对话管理（DM）、自然语言生成（NLG）Rasa实现了NLU和DM，NLG需要用户自己实现，但提供了接口以及接入标准此外Rasa使用Action Server作为NLG的“平替”，实现对用户输入的回应图2. Rasa的模块组成NLU PipelineNLU实现对用户输入语句的意图识别和实体抽取要实现这个功能需要多类算法的支持，一个典型的调用顺序为：分词(tokenizer)、词元向量化(word embedding)、实体抽取(entity recognition)、意图分类(intent classification)Rasa使用Pipeline的概念将各个子模块组合起来，可以方便的定义各子模块的具体算法和相互的调用顺序### 一个典型的nlu pipelinepipeline:  # tokenizer  - name: JiebaTokenizer  # word embedding  - name: LanguageModelFeaturizer    model_name: bert    model_weights: /home/nick/data/models/bert-base-chinese  # intent classification + entity recognition  - name: DIETClassifier    epochs: 1000    learning_rate: 0.001【对话系统】NLU训练数据及相关组件（2）会对NLU Pipeline做更详细的分析Dialogue Policies该模块实现对话管理（DM）功能：根据对话的上下文决定下一步的动作(Action)这里的Action是提前在Rasa配置文件中设定好的，因此模块其实就是决定选择其中的哪一个Rasa内置了多种决策器：基于规则的、基于机器学习的等等用户可以在配置文件中选择一个或多个决策器，Rasa会根据每个决策器输出的置信度做排序，选择置信度最高的作为最终输出。如果同时存在多个置信度最高的决策输出，则选择优先级（Rasa内部自定义）更高的那个；如果优先级也相同，则随机选择### 选择了三种决策器policies:  # 检查当前对话是否与任意story相匹配，若匹配则根据story预测动作  - name: MemoizationPolicy  # 使用机器学习预测下一步动作  - name: TEDPolicy    max_history: 5    epochs: 100    constrain_similarities: true  # 基于规则预测动作，动作预测得分太低则用utter_ask_rephrase  - name: RulePolicy    core_fallback_threshold: 0.3    core_fallback_action_name: \"utter_ask_rephrase\"【对话系统】Rasa Core对话管理-意图intent、实体entity、槽slot（3）会对Dialogue Policies做更详细的分析Action ServerAction Server通过http的POST和GET方法与Rasa通信，松耦合的设计可以使得对话系统在资源配置和功能扩展上都有极大灵活度actions是Dialogue Policies执行的对象，从形式上主要可以分为Responses、Actions和Forms其中Responses和Forms分别为回复信息和表格，两者都在rasa的配置文件中设定，不需要运行Action Server# utter_greet是针对greet意图自定义的回复信息responses:  utter_greet:    - text: \"您好，请问您哪里不舒服？\"    - text: \"您好，我是您的用药小顾问，请问有什么可以帮您？\"而Actions则是在actions.py中自己编写，可以实现更复杂的操作，比如查询数据库、对接知识图谱、发送外部信号等，需要运行Action Server【对话系统】Rasa Core对话管理-回复Response、Action Server、事件Event（6）和【对话系统】Rasa Core对话管理-决策组件、默认Actions、Fallback机制（7）会对Action和Action Server做更详细的分析Tracker Stores该模块实现对话过程的保存，保存的内容组织为类Tracker：  events：目前为止会话中所有的事件列表  sender_id：会话消息来源的id  slots：domain.yml文件中定义的slots列表  latest_message：最近对话信息的字典，包含intent、entities和text  active_loop：当前active loop的名称  latest_action_name：最近一次bot执行的action名称  _paused：当前tracker是否暂停关于Tracker的属性和方法参考官方文档Trackerrasa内置了多种Tracker实现：InMemoryTrackerStore、SQLTrackerStore、RedisTrackerStore、MongoTrackerStore、DynamoTrackerStore，rasa同时也支持重载Tracker实习自定义的方法其中InMemoryTrackerStore是默认方法，将过程保存在内存中，重启rasa服务器后信息丢失；其余方法都是通过http的方式连接数据库，相关参数在endpoints.yml中配置更详细说明参考官方Tracker StoresLock Storesrasa使用票据锁机制确保同一个会话ID中输入信息可以按时间先后顺序被正确处理Lock Stores也是通过http与rasa实现通信，目前rasa内置了多种实现：  InMemoryLockStore：默认方法，单进程实现，仅支持一个rasa服务器运行  RedisLockStore：使用Redis，是多个rasa服务器并行场景的推荐方法  ConcurrentRedisLockStore：使用Redis，是一种新的能支持多个rasa服务器并行运行的方法同时rasa也支持自定义新的Lock Stores方法，更详细说明参考官方文档Lock StoresFilesystemrasa的训练数据以yaml格式保存，训练完成的模型是tar.gz的压缩格式rasa支持从本地、服务器和云端读取模型：1）从本地读取# 读取指定路径的模型rasa run --model models/20190506-100418.tar.gz# 读取models下最新的模型rasa run --model models/# 读取默认路径(models/)下最新的模型rasa run 2）从服务器读取### endpoints.ymlmodels:  # 服务器模型保存路径  url: http://my-server.com/models/default  # 每隔多少秒拉取一次，设为null则只在开始rasa服务的时候拉取一次  wait_time_between_pulls: 103）从云端读取rasa支持从Amazon S3、Google Cloud Storage、Azure Storage和自定义的其他云服务器读取模型rasa run --model 20190506-100418.tar.gz --remote-storage aws   # amazonrasa run --model 20190506-100418.tar.gz --remote-storage gcs   # googlerasa run --model 20190506-100418.tar.gz --remote-storage azure # azure关于服务器端和云端的配置见官方文档Model StorageRasa的代码模块组成rasa通过yml文件实现各个功能模块的配置，最简单的对话甚至不需要编写代码# rasa的代码模块结构树.├── actions│   ├── __init__.py│   └── actions.py├── config.yml├── credentials.yml├── data│   ├── nlu.yml│   └── stories.yml├── domain.yml├── endpoints.yml├── models│   └── &lt;timestamp&gt;.tar.gz└── tests   └── test_stories.yml      文件    包含内容    功能        config.yml    对话语言、NLU Pipeline、Dialogue Policies    决定rasa如何根据用户输入做出action预测（NLU+DM）        domain.yml    会话设置、intents声明、entity声明、response声明、action声明、slot声明、form声明    会话中用到的基本元素的声明        endpoints.yml    模型（保存在服务器或云端时）的url等参数、action server的url、tracker store的参数、event_broker的参数、lock store的参数    图2中与rasa通过http通信模块的接口声明        credentials.yml    rest设置、websocket设置、主流对话产品等的设置    rasa与前端的连接设置（channel connectors）        data/nlu.yml    domain.yml中声明的intent、entity的训练数据、同义词synonym的定义、正则表达式regex的定义、查询表lookup的定义    nlu训练数据        data/stories.yml    编写由intent和action（在domain.yml中定义）组成的故事，通过训练后可以对故事做扩展生成对话路径    对话管理模块的训练数据，训练方法由config.yml的policies指定        data/rules.yml    编写由intent和action（在domain.yml中定义）组成的规则，只要检测到规则中的意图，一定会执行相应的action    对话管理模块的训练数据，训练方法由config.yml的policies指定        actions/actions.py    重载Action类，实现自定义的action（action名称需要在domain.yml中定义）    使用action server自定义action        models/xxx.tar.gz    rasa train模型默认保存路径    rasa模型默认保存位置        tests/test_stories.yml    使用rasa test可以测试所有test_开头的用例    测试用例    domain.yml的内容可以由多个yml文件组成，此时需要在rasa训练时指定文件夹路径    rasa train --domain path_to_domain_direction        rasa支持多种前端（credentials.yml）连接，包含自定义前端（如rasa-webchat，通过socketio连接）、facebook、slack、Telegram、Twilio、Google Hangouts Chat、Microsoft Bot、Cisco Webex、RocketChat、Mattermost，详细配置见官方channel connectors章节  rasa的语音输入需要rasa pro版本才支持rasa 主要操作接口### 创建新的对话projectrasa init### 训练project，默认保存在models/下rasa trainrasa train nlu        # 仅训练nlu部分rasa train core       # 仅训练dm部分### 在终端开始交互式对话，标记新的训练数据rasa interactive### 在终端开始对话rasa shellrasa shell nlu        # 仅输出nlu出来的结果rasa shell -vv        # 同时输出debug信息### 以后台服务的方式开启rasa，默认使用models/下最新的模型rasa runrasa run --cors \"*\"   # 支持跨域访问rasa run --enable-api # 支持RESTful APIrasa run -vv          # 输出debug信息### 开启action server服务器（对应actions.py的实现）rasa run actions更详细的命令行接口见官方文档Command Line Interface"
  },
  
  {
    "title": "【词向量】（4）ELMo算法分析-下",
    "url": "/posts/%E8%AF%8D%E5%90%91%E9%87%8F-4-ELMo%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90-%E4%B8%8B/",
    "categories": "NLP, 词向量",
    "tags": "词向量",
    "date": "2023-02-01 00:00:00 +0000",
    





    
    "snippet": "模块biLMs该模块建立了一个两层的双向LSTM网络，对token_embedder模块输出的“静态”向量表示根据上下文做动态调整模块输入尺寸为$3\\times 11\\times 512$，输出尺寸为$2\\times 3\\times 11\\times 1024$。其中2表示两层网络的输出，每个词元的向量表示维度为512，而每层网络同时包含正向和方向输出，因此将两个方向做cat，输出为1024...",
    "content": "模块biLMs该模块建立了一个两层的双向LSTM网络，对token_embedder模块输出的“静态”向量表示根据上下文做动态调整模块输入尺寸为$3\\times 11\\times 512$，输出尺寸为$2\\times 3\\times 11\\times 1024$。其中2表示两层网络的输出，每个词元的向量表示维度为512，而每层网络同时包含正向和方向输出，因此将两个方向做cat，输出为1024双向语言模型biLMs原理给定一个包含$N$个词元(token)的序列$(t_1,t_2,…,t_N)$正向的语言模型计算给定前面tokens的情况下当前token的概率\\[p(t_1,t_2,...,t_N)=\\prod_{k=1}^Np(t_k\\lvert t_1,t_2,...,t_{k-1}) \\tag{1}\\]在每个位置$k$，模型会在每一层输出其上下文相关的表征$\\vec{h}_{k,j}^{LM}$，其中$j=1,…,L$表示第几层，顶层的输出用于预测下一个token：$t_{k+1}$反向的语言模型计算给定后面tokens的情况下当前token的概率\\[p(t_1,t_2,...,t_N)=\\prod_{k=1}^Np(t_k\\lvert t_{k+1},t_{k+1},...,t_{N}) \\tag{2}\\]在每个位置$k$，模型会在每一层输出其上下文相关的表征$\\overleftarrow{h}_{k,j}^{LM}$biLMs中结合正向和反向的语言模型，最大化两者的似然值\\[\\sum_{k=1}^N(log p(t_k\\lvert t_1,...,t_{k-1};\\Theta_x,\\overrightarrow{\\Theta}_{LSTM},\\Theta_s)+log p(t_k\\lvert t_{k+1},...,t_N;\\Theta_x,\\overleftarrow{\\Theta}_{LSTM},\\Theta_s)) \\tag{3}\\]其中$\\Theta_x$是token_embedder模块输出的词向量，$\\Theta_s$是biLMs顶层输出，$\\vec{\\Theta}{LSTM}$和$\\overleftarrow{\\Theta}{LSTM}$是正反向LSTM的参数借用《ELMo解读（论文 + PyTorch源码）》的图图1. biLMs整体结构biLMs的实现如图1所示，token_embedder模块输出首先经过第1层LSTM，输出正向结果和反向结果，将其作为第2层LSTM的输入，得到输出# 对每一层LSTM，分别做一次正向和反向推理forward_output_sequence, forward_state = forward_layer(    forward_output_sequence, batch_lengths, forward_state)backward_output_sequence, backward_state = backward_layer(    backward_output_sequence, batch_lengths, backward_state)代码中forward_output_sequence和backward_output_sequence是biLSTM对每个token的编码输出，尺寸为$3\\times 11\\times 512$，即每个token的特征维度为512forward_state和backward_state是列表类型，包含序列（一个语句，这里一共11个tokens）结束后每个神经元（这里一个token维度为512，因此神经元个数为512）的输出门结果（$3\\times 512$）和记忆门结果（$3\\times 4096$）# forward_layer/backward_layerfor timestep in range(total_timesteps):    # 按正向或反向对每个token做LSTM    index = timestep if self.go_forward else total_timesteps - timestep - 1    ...    full_batch_previous_memory[0 : current_length_index + 1] = memory    # forward_output_sequence(在这里为output_accumulator)记录了每个token的输出门结果    # forward_state(在这里为full_batch_previous_state)则仅保存语句最后一个token的输出门结果    # full_batch_previous_state和full_batch_previous_memory的信息会影响下一个token    full_batch_previous_state[0 : current_length_index + 1] = timestep_output    output_accumulator[0 : current_length_index + 1, index] = timestep_outputfinal_state = (    full_batch_previous_state.unsqueeze(0),   # 输出门+linear    full_batch_previous_memory.unsqueeze(0),  # 记忆门)return output_accumulator, final_state此外代码在biLMs中还对两层LSTM做了类似resdual的跳层设计# Skip connections, just adding the input to the output.# forward_cache是上一层LSTM输出的forward_output_sequenceif layer_index != 0:    forward_output_sequence += forward_cache    backward_output_sequence += backward_cache图2. biLMs的跳层设计模块scalar_mixes该模块将biLMs的两层输出（修正后的动态向量表示）和token_embedder的输出（静态向量表示）做加权和，得到最终的向量表示由于token_embedder的输出维度为$3\\times 11\\times 512$，而biLMs的两层输出因为包含了正向和反向的结果，因此均为$3\\times 11 times 1024$，所以在实现上首先将token_embedder的输出做cat# type_representation为token_embedder的输出，cat后维度变成1024output_tensors = [    torch.cat([type_representation, type_representation], dim=-1) * mask.unsqueeze(-1)]# lstm_outputs为biLMs的两层输出for layer_activations in torch.chunk(lstm_outputs, lstm_outputs.size(0), dim=0):    output_tensors.append(layer_activations.squeeze(0))尺寸问题处理好后，对output_tensors列表（长度为3）做加权和# normed_weights是长度为3的权重列表，gamma是尺度变换因子，这两组值也是通过训练得到的pieces = []for weight, tensor in zip(normed_weights, output_tensors):    pieces.append(weight * tensor)    return self.gamma * sum(pieces)ELMo使用了2个scalar_mixes，因此最后的输出是一个长度为2的列表，表示两种加权和方案的向量res = elmo(character_ids)&gt;&gt;&gt; len(res['elmo_representations'])2# 两种加权和方案的结果，在scalar_mixes后会去掉句子前后的&lt;bos&gt;和&lt;eos&gt;&gt;&gt;&gt; res['elmo_representations'][0].shapetorch.Size([3, 9, 1024])&gt;&gt;&gt; res['elmo_representations'][1].shapetorch.Size([3, 9, 1024])其他ELMo的训练官方代码的训练部分没有细看，不能确定token_embedder、biLMs、scalar_mixes三个模块是统一训练还是分别训练，个人认为可能是分开做的，特别是其中的biLMs，正向LSTM和反向LSTM似乎只能分别训练此外还有一个“证据”就是算法的权重文件’elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5’内部是由不同的模块权重组成的def _load_weights(self):    self._load_char_embedding()    self._load_cnn_weights()    self._load_highway()    self._load_projection()参考式1和式2可知，模型的训练与Word2Vec类似，输入词元$t_i$，对应的label为$t_{i+1}$或$t_{i-1}$，损失函数使用交叉熵即可与下游NLP任务的结合作者认为ELMo非常灵活：1）从位置上，ELMo即可以作为输入放在NLP任务之前，也可以同时放在NLP任务的前面和后面。不同的NLP任务适用不同的方案图3. ELMo不同的位置方案2）从训练上，ELMo既可以frozen，只训练NLP任务相关的参数，也可以一并训练bow、eow、bos、eos的意义ELMo在预处理阶段会给每个token前后加bow和eow，每个语句前后加bos和eos；而最终输出可以去掉bos和eosif self._keep_sentence_boundaries:    processed_representation = representation_with_bos_eos    processed_mask = mask_with_bos_eoselse:    representation_without_bos_eos, mask_without_bos_eos = remove_sentence_boundaries(        representation_with_bos_eos, mask_with_bos_eos    )    processed_representation = representation_without_bos_eos    processed_mask = mask_without_bos_eos但这么做的意义我目前还不太明白，个人猜测是这样可以帮助网络学习通常作为语句开头和结尾的token，以及通常作为token开头和结尾的字符，提升NLG（自然语言生成）的性能代码官方开源了基于pyTorch的代码和基于tensorflow的实现此外在pypi上还有一个简单实现哈工大基于官方代码实现了多语言的支持"
  },
  
  {
    "title": "【词向量】（3）ELMo算法分析-上",
    "url": "/posts/%E8%AF%8D%E5%90%91%E9%87%8F-3-ELMo%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90-%E4%B8%8A/",
    "categories": "NLP, 词向量",
    "tags": "词向量",
    "date": "2023-01-30 00:00:00 +0000",
    





    
    "snippet": "ELMo（Embeddings from Language Models）发表于NAACL2018【1】，作者是华盛顿大学AllenNLP项目（基于PyTorch的开源NLP库）的领导人以Word2Vec和GloVe为代表的词向量算法基于语料库训练得到词元的静态向量表示（如Word2Vec的训练结果是一个embedding类型的张量，第$i$行表示词汇表中第$i$个词元的向量表示），这种方式...",
    "content": "ELMo（Embeddings from Language Models）发表于NAACL2018【1】，作者是华盛顿大学AllenNLP项目（基于PyTorch的开源NLP库）的领导人以Word2Vec和GloVe为代表的词向量算法基于语料库训练得到词元的静态向量表示（如Word2Vec的训练结果是一个embedding类型的张量，第$i$行表示词汇表中第$i$个词元的向量表示），这种方式无法应对一词多义的问题：  植物从土壤中吸收水分  他说的话里有很大的水分上面引用的例子中“水分”显然是不同的含义，但在Word2Vec中其向量表示是同一个ELMo通过biLSTM实现词元与其上下文信息的关联，得到动态向量表示，论文的实验表明其在NLP领域的6种任务均带来了明显的提升图1. 应用ELMo后均超越了SOTA参考文章推荐  《ELMo解读（论文 + PyTorch源码）》  《ELMo》  《ELMo论文笔记+源码分析》本文以下分析均基于官方代码ELMo原理分析从结构上看可以将ELMo按顺序分成3个模块：token_embedder、biLMs、scalar_mixes  模块token_embedder实现词元的“静态”向量化（这一步类似Word2Vec的效果，得到词元固定的向量表示）  模块biLMs使用双向LSTM关联词元的上下文，对“静态”向量表示做调整  模块scalar_mixes将以上两个模块的输出做线性加权，得到最终的“动态”向量表示在输入ELMo之前需要通过字符映射模块提前将词元映射到词汇表中（对应案例中的batch_to_ids）# 一个ELMo的测试案例import osimport sysroot_dir = os.path.dirname(os.path.abspath(__file__))sys.path.append(root_dir)from allennlp.modules.elmo import Elmo, batch_to_idsmodel_dir = os.path.join(root_dir, 'models')options_file = os.path.join(model_dir, 'elmo_2x4096_512_2048cnn_2xhighway_options.json')weights_file = os.path.join(model_dir, 'elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5')# scalar_mixes的次数num_output_representations = 2elmo = Elmo(    options_file=options_file,    weight_file=weights_file,    num_output_representations=num_output_representations,    dropout=0)sentence_lists = [['I', 'have', 'a', 'dog', ',', 'it', 'is', 'so', 'cute'],                  ['an'],                  ['That', 'is', 'a', 'question']]character_ids = batch_to_ids(sentence_lists)res = elmo(character_ids)字符映射模块该模块在官方代码中由ELMoCharacterMapper实现以案例代码为例，模块输入是包含3个语句且已完成分词的列表，输出是一个尺寸为$3\\times 9\\times 50$的张量  其中3代表语句数量，9是这3个语句中最长的那个对应的词元数，50是一个预设值，表示一个词元最大的字符数  张量的内容是每个字母在utf-8字符集中的id  模块还会在每个词元前后添加词元开始标记bow(begin of word)和词元结束标记eow(end of word)  模块会在词元中剩下的位置添加padding值（预设一个词元有50个字符，如果该词元只有2个字符那么剩下的位置做padding）  最后模块会将所有的id值加1，将id=0留给语句中不包含词元的位置（例子中一个语句有9个词元，但第2句话只有1个词元，那么剩下的8个位置均赋值0）utf-8编码unicode是一个字符集，基本包含了世界上所有的语言字符，utf-8是unicode最广泛的一种实现方式（更详细的介绍参考文章《汉字编码（【Unicode】 【UTF-8】 【Unicode与UTF-8之间的转换】 【汉字 Unicode 编码范围】【中文标点Unicode码】【GBK编码】【批量获取汉字UNICODE码】）》）在utf-8中，英文和数字占1个byte，欧洲语言（如希腊语）占2个byte，中文占3个byte由于官方代码仅考虑英语，因此字符编码的id范围为0-255，在python中，可以使用str类的encode函数实现utf-8的转换# 英文转换后占1个byte# encode返回bytes类型，可以用list的方式取得其id值&gt;&gt;&gt; encoded = 'a'.encode('utf-8', 'ignore')&gt;&gt;&gt; type(encoded)&lt;class 'bytes'&gt;&gt;&gt;&gt; len(encoded)1&gt;&gt;&gt; encoded[0]97# 中文转换后占3个byte&gt;&gt;&gt; encoded = '中'.encode('utf-8', 'ignore')&gt;&gt;&gt; len(encoded)3&gt;&gt;&gt; [id for id in encoded][228, 184, 173]开始标记、结束标记和padding是代码自定义的：# char ids 0-255 come from utf-8 encoding bytes# assign 256-300 to special charsbeginning_of_sentence_character = 256  # &lt;begin sentence&gt;end_of_sentence_character = 257  # &lt;end sentence&gt;beginning_of_word_character = 258  # &lt;begin word&gt;end_of_word_character = 259  # &lt;end word&gt;padding_character = 260  # &lt;padding&gt;模块输出示例需要注意的是，这里模块的输出仅对词元前后添加bow和eow，没有对语句前后添加bos和eos# 案例中第2句话（['an']）的映射结果# 映射的id均+1处理&gt;&gt;&gt; character_ids[1][0]tensor([259,  98, 111, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261])# 由于第2句话的第2-9个位置没有词元，因此相应位置填0&gt;&gt;&gt; character_ids[1][1]tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,        0, 0])模块token_embedder该模块在官方代码中由_ElmoCharacterEncoder实现，输出词元的“静态”向量表示模块的输入是字符映射模块的输出，即$3\\times 9\\times 50$的张量，输出包含token_embedding和mask两个张量  token_embedding是语句中每个词元的“静态”向量表示，尺寸为$3\\times 11\\times 512$  mask是语句有效词元的掩膜，尺寸为$3\\times 11$模块可以分解为预处理阶段、`embedding`阶段、`CNN`阶段、`highway`阶段和`projection`阶段借用《ELMo解读（论文 + PyTorch源码）》的图：图2. 模块`token_embedder`的分解预处理阶段该阶段会在每个语句前后添加bos和eos，输入$3\\times 9\\times 50$，处理后的输出尺寸为$3\\times 11\\times 50$# 第3个语句['That', 'is', 'a', 'question']前后添加&lt;bos&gt;和&lt;eos&gt;后的结果# 需要注意&lt;bos&gt;(对应256+1)和&lt;eos&gt;(对应257+1)也被视为一个词元，因此前后也要添加&lt;bow&gt;和&lt;eow&gt;&gt;&gt;&gt; character_ids_with_bos_eos[2][0]tensor([259, 257, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261])&gt;&gt;&gt; character_ids_with_bos_eos[2][5]tensor([259, 258, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,        261, 261, 261, 261, 261, 261, 261, 261])# mask_with_bos_eos记录语句中有效词元的位置&gt;&gt;&gt; mask_with_bos_eos[2]tensor([ True,  True,  True,  True,  True,  True, False, False, False, False,        False])embedding阶段该阶段使用torch.nn.Embedding实现每个字符id到特征向量表示的映射输入$3\\times 11\\times 50$，输出$33\\times 50\\times 16$，其中16是映射后的每个字符的特征维度:# (batch_size * sequence_length, max_chars_per_token, embed_dim)CNN阶段对embedding阶段输出的特征向量，分别使用尺寸为1、2、3、4、5、6、7的核做1维卷积，然后做最大池化+激活然后将所有输出做拼接，输出尺寸为$33\\times 2048$filters = [[1, 32], [2, 32], [3, 64], [4, 128], [5, 256], [6, 512], [7, 1024]]# char_embed_dim为16for i, (width, num) in enumerate(filters):    conv = torch.nn.Conv1d(        in_channels=char_embed_dim, out_channels=num, kernel_size=width, bias=True    )以核尺寸3为例，1个字符的原始特征维度是16，每次对相邻3个字符同一个位置的特征做卷积因为卷积的意义就是综合邻域信息，在这里邻域就是同一个词元中的其他字符图3. CNN卷积highway阶段highway networks【2】于2015年提出，可以降低深层网络的训练难度几个月后何凯明大神发表的resnet【3】借鉴了highway networks的思路\\[y=H(x,W_H)T(x,W_T)+x(1-T(x,W_T)) \\tag{1}\\]式1是highway的核心公式，其中$x$是网络输入，$H(x,W_H)$表示对输入的非线性变换（深度网络），$T(x,W_T)$表示对输入的线性变换何在resnet中对hightway的评价是：  highway的线性变换部分是有待训练参数的，而resdual该部分是无参数的  实验证明resdual的训练效果比highway更好何在随后的另一篇论文【4】中对比了resdual和highway结构图4. (a):resdual, (c):highway总的来说，resdual更像是highway的简化版本具体到ELMo的highway阶段，是用highway结构去“优化”线性层，该阶段的输出尺寸与输入一致:# self._layers就是被highway优化的网络self._layers = torch.nn.ModuleList(    [torch.nn.Linear(input_dim, input_dim * 2) for _ in range(num_layers)])def forward(self, inputs: torch.Tensor) -&gt; torch.Tensor:    current_input = inputs    for layer in self._layers:        projected_input = layer(current_input)        linear_part = current_input        # NOTE: if you modify this, think about whether you should modify the initialization        # above, too.        nonlinear_part, gate = projected_input.chunk(2, dim=-1)        nonlinear_part = self._activation(nonlinear_part)        gate = torch.sigmoid(gate)        current_input = gate * linear_part + (1 - gate) * nonlinear_part    return current_inputprojection阶段将上阶段输出的张量做维度映射，确保每个词元的特征向量长度为512，最终输出的尺寸为$3\\times 11\\times 512$文献【1】Peters M ,  Neumann M ,  Iyyer M , et al. Deep Contextualized Word Representations[J].  2018.【2】Srivastava R K ,  Greff K ,  Schmidhuber J . Highway Networks[J]. Computer Science, 2015.【3】He, Kaiming, et al. “Deep residual learning for image recognition.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.【4】He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016, October). Identity mappings in deep residual networks. InEuropean conference on computer vision(pp. 630-645). Springer, Cham."
  },
  
  {
    "title": "【对话系统】Rasa安装与rasa-webchat前端配置",
    "url": "/posts/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F-Rasa%E5%AE%89%E8%A3%85%E4%B8%8Erasa-webchat%E5%89%8D%E7%AB%AF%E9%85%8D%E7%BD%AE/",
    "categories": "NLP, 对话系统",
    "tags": "对话系统",
    "date": "2023-01-18 00:00:00 +0000",
    





    
    "snippet": "环境及软件版本内网服务器通过docker运行Rasa服务，外网机器与内网服务器通过ssh连接，实现指令与结果的发生和接收同时外网机器配置rasa-webchat前端，实现友好的对话界面      名称    Python    Rasa    Rasa SDK    rasa-webchat        版本    3.7.5    3.4.0    3.4.0    1.0.1  安装及...",
    "content": "环境及软件版本内网服务器通过docker运行Rasa服务，外网机器与内网服务器通过ssh连接，实现指令与结果的发生和接收同时外网机器配置rasa-webchat前端，实现友好的对话界面      名称    Python    Rasa    Rasa SDK    rasa-webchat        版本    3.7.5    3.4.0    3.4.0    1.0.1  安装及配置Rasa参考官方文档，rasa支持的python版本为3.7、3.8、3.9、3.10# 首先升级pip到最新版本pip3 install -U pip# 指定安装的rasa版本，会同时安装rasa sdkpip3 install rasa==3.4.0由于本案例会使用jieba分词和bert语言模型，因此还需分别配置pip3 install jieba在huggingface的模型库中下载tf_model.h5、config.json和vocab.txt图1. 下载bert-base-chinese模型案例工程配置首先创建rasa工程rasa init1）修改config.yml文件recipe: default.v1language: zhpipeline:  - name: JiebaTokenizer  - name: LanguageModelFeaturizer    model_name: bert    model_weights: /home/nick/data/models/bert-base-chinese  - name: \"DIETClassifier\"    epochs: 100其中model_weights就是上面bert-base-chinese模型包含的3个文件所在路径2）修改nlu.yml文件version: \"3.1\"nlu:  - intent: greet    examples: |      - 你好      - hello      - hi      - 喂      - 在么  - intent: goodbye    examples: |      - 拜拜      - 再见      - 拜      - 退出      - 结束  - intent: medicine    examples: |      - [感冒](disease)了该吃什么药      - 我[便秘](disease)了，该吃什么药      - 我[胃痛](disease)，该吃什么药      - 一直[打喷嚏](disease)吃什么药好      - 父母都有[高血压](disease)，我应该推荐他们吃什么药好呢      - 头上烫烫的，感觉[发烧](disease)了，该吃什么药好      - [减肥](disease)有什么好的药品推荐吗？  - intent: medical_department    examples: |      - [感冒](disease)了该吃去哪个科室看病      - 我[便秘](disease)了，该去挂哪个科室的号      - 我[胃痛](disease)，该去医院看哪个门诊啊      - 一直[打喷嚏](disease)挂哪一个科室的号啊      - [头疼](disease)该挂哪科  - intent: medical_hospital    examples: |      - 我生病了，不知道去哪里看病      - [减肥](disease)有什么好的医院或者健康中心推荐吗？      - 想做个[体检](disease)，有哪家医院或者哪里的诊所或者健康中心比较实惠啊？      - 父母都有[高血压](disease)，我应该推荐他们去哪家医院好呢3）修改domain.yml文件version: \"3.1\"intents:  - greet  - goodbye  - medicine  - medical_department  - medical_hospitalresponses:  utter_greet:    - text: \"您好，请问有什么我可以帮您？\"    - text: \"您好\"  utter_goodbye:    - text: \"再见\"    - text: \"再见，祝您健康\"  utter_medicine:    - text: \"忍着\"  utter_medical_department:    - text: \"头疼的话，建议去看看骨科，重点看脚\"  utter_hospital:    - text: \"没差，长沙的医院都是依托答辩\"    - text: \"请问您在哪个城市？\"actions:  - utter_greet  - utter_goodbye  - utter_medicine  - utter_medical_department  - utter_hospitalentities:  - disease4）修改stories.yml文件version: \"3.1\"stories:- story: happy path  steps:  - intent: greet  - action: utter_greet  - intent: medicine  - action: utter_medicine  - intent: medical_department  - action: utter_medical_department  - intent: medical_hospital  - action: utter_hospital  - intent: goodbye  - action: utter_goodbye5）修改credentials.yml文件该文件确定如何连接客户端因为rasa webchat使用的低层协议是socketio，所以取消socketio一项的注释，并做修改socketio: user_message_evt: user_uttered bot_message_evt: bot_uttered session_persistence: false配置rasa-webchatrasa-webchat是一个支持rasa的网页对话前端使用方法很简单：在自己的html文件中嵌入rasa的script就可以了，一个简单的完整示例：&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;    &lt;meta charset=\"UTF-8\"&gt;    &lt;title&gt;智能医疗问询&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script&gt;!(function () {    let e = document.createElement(\"script\"),        t = document.head || document.getElementsByTagName(\"head\")[0];    (e.src =        // 注意版本必须为1.0.1，否则会通信失败        \"https://cdn.jsdelivr.net/npm/rasa-webchat@1.0.1/lib/index.js\"),        // Replace 1.x.x with the version that you want        (e.async = !0),        (e.onload = () =&gt; {            window.WebChat.default(                {                    customData: {language: \"zh\"},                    socketUrl: \"http://localhost:5005\",                    // add other props here                },                null            );        }),        t.insertBefore(e, t.firstChild);})();&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;方法需要客户机能够访问https://cdn.jsdelivr.net/npm/rasa-webchat@1.0.1/lib/index.js启动对话服务docker和内网服务器通信因为rasa服务是在内网服务器的docker中，需要配置5005端口映射docker run -it\\           --user nick\\\t\t   -p 5005:5005\\           rasa:cpu-0.0.4\\           /bin/bash内网服务器和外网客户端通信同样通过端口转发，Termius和XShell都能实现该功能图2. Termius端口转发的设置启动rasa和webchat# 首先训练rasa模型rasa train# 然后启动rasa服务, --cors实现跨域访问rasa run --cors \"*\"在客户机上用浏览器打开上述html文件，就可以实现对话了图3. rasa+rasa-webchat使用postman获取rasa的api返回rasa也支持RESTful HTTP API的服务方式# 启动rasa服务时需要打开apirasa run --enable-api使用postman软件向地址localhost:5005/model/parse发送信息\"text\":\"hi\"图4. rasa+postman"
  },
  
  {
    "title": "【词向量】（2）Word2Vec代码实现",
    "url": "/posts/%E8%AF%8D%E5%90%91%E9%87%8F-2-Word2Vec%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/",
    "categories": "NLP, 词向量",
    "tags": "词向量",
    "date": "2023-01-10 00:00:00 +0000",
    





    
    "snippet": "Word2Vec的作者开源了基于c语言的实现本文针对第三方基于pyTorch实现的版本，作者在其博客《Word2vec with PyTorch: Implementing the Original Paper》写了篇分析文章该版本代码与官方代码相比有所简化：  对于CBOW模型中的背景词，对其做平均操作而非累加操作  对于skip-gram模型中的背景词，对其以相同概率做采样  没有使用层...",
    "content": "Word2Vec的作者开源了基于c语言的实现本文针对第三方基于pyTorch实现的版本，作者在其博客《Word2vec with PyTorch: Implementing the Original Paper》写了篇分析文章该版本代码与官方代码相比有所简化：  对于CBOW模型中的背景词，对其做平均操作而非累加操作  对于skip-gram模型中的背景词，对其以相同概率做采样  没有使用层次softmax和哈夫曼树做优化，而是直接用全连接的softmax  使用adam优化器代替官方代码的adagrad优化器  词元的向量表示会强制做归一化WikiText数据集代码使用WikiText数据集【1】，该数据集是从WikiPedia的优质文章提取得到，是一个包含超过1亿个语句的英文语料库数据集包含WikiText-2和WikiText-103两个版本，前者是后者的子集图1. WikiText数据集的统计对比图1是官方对WikiText数据集的统计，可以看到WikiText-2中包含33278个词元，未登录词OoV(Out-of-Vocabulary，训练集中未包含但测试集中包含的词元)占比2.6%，WikiText-103中包含267735个词元，未登录词占比0.4%WikiText将数据分别存储在wiki.train.tokens、wiki.valid.tokens、wiki.test.tokens三个文本文件中，内容如下所示= Valkyria Chronicles III =   Senjō no Valkyria 3 : &lt;unk&gt; Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . &lt;unk&gt; the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" &lt;unk&gt; Raven \" .   = = Gameplay = =   As with previous &lt;unk&gt; Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through &lt;unk&gt; text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely &lt;unk&gt; through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main &lt;unk&gt; , although they take a very minor role .  The game 's battle system , the &lt;unk&gt; system , is carried over directly from &lt;unk&gt; Chronicles . During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person . A character can only act once per @-@ turn , but characters can be granted multiple turns at the expense of other characters ' turns . Each character has a field and distance of movement limited by their Action &lt;unk&gt; . Up to nine characters can be assigned to a single mission . During gameplay , characters will call out if something happens to them , such as their health points ( HP ) getting low or being knocked out by enemy attacks . Each character has specific \" Potentials \" , skills unique to each character . They are divided into \" Personal Potential \" , which are innate skills that remain unaltered unless otherwise dictated by the story and can either help or impede a character , and \" Battle Potentials \" , which are grown throughout the game and always grant &lt;unk&gt; to a character . To learn Battle Potentials , each character has a unique \" Masters Table \" , a grid @-@ based skill table that can be used to acquire and link different skills . Characters also have Special &lt;unk&gt; that grant them temporary &lt;unk&gt; on the battlefield : Kurt can activate \" Direct Command \" and move around the battlefield without &lt;unk&gt; his Action Point gauge , the character &lt;unk&gt; can shift into her \" Valkyria Form \" and become &lt;unk&gt; , while Imca can target multiple enemy units with her heavy weapon . WikiText的特殊标记WikiText文本中有几类特殊标记，参考文章《WikiText数据集_自然语言处理》对其做整理和解释1）=xx=标记= xxx =表示这是一级标题（文章标题），一级标题下的内容为一篇文章= = xxx = = 表示这是二级标题，类似的= = = xxx = = =表示三级标题2）&lt;unk&gt;标记原始单词被替换为&lt;unk&gt;，表示该单词是低频词，不在词频统计范围内3）@标记连接符，原始内容role-playing在数据集中被表示为role @-@ playing，显然这样处理是为了方便分词4）&lt;eos&gt;标记一段话结尾添加的标识符数据dataloader创建基本流程为：加载数据集、分词、建立词汇表、在每个batch中对数据做后处理生成输入数据和label数据def get_dataloader_and_vocab(    model_name, ds_name, ds_type, data_dir, batch_size, shuffle, vocab=None):    data_iter = get_data_iterator(ds_name, ds_type, data_dir)    tokenizer = get_english_tokenizer()    if not vocab:        vocab = build_vocab(data_iter, tokenizer)            # 这里x是data_iter出来的一行文字    # 首先做分词形成词语列表，vocab(['w1', 'w2',...])返回词w1,w2的索引列表    text_pipeline = lambda x: vocab(tokenizer(x))    if model_name == \"cbow\":        collate_fn = collate_cbow    elif model_name == \"skipgram\":        collate_fn = collate_skipgram    else:        raise ValueError(\"Choose model from: cbow, skipgram\")    dataloader = DataLoader(        data_iter,        batch_size=batch_size,        shuffle=shuffle,        collate_fn=partial(collate_fn, text_pipeline=text_pipeline),    )    return dataloader, vocab  get_data_iterator是语料库数据迭代器，每次输出一个语句  get_english_tokenizer生成一个英文分词器，将一个英文语句切分为由词元组成的列表  build_vocab遍历语料库，将每个语句分词后生成词汇表（内部包含一个词元列表，按词频从高到低排序）  text_pipeline是一个lambda函数，每次迭代以一个语句为输入，首先对其做分词，输出每个词元在词汇表的序号语料库数据迭代器WikiText2和WikiText103是torchtext模块内置的函数，该函数会下载数据集，解压缩后生成流式迭代器，每次输出一个语句有别于常见的Dataset，函数内部调用了torchdata.datapipes.iter生成IterDataPipes形式的数据流def get_data_iterator(ds_name, ds_type, data_dir):    if ds_name == \"WikiText2\":        data_iter = WikiText2(root=data_dir, split=(ds_type))    elif ds_name == \"WikiText103\":        data_iter = WikiText103(root=data_dir, split=(ds_type))    else:        raise ValueError(\"Choose dataset from: WikiText2, WikiText103\")    data_iter = to_map_style_dataset(data_iter)    return data_itertorch常见的用法是Dataset+DataLoader或iterableDataset+DataLoader，前者是map-style后者是iterable-stylemap-style适合已知所有样本的情况，同时在ddp中可以用torch.utils.data.Sample来控制采样iterable-style适合实时产生的流式数据，但不支持Sample采样，在多进程读取时要单独处理防止数据重复加载从torch1.11开始引入了TorchData对标tensorflow的tf.data，其提供DataPipes来替代现有的Dataset在TorchData中用法是MapDataPipes+DataLoader或IterDataPipes+DataLoader，前者是map-style后者是iterable-style文章《TorchData：PyTorch新的数据构建方式》对此有详细的描述英文分词器def get_english_tokenizer():    \"\"\"    Documentation:    https://pytorch.org/text/stable/_modules/torchtext/data/utils.html#get_tokenizer    英文分词器    \"\"\"    tokenizer = get_tokenizer(\"basic_english\", language=\"en\")    return tokenizer代码使用了torch的内置分词器torchtext.data.utils.get_tokenizer(tokenizer, language='en')      tokenizer取值    分词说明        None    忽略language取值，直接调用split()函数，根据空格对语句做切分        'basic_english'    首先对其做正则化，然后调用split()函数，正则化操作包括：大写转小写、在'\\','.','(',')','!','?'的前后添加空格、将';',':'换成空格、将多个连续空格合并为1个空格等        自定义函数句柄    返回自定义函数句柄，忽略language        'spacy'    使用'spacy'分词库，内部会检查language是否被支持        'moses'    使用'moses'分词库，忽略language        'toktok'    使用'toktok'分词库，忽略language        'revtok'    使用'revtok'分词库，忽略language        'toktok'    使用'subword'分词库，忽略language  词汇表生成def build_vocab(data_iter, tokenizer):    \"\"\" Builds vocabulary from iterator    \"\"\"    vocab = build_vocab_from_iterator(        map(tokenizer, data_iter),        specials=[\"&lt;unk&gt;\"],        min_freq=MIN_WORD_FREQUENCY,    )    vocab.set_default_index(vocab[\"&lt;unk&gt;\"])    return vocabtorchtext.vocab.build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Optional[List[str]] = None, special_first: bool = True, max_tokens: Optional[int] = None)'''iterator: 分词后的词元列表min_freq: 词频小于该阈值的词元被丢弃specials: 额外添加的词元列表special_first: 是否将额外的词元置于词汇表的前面max_tokens: 对词元按词频从高到低排序，仅保留前面max_tokens-len(specials)个词元和specials中定义的词元'''举个例子&gt;&gt; tokens = [\"e\", \"d\",\"d\", \"c\", \"b\", \"b\", \"b\", \"a\"]&gt;&gt; vocab = build_vocab_from_iterator(tokens, min_freq=2, specials=[\"&lt;unk&gt;\", \"&lt;eos&gt;\"],  special_first=False)&gt;&gt; print(vocab.get_itos())    ['b', 'd', '&lt;unk&gt;', '&lt;eos&gt;']&gt;&gt; print(vocab.get_stoi())    {'&lt;eos&gt;': 3, '&lt;unk&gt;': 2, 'b': 0, 'd': 1}  vocab.get_itos()返回按词频从大到小排列的list，  vocab.get_stoi()返回dict，其key为词value是该词在get_itos()返回list的序号CBOW模型的数据预处理对每个句子分词后得到的词元列表，从前往后每次取$2R+1$个词元作为一个序列对于CBOW模型，取序列中第$R$个词元（中心词）作为label，其余的$2R$个词元（背景词）作为网络输入，最终希望网络的输出向量与label代表的词元向量接近def collate_cbow(batch, text_pipeline):    ''' 函数的输入输出内容都是词元在词汇表中的序号        因此每个输入输出其实就是one-hot向量    '''    batch_input, batch_output = [], []    for text in batch:        # 得到该语句每个词元在词汇表中的序号列表        text_tokens_ids = text_pipeline(text)        # 句子过滤，长度小于2R+1的忽略        if len(text_tokens_ids) &lt; CBOW_N_WORDS * 2 + 1:            continue        # 长度大于阈值的句子截断        if MAX_SEQUENCE_LENGTH:            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]            output = token_id_sequence.pop(CBOW_N_WORDS) # 取中间的词作为输出            input_ = token_id_sequence                   # 两边的词（上下文）作为输入            batch_input.append(input_)            batch_output.append(output)    # batch_size表示一次处理多少句子，每个句子再按2R+1长度做拆分    batch_input = torch.tensor(batch_input, dtype=torch.long)    batch_output = torch.tensor(batch_output, dtype=torch.long)    return batch_input, batch_outputskip-gram模型的数据预处理对于skip-gram模型，取序列中第$R$个词元（中心词）作为网络输入，其余的$2R$个词元（背景词）依次作为label，最终希望网络的输出向量与label代表的词元向量接近def collate_skipgram(batch, text_pipeline):    batch_input, batch_output = [], []    for text in batch:        text_tokens_ids = text_pipeline(text)        if len(text_tokens_ids) &lt; SKIPGRAM_N_WORDS * 2 + 1:            continue        if MAX_SEQUENCE_LENGTH:            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS) # 取中间的词作为输入            outputs = token_id_sequence                      # 两边的词（上下文）作为输出            for output in outputs:                # 依次将每个上下文词作为label，与中间词组成一对训练数据                batch_input.append(input_)                batch_output.append(output)    batch_input = torch.tensor(batch_input, dtype=torch.long)    batch_output = torch.tensor(batch_output, dtype=torch.long)    return batch_input, batch_output网络模型创建CBOW模型class CBOW_Model(nn.Module):    \"\"\"    Implementation of CBOW model described in paper:    https://arxiv.org/abs/1301.3781    \"\"\"    def __init__(self, vocab_size: int):        super(CBOW_Model, self).__init__()        self.embeddings = nn.Embedding(            num_embeddings=vocab_size,            embedding_dim=EMBED_DIMENSION,            max_norm=EMBED_MAX_NORM,        )        self.linear = nn.Linear(            in_features=EMBED_DIMENSION,            out_features=vocab_size,        )    def forward(self, inputs_):        '''        inputs_: [N, 2R]                N表示有多少个序列，是在一个batch中所有句子筛选后截取出来的                R表示每个序列的半径即CBOW_N_WORDS                注意inputs_内容是每个词在vocab字典中的序号        self.embeddings.weights: [K, D]                K表示该数据集中词的数量，即vocab字典的长度                D表示每个词向量化后的维度                因此self.embeddings.weights第i行是vocab中第i个词的向量                self.embeddings操作就是根据inputs_表示的每个词的序号索引得到其向量                其输出是[N, 2R, D]        x.mean 就是将上下文（2R）个词的向量取平均        self.linear 就是做xA+b线性变换，输出为[N, K]        整体看模型的功能是对输入的每个词先做向量化再做分类，当然算法的意图是获得self.embeddings.weights        '''        x = self.embeddings(inputs_)        x = x.mean(axis=1)        x = self.linear(x)        return xskip-gram模型class SkipGram_Model(nn.Module):    \"\"\"    Implementation of Skip-Gram model described in paper:    https://arxiv.org/abs/1301.3781    \"\"\"    def __init__(self, vocab_size: int):        super(SkipGram_Model, self).__init__()        self.embeddings = nn.Embedding(            num_embeddings=vocab_size,            embedding_dim=EMBED_DIMENSION,            max_norm=EMBED_MAX_NORM,        )        self.linear = nn.Linear(            in_features=EMBED_DIMENSION,            out_features=vocab_size,        )    def forward(self, inputs_):        x = self.embeddings(inputs_)        x = self.linear(x)        return x损失函数直接使用交叉熵函数，注意到labels是词元在词汇表中的one-hot向量组成的，outputs也是由与one-hot向量维度一致的向量组成的CrossEntropyLoss等价于LogSoftmax + NLLLoss，其内部的softmax为\\[log\\frac{e^{x_c}}{\\sum_{i=1}^Ce^{x_i}} \\tag{1}\\]式1中$x_c$是网络输出向量第$c$个元素值，$C$是总类别数（也就是向量长度）可以看到这里分母只是对网络输出向量做累加实现向量的归一化，是Word2Vec原始公式（《【Word2Vec】（1）Word2Vec原理分析》公式1）的简化也正是因为这个简化，才能在没有应用层次softmax和negative sampling时确保训练速度criterion = nn.CrossEntropyLoss()loss = criterion(outputs, labels)词元向量提取网络训练完成后所有词元的向量矩阵是embeddings.weights# embedding from first model layerembeddings = list(model.parameters())[0]embeddings = embeddings.cpu().detach().numpy()文献【1】Merity S, Xiong C, Bradbury J, et al. Pointer sentinel mixture models[J]. arXiv preprint arXiv:1609.07843, 2016."
  },
  
  {
    "title": "【词向量】（1）Word2Vec原理分析",
    "url": "/posts/%E8%AF%8D%E5%90%91%E9%87%8F-1-Word2Vec%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/",
    "categories": "NLP, 词向量",
    "tags": "词向量",
    "date": "2023-01-05 00:00:00 +0000",
    





    
    "snippet": "Word2Vec【1】是NLP中词元向量化（Word Embedding）的一种方法关于词元向量化的背景介绍可参考《【循环神经网络】（1）概念及应用》的“生成词表”一节Word2Vec在2018年之前是主流方法，随着BERT、GPT2.0的出现已经不是SOTA的方法了本文推荐参考文章  《斯坦福NLP课程 第1讲 - NLP介绍与词向量初步》  《深度学习方法（十七）：word2vec算法原...",
    "content": "Word2Vec【1】是NLP中词元向量化（Word Embedding）的一种方法关于词元向量化的背景介绍可参考《【循环神经网络】（1）概念及应用》的“生成词表”一节Word2Vec在2018年之前是主流方法，随着BERT、GPT2.0的出现已经不是SOTA的方法了本文推荐参考文章  《斯坦福NLP课程 第1讲 - NLP介绍与词向量初步》  《深度学习方法（十七）：word2vec算法原理（1）：跳字模型（skip-gram） 和连续词袋模型（CBOW）》  《NLP之——Word2Vec详解》动机理想情况下，我们希望词元的向量形式满足  维度不会随着词汇表的增加而迅速增加  向量的距离能够反映对应词元在语义上的相似度针对第一点，Word2Vec提前固定向量维度，确保其与词汇表大小无关针对第二点，Word2Vec认为两个相似的词元，它们的上下文往往也是相似的，基于此假设Word2Vec提出了连续词袋模型(CBOW)和跳字模型(skip-gram)  “You shall know a word by the company it keeps” (J. R. Firth 1957: 11)（一个单词的意思是由经常出现在它附近的单词给出的）需要注意的是上述两种模型只是思路不一样，但都可实现词元向量化如文章《word2vec中cbow 与 skip-gram的比较》分析，一般认为CBOW的计算量小于skip-gram，但训练效果skip-gram要更加好CBOW和skip-gram的基本思路  skip-gram：已知（固定）中心词$w_c$的向量表示$v_c$，通过调整其上下文词元$w_o$的向量表示$u_o$，使得条件概率$p(w_o\\lvert w_c)$最大化  CBOW：已知（固定）上下文词元$w_o$的向量表示$u_o$，通过调整其中心词$w_c$的向量表示$v_c$，使得条件概率$p(w_c\\lvert w_o)$最大化举个例子，假设语料库中有两句话“我爱工作”、“我喜欢工作”，分词后分别为[“我”,”爱”,”工作”]、[“我”,”喜欢”,”工作”]那么理论上无论是最大化$p(w_c\\lvert w_o)$还是$p(w_o\\lvert w_c)$，模型总会趋向于将”爱”和”喜欢”的向量表示调整得更接近因此通过最大化上述条件概率，其实就达到了向量距离与词元语义相似性正相关的目标（这就是为什么Word2Vec属于distributed的方法：目标函数不是最终目的，通过达到该目标获得的中间值才是）skip-gram的目标函数在skip-gram模型中，假设每个词的向量表示维度为$d$，对于在词汇表中索引为$i$的词，当其作为中心词时向量表示为$v_i\\in \\mathbb{R}^d$，当其作为背景词时向量表示为$u_i\\in \\mathbb{R}^d$那么对于某个中心词$w_t$（令其索引为$t$）及其背景词（背景词有多个，数目是算法邻域窗口的尺寸$m$的2倍）中的某个词$w_{t+j}$（令其索引为$t+j$），有\\[p(w_{t+j}|w_t)=\\frac{exp(u_{t+j}^Tv_t)}{\\sum_{j\\in \\nu}exp(u_j^Tv_t)} \\tag{1}\\]其中$\\nu$表示中心词$w_t$的所有背景词在词汇表中的索引范围$[-m,m]$图1. 假设邻域窗口尺寸为2基于以上讨论，skip-gram的目标函数为\\[L(\\theta)=\\prod_{t=1}^T\\prod_{\\begin{aligned}-m\\le &amp;j\\le m\\\\j&amp;\\ne 0\\end{aligned}}P(w_{t+j}|w_t;\\theta) \\tag{2}\\]上式对于每个位置$t=1,…,T$，依次将其作为中心词，在大小为$m$的邻域窗口内预测背景词，$\\theta$是模型所有待优化的权重变量从式2可以看到，Word2Vec认为每个词相互独立，因此可以用连乘来估计最大似然为了优化方便，一般会对连乘的形式取对数转化为累加的形式，式3为skip-gram的损失函数：\\[\\begin{aligned}J(\\theta)&amp;=-\\frac{1}{T}logL(\\theta)\\\\&amp;=-\\frac{1}{T}\\sum_{t=1}^T\\sum_{\\begin{aligned}-m\\le &amp;j\\le m\\\\j&amp;\\ne 0\\end{aligned}}logP(w_{t+j}|w_t;\\theta)\\end{aligned} \\tag{3}\\]CBOW的目标函数CBOW与skip-gram相反，对于每个位置$t=1,…,T$，依次将其作为中心词，在大小为$m$的邻域窗口内根据背景词预测该中心词由于背景词数量有$2m$个，因此需要将所有背景词取平均再通过网络后与中心词计算误差CBOW相当于一个老师（中心词）教多个学生（背景词），而这多个学生又是被平等看待的（取了平均），因此理论上学习效果就比不上skip-gram对于某个中心词$w_t$（令其索引为$t$）及其所有背景词$w_o$，有\\[p(w_t|w_o)=\\frac{exp(v_t^T\\bar{u}_o)}{\\sum_{j\\in \\nu}exp(v_j^T\\bar{u}_o)} \\tag{4}\\]其中$v_t$是索引为$t$的中心词的向量表示，$\\bar{u}_o$是对所有背景词的向量表示取平均，$\\nu$是中心词$w_t$的所有背景词在词汇表中的索引范围$[-m,m]$与skip-gram类似，可推得其目标函数为\\[\\begin{aligned}L(\\theta)&amp;=\\prod_{t=1}^TP(w_t|w_{t-m},...,w_{t-1},w_{t+1},...,w_{t+m};\\theta)\\\\&amp;=\\prod_{t=1}^TP(w_t|w_o;\\theta)\\end{aligned} \\tag{5}\\]进一步得到其损失函数\\[\\begin{aligned}J(\\theta)&amp;=-\\frac{1}{T}logL(\\theta)\\\\&amp;=-\\frac{1}{T}\\sum_{t=1}^TlogP(w_t|w_o;\\theta)\\end{aligned} \\tag{6}\\]Word2Vec的网络设计图2. Word2Vec网络示例如图2所示，假设当前语料库中词元的数量为10000，每个词元向量化后的维度为300，那么网络输入层包含10000个神经元，隐藏层包含300个神经元，输出层包含10000个神经元其中输入层到隐含层是一个索引过程，假设输入词元在词汇表中的索引为7，那么输入向量为one-hot形式000000010000...，想象隐含层记录了一个$10000\\times 300$的矩阵$W$，通过输入的one-hot索引到矩阵的第7行向量（该向量就是对应词元当前的向量表示），然后将该$1\\times 300$的向量与输出层做全连接，输出$1\\times 10000$的向量，网络期望该输出向量与label向量距离最小从上面的分析可以看到，网络其实描述的就是一个线性变换，隐含层记录的$W$矩阵就是算法最终希望得到的向量化表示Word2Vec的梯度推导（以skip-gram为例）skip-gram是以语料库中的所有语句为训练数据，在每次迭代中以中心词为网络输入，对应的背景词为label，按照式3最小化网络输出和label的损失首先随机初始化隐含层矩阵$W$，在每轮迭代中选定中心词$c$作为label，令中心词的向量表示为$v_c$，某个背景词的向量表示为$u_o$1）首先分析损失函数对中心词向量表示的偏导\\[\\begin{aligned}\\frac{\\partial}{\\partial v_c}logP(o|c)&amp;=\\frac{\\partial}{\\partial v_c}log\\frac{exp(u_o^Tv_c)}{\\sum_{i\\in \\nu}exp(u_i^Tv_c)}\\\\&amp;=\\frac{\\partial}{\\partial v_c}\\left(loge^{(u_o^Tv_c)}-log\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}\\right)\\\\&amp;=\\frac{\\partial}{\\partial v_c}\\left(u_o^Tv_c-log\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}\\right)\\\\&amp;=u_o-\\frac{\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}u_i}{\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}}\\\\&amp;=u_o-\\sum_{i\\in \\nu}\\frac{e^{(u_i^Tv_c)}}{\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}}u_i\\\\&amp;=u_o-\\sum_{i\\in \\nu}P(i|c)u_i\\end{aligned} \\tag{7}\\]从式7可以看到，第一项$u_o$是真正的背景词向量形式，第二项是预测的背景词向量形式，训练的过程就是不断降低两者的差异此外个人认为式7的第二项说明了图2中输出层使用全连接的合理性2）然后分析损失函数对背景词向量表示的偏导\\[\\begin{aligned}\\frac{\\partial}{\\partial u_o}logP(o|c)&amp;=\\frac{\\partial}{\\partial u_o}log\\frac{e^{(u_o^Tv_c)}}{\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}}\\\\&amp;=\\frac{\\partial}{\\partial u_o}\\left(loge^{(u_o^Tv_c)}-log\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}\\right)\\\\&amp;=\\frac{\\partial}{\\partial u_o}\\left(u_o^Tv_c-log\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}\\right)\\\\&amp;=v_c-\\frac{\\sum_{i\\in \\nu}\\frac{\\partial}{\\partial u_o}e^{(u_i^Tv_c)}}{\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}}\\\\&amp;=v_c-\\frac{e^{(u_o^Tv_c)}}{\\sum_{i\\in \\nu}e^{(u_i^Tv_c)}}v_c\\\\&amp;=v_c-P(o|c)v_c\\\\&amp;=(1-P(o|c))v_c\\end{aligned}\\tag{8}\\]需要说明的是上式中的$u_o$是背景词中的某一个的向量表示，因此上式中的\\[\\begin{aligned}\\sum_{i\\in \\nu}\\frac{\\partial}{\\partial u_o}e^{(u_i^Tv_c)}&amp;=\\sum_{i\\in \\nu}(e^{u_i^Tv_c}\\frac{\\partial}{\\partial u_o}u_i^Tv_c)\\\\&amp;=e^{u_o^Tv_c}v_c\\end{aligned} \\tag{9}\\]根据式8可以理解，当$P(o\\lvert c)=1$，即通过中心词$c$可以正确预测背景词$o$时，不再需要调整$u_o$Word2Vec的优化从式1和式4可以看到，计算每个词的条件概率时都需要将其与序列中所有其他词向量做点积，最终实现输出的归一化这种方式计算量非常大，严重制约了Word2Vec训练和推理的性能为此Word2Vec作者又在论文【2】中提出了层次softmax(hierarchical softmax)和负采样(negative sampling)两种优化方法hierarchical softmax从整体结构上看，层次softmax将原始的Word2Vec（图2）隐藏层到输出层的全连接以及输出层的softmax替换成二叉树，二叉树的叶子节点数量与输入层的神经元数量（即词汇表中词元数量）一致隐藏层每个节点（词向量）进入二叉树后与当前节点的参数向量做点积+sigmoid决定下一步是左子树还是右子树，最终到达第$i$个叶子节点，代表网络预测其为词汇表中第$i$个词元图3. 层次softmax可以看到在层次softmax中，每个输入词元的最终输出不需要再与序列中其他词元做计算（因为在二叉树中每个节点都已经通过sigmoid实现了归一化），因此大大降低了计算量此外层次softmax使用哈夫曼树进一步优化了计算量图4. 右：哈夫曼二叉树如图4右所示，哈夫曼树是一种带权路径长度最短的二叉树在层次softmax中，首先统计词汇表中每个词元的词频，将其作为参考权重建立哈夫曼树，使得最频繁出现的词路径最短，从而降低计算量详细的公式推导参考《word2vec原理(二) 基于Hierarchical Softmax的模型》negative sampling层次softmax可以显著提高训练速度，但对于低频的词元，哈夫曼树的路径会非常长negative sampling 方法仍然采用图2所示的Word2Vec网络结构，但是在训练时不再更新所有权重：  以skip-gram为例，假设对于待训练的中心词$w_c$，当前迭代的背景词是$w_o$，那么网络输出对应$w_o$的那个节点label为1  同时在词汇表中挑选$K$个（小数据集取[5,20]，大数据集取[2,5]）词作为负样本，其在网络输出对应的节点lable为0  本轮迭代仅更新隐含层到输出层这$N+1$个词元对应的权重负样本的选择：按词频$f(w_i)$进行采样，在词汇表中出现概率越高的词被选中的概率越大为了避免类似’a’、’the’、’of’等高频词的影响，作者建议对词频取3/4幂\\[P(w_i)=\\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{i=1}^Nf(w_i)^{\\frac{3}{4}}} \\tag{10}\\]其它优化1）把常见的词组作为一个单词一般是一些专有名词，如“共和国”，可以将其作为一个词元训练自己的向量2）少采样常见的词类似’a’、’the’、’of’等词元出现的频率非常高，可以设置一个函数，当词频大于某个值时降低采样率\\[P(w_i)=\\left(\\sqrt{\\frac{z(w_i)}{t}}+1\\right)\\frac{t}{z(w_i)} \\tag{11}\\]一般取$t=0.001$，当词频$z(w_i)\\le 0.0026$时$P(w_i)=1$，此时该词元不会被丢弃，当$z(w_i)\\le 0.00746$时，$P(w_i)=0.5$，此时该词元有50%的概率被丢弃Word2Vec的优缺点优点：  Word2Vec考虑上下文，比之前的方法效果更好  Word2Vec的向量维度可控，避免了维数灾难  通用性很强，适用于各种NLP任务在Word2Vec论文中，经过训练的模型可以实现语义上看上去很有趣的结果：king - man + woman = queenbigger - big + small = smallerParis - France + Germany = Berlin缺点：  由于词和向量是一一对应关系，多义词的问题无法解决  2019年提出的BERT已经摒弃了预训练词向量的方法了，而是集成到BERT中做end2end的训练文献【1】Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013.【2】Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality[J]. Advances in neural information processing systems, 2013, 26."
  },
  
  {
    "title": "【循环神经网络】（4）长短时记忆LSTM、循环门单元GRU",
    "url": "/posts/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-4-%E9%95%BF%E7%9F%AD%E6%97%B6%E8%AE%B0%E5%BF%86LSTM-%E5%BE%AA%E7%8E%AF%E9%97%A8%E5%8D%95%E5%85%83GRU/",
    "categories": "深度学习, RNN",
    "tags": "RNN",
    "date": "2022-12-28 00:00:00 +0000",
    





    
    "snippet": "传统RNN简单的对历史状态$s_t$做加权，时间越久远，信息丢失的越多另外RNN中误差沿反向传播是沿时间连乘的形式（《【循环神经网络】（2）BPTT误差反向传播算法推导》式5），因此很容易出现梯度消失和梯度爆炸长短时记忆(Long Short-term memory, LSTM)是对传统RNN的改进，在循环层中增加遗忘机制和记忆机制，实现对历史信息的“主动挑选”同时LSTM的结构可以有效缓解...",
    "content": "传统RNN简单的对历史状态$s_t$做加权，时间越久远，信息丢失的越多另外RNN中误差沿反向传播是沿时间连乘的形式（《【循环神经网络】（2）BPTT误差反向传播算法推导》式5），因此很容易出现梯度消失和梯度爆炸长短时记忆(Long Short-term memory, LSTM)是对传统RNN的改进，在循环层中增加遗忘机制和记忆机制，实现对历史信息的“主动挑选”同时LSTM的结构可以有效缓解梯度消失和梯度爆炸的问题循环门单元GRU则是对LSTM的改进，计算量更小的同时能保持接近的实验效果LSTM结构图1. 左：传统RNN；右：LSTM上图参考《人人都能看懂的LSTM》，可以看到与传统RNN相比，LSTM的循环层多了$c^t$代表的细胞状态  在LSTM中细胞状态$c^t$通常变化很小，由$c^{t-1}$加上一个微小的值得到  隐藏状态$h^t$则变化比较大参考文章认为图1左中的$h^t$对应图1右的$c^t$，个人不这么认为：1）$c^t$是LSTM额外增加的描述该神经元总体特性的指标，其受历史输入缓慢影响2）$h^t$是神经元当前的隐藏状态，其受神经元总体特性和当前输入的影响，最终决定了当前时刻神经元的输出。后面会分析$h^t$在RNN和LSTM中的更新公式，可以发现其本质是一样的循环层的内部结构图2. LSTM循环层内部结构图2参考《Understanding LSTM Networks》，对循环层的每个神经元，可以进一步分解为遗忘门、记忆门和输出门3个部分遗忘门图3. LSTM遗忘门遗忘门以当前时刻输入$x_t$和上一时刻隐藏状态$h_{t-1}$为输入，获得一个权重向量$f_t$，对上一时刻的细胞状态$C_{t-1}$做选择性丢弃\\[f_t=\\sigma(W_f[h_{t-1},x_t]+b_f) \\tag{1}\\]注意$[h_{t-1},x_t]$是将隐藏状态向量和输入序列向量做拼接，等价于将隐藏状态向量和输入序列向量做加权和$\\sigma$表示经过一个sigmoid激活层，其一方面增强其非线性能力，另一方面将输出统一到$[0,1]$，以便作为权重对细胞状态向量做筛选记忆门图4. LSTM记忆门记忆门同样以当前时刻输入$x_t$和上一时刻隐藏状态$h_{t-1}$为输入，将两种信息选择性的添加到细胞状态中\\[\\begin{aligned}i_t&amp;=\\sigma(W_i[h_{t-1},x_t]+b_i)\\\\\\tilde{C}_t&amp;=tanh(W_C[h_{t-1},x_t]+b_C)\\end{aligned} \\tag{2}\\]最终当前时刻的细胞状态$C_t$由式3确定\\[C_t=f_t\\odot C_{t-1}+i_t\\odot \\tilde{C}_t \\tag{3}\\]其中$\\odot$表示向量逐元素相乘输出门图5. LSTM输出门输出门以当前时刻输入$x_t$和上一时刻隐藏状态$h_{t-1}$为输入，同时结合当前时刻细胞状态$C_t$确定该神经元当前时刻隐藏状态$h_t$\\[\\begin{aligned}o_t&amp;=\\sigma(W_o[h_{t-1},x_t]+b_o)\\\\h_t&amp;=o_t\\odot tanh(C_t)\\end{aligned} \\tag{4}\\]  这里对比$h_t$和《【循环神经网络】（1）概念及应用》式1中$S_t$的更新公式，式4就是在$S_t$的基础上增加了细胞状态$C_t$的影响（因此本文认为图1中RNN的$h_t$与LSTM的$h_t$相对应）对当前层当前时刻的神经元输出$y_t$，有\\[y_t=\\sigma(W'h_t) \\tag{5}\\]LSTM缓解梯度消失和梯度爆炸的机制观察细胞状态$c_t$更新公式3和隐藏状态$h_t$更新公式4可以看到，两者都是经过一个sigmoid函数（其结果使得向量元素值趋向于0或1）后充当另一个向量的门控信号这种方式使得梯度要么被保持，要么被直接丢弃，避免了被迅速放大的风险文献【1】和文献【2】对LSTM的机制做了更细致的分析GRU结构GRU去掉了LSTM的细胞状态$c_t$，总体上看是传统RNN的一般形式图6. GRU结构重置门与更新门重置门以当前时刻输入$x_t$和上一时刻隐藏状态$h_{t-1}$为输入，对上一时刻的隐藏状态做筛选\\[\\begin{aligned}r_t&amp;=\\sigma(W_r[h_{t-1},x_t])\\\\\\tilde{h}_t&amp;=tanh(W[r_t\\odot h_{t-1},x_t])\\end{aligned} \\tag{6}\\]更新门同样以当前时刻输入$x_t$和上一时刻隐藏状态$h_{t-1}$为输入，完成隐藏状态的更新\\[\\begin{aligned}z_t&amp;=\\sigma(W_r[h_{t-1},x_t])\\\\h_t&amp;=(1-z_t)\\odot h_{t-1}+z_t\\odot \\tilde{h}_t\\end{aligned} \\tag{7}\\]令$r_t$为1，$z_t$为0，式6和式7代表的GRU特异化为传统的RNNPyTorch的LSTM和GRU接口LSTM与GRU的定义与RNN类似，也是作为一个层应用在网络中'''Applies a multi-layer long short-term memory (LSTM) RNN to an input sequence.'''CLASS torch.nn.LSTM(*args, **kwargs)# input_size    – The number of expected features in the input x# hidden_size   – The number of features in the hidden state h# num_layers    – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1# bias          – If False, then the layer does not use bias weights b_ih and b_hh. Default: True# batch_first   – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False# dropout       – If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0# bidirectional – If True, becomes a bidirectional LSTM. Default: False# proj_size     – If &gt; 0, will use LSTM with projections of corresponding size. Default: 0LSTM的call函数输入输出与RNN类似，多了状态$c_t$1）输入序列input和初始状态h_0、c_0函数的输入序列尺寸为(seq_len, batch, input_size)，如果构造函数中的batch_first为True，则为(batch, seq_len, input_size)初始状态$h_0$尺寸为(num_layers*D, batch, hidden_size)，当bidirectional=True时D=2，否则为1初始状态$c_0$尺寸为(num_layers*D, batch, hidden_size)，当bidirectional=True时D=2，否则为12）输出序列output和最终状态h_n、c_n函数的输出序列尺寸为(seq_len, batch, hidden_size)，如果构造函数中的batch_first为True，则为(batch, seq_len, hidden_size)函数输出的最终状态h_n尺寸为(num_layers*D, batch, hidden_size)，当bidirectional=True时D=2，否则为1函数输出的最终状态c_n尺寸为(num_layers*D, batch, hidden_size)，当bidirectional=True时D=2，否则为1官方的一个小例子&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)&gt;&gt;&gt; input = torch.randn(5, 3, 10)&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)&gt;&gt;&gt; c0 = torch.randn(2, 3, 20)&gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))'''Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.'''CLASStorch.nn.GRU(*args, **kwargs)# input_size    – The number of expected features in the input x# hidden_size   – The number of features in the hidden state h# num_layers    – Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two GRUs together to form a stacked GRU, with the second GRU taking in outputs of the first GRU and computing the final results. Default: 1# bias          – If False, then the layer does not use bias weights b_ih and b_hh. Default: True# batch_first   – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature). Note that this does not apply to hidden or cell states. See the Inputs/Outputs sections below for details. Default: False# dropout       – If non-zero, introduces a Dropout layer on the outputs of each GRU layer except the last layer, with dropout probability equal to dropout. Default: 0# bidirectional – If True, becomes a bidirectional GRU. Default: False&gt;&gt;&gt; rnn = nn.GRU(10, 20, 2)&gt;&gt;&gt; input = torch.randn(5, 3, 10)&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)&gt;&gt;&gt; output, hn = rnn(input, h0)文献【1】Jozefowicz R, Zaremba W, Sutskever I. An empirical exploration of recurrent network architectures[C]//International conference on machine learning. PMLR, 2015: 2342-2350.【2】Chung J, Gulcehre C, Cho K H, et al. Empirical evaluation of gated recurrent neural networks on sequence modeling[J]. arXiv preprint arXiv:1412.3555, 2014."
  },
  
  {
    "title": "【循环神经网络】（3）PyTorch的RNN接口",
    "url": "/posts/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-3-PyTorch%E7%9A%84RNN%E6%8E%A5%E5%8F%A3/",
    "categories": "深度学习, RNN",
    "tags": "RNN",
    "date": "2022-12-19 00:00:00 +0000",
    





    
    "snippet": "在PyTorch实现上，CNN通过搭建不同的功能层组成一个网络，而RNN可以理解为一个层（从图1可以看到RNN实现在PyTorch中与CNN的层实现属于同一级）图1. RNN在PyTorch中的定义RNN在PyTorch中的定义图2. RNN展开结构RNN的构造函数RNN的构造函数需要确定循环层输入和状态尺寸'''Applies a multi-layer Elman RNN with ta...",
    "content": "在PyTorch实现上，CNN通过搭建不同的功能层组成一个网络，而RNN可以理解为一个层（从图1可以看到RNN实现在PyTorch中与CNN的层实现属于同一级）图1. RNN在PyTorch中的定义RNN在PyTorch中的定义图2. RNN展开结构RNN的构造函数RNN的构造函数需要确定循环层输入和状态尺寸'''Applies a multi-layer Elman RNN with tanh or ReLU non-linearity to an input sequence'''CLASS torch.nn.RNN(*args, **kwargs)# input_size    – The number of expected features in the input x# hidden_size   – The number of features in the hidden state h# num_layers    – Number of recurrent layers# nonlinearity  – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'# bias          – If False, then the layer does not use bias weights b_ih and b_hh. Default: True# batch_first   – If True, then the input and output tensors are provided as (batch, seq, feature) instead of (seq, batch, feature)# dropout       – If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer. Default: 0# bidirectional – If True, becomes a bidirectional RNN. Default: False\\[h_t=tanh(x_tW_{ih}^T+b_{ih}+h_{t-1}W_{hh}^T+b_{hh}) \\tag{1}\\]1）input_size：是第一个循环层的输入节点数，对应图2右的蓝色圆点个数，对应式1中$x_t$的长度这里特别需要与序列长度区别：假设对于输入序列i am what i am，序列长度是4，而每个单词都向量化后的长度$m$就是input_size2）hidden_size：是循环层的节点数，对应图2右的红色圆点个数，对应式1中$h_t$的长度3）num_layers：是循环层的层数在RNN中所有循环层的hidden_size都是一样的，层与层之间是全连接4）nonlinearity：RNN类的实现仅支持`tanh`和`relu`两种激活函数5）bias和dropout：在循环层是否使用bias和dropout6）batch_first：默认的输入为(seq_len, batch, input_size)，输出为(seq_len, batch, hidden_size)，如果该项为True，则输入和输出的batch项在首位注意：该选项不改变循环层的状态变量尺寸，其总是将batch放在第2位7）bidirectional：是否为双向RNNRNN的call函数该函数实现对输入序列的前向和后向计算  函数输入包含输入序列、初始循环层的状态  函数输出包含最后一个循环层所有时刻（全序列）输出、所有循环层的最后时刻状态1）输入序列input和初始状态h_0函数的输入序列尺寸为(seq_len, batch, input_size)，如果构造函数中的batch_first为True，则为(batch, seq_len, input_size)初始状态尺寸为(num_layers*num_directions, batch, hidden_size)，当网络为双向RNN时（bidirectional=True），num_directions=2，否则为1因为RNN是以网络层的身份嵌入某个网络的，因此需要在训练迭代过程中需要用当前循环层状态作为下一轮迭代RNN的初始状态来更新计算（参考下节“一个示例”）2）输出序列output和最终状态h_n函数的输出序列尺寸为(seq_len, batch, hidden_size)，如果构造函数中的batch_first为True，则为(batch, seq_len, hidden_size)函数输出的最终状态尺寸为(num_layers*num_directions, batch, hidden_size)，当网络为双向RNN时（bidirectional=True），num_directions=2，否则为1注意到RNN的输出output是最后一层每个节点在每个时刻（序列的每个元素）的值，因此后面一般还需要接全连接等层得到真正需要的输出一个示例该示例来自《最简单的RNN回归模型入门(PyTorch)》，使用sin曲线序列为输入，希望RNN实现cos曲线序列输出import torchfrom torch import nnimport numpy as nptorch.manual_seed(2019)# 超参设置TIME_STEP = 10  # RNN时间步长（序列长度）INPUT_SIZE = 1  # RNN输入尺寸（input_size）INIT_LR = 0.02  # 初始学习率N_EPOCHS = 100  # 训练回数class RNN(nn.Module):    def __init__(self):        super(RNN, self).__init__()        self.rnn = nn.RNN(            input_size=INPUT_SIZE,            hidden_size=32,  # RNN隐藏神经元个数            num_layers=1,  # RNN隐藏层个数        )        self.out = nn.Linear(32, 1)    def forward(self, x, h):        # x (time_step, batch_size, input_size)        # h (n_layers, batch, hidden_size)        # out (time_step, batch_size, hidden_size)        out, h = self.rnn(x, h)        prediction = self.out(out)        return prediction, hrnn = RNN()print(rnn)optimizer = torch.optim.Adam(rnn.parameters(), lr=INIT_LR)loss_func = nn.MSELoss()h_state = None  # 初始化隐藏层for step in range(N_EPOCHS):    start, end = step * np.pi, (step + 1) * np.pi  # 时间跨度    # 使用Sin函数预测Cos函数    steps = np.linspace(start, end, TIME_STEP, dtype=np.float32, endpoint=False)    x_np = np.sin(steps)    y_np = np.cos(steps)    x = torch.from_numpy(x_np[:, np.newaxis, np.newaxis])  # 尺寸大小为(time_step, batch, input_size)    y = torch.from_numpy(y_np[:, np.newaxis, np.newaxis])    prediction, h_state = rnn(x, h_state)  # RNN输出（预测结果，隐藏状态）    h_state = h_state.detach()  # 这一行很重要，将每一次输出的中间状态传递下去(不带梯度)    loss = loss_func(prediction, y)    optimizer.zero_grad()    loss.backward()    optimizer.step()序列长度不统一的问题在很多RNN应用中无法保证输入的序列长度都是统一的，比如对于语言建模，语句长度总是长短不一但与CNN一样，RNN中一个batch的tensor尺寸需要提前固定为此PyTorch提供了序列包装方法torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)参考官网的例子很容易理解&gt;&gt;&gt; import torch&gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence&gt;&gt;&gt; a = torch.tensor([1,2,3])&gt;&gt;&gt; b = torch.tensor([4,5])&gt;&gt;&gt; c = torch.tensor([6])&gt;&gt;&gt; pack_sequence([a, b, c])PackedSequence(data=tensor([1, 4, 6, 2, 5, 3]), batch_sizes=tensor([3, 2, 1]), sorted_indices=None, unsorted_indices=None)当pack_sequence的参数enforce_sorted=True时，需要确保序列[a, b, c]的长度是从大到小的图3. pack_sequence原理如图3所示，batch_size=2，样本1的序列长度为3，样本2的序列长度为1，第1次循环时同时输入样本1的1和样本2的1，第2次循环时仅输入样本1的2（此时实际batch_size=1）现在再来看例子中的data=tensor([1, 4, 6, 2, 5, 3]和batch_sizes=tensor([3, 2, 1])，应该就很好理解了此外，如果输入的序列实际长度虽然不一样，但短序列已经用0做padding补齐了，可以用pack_padded_sequence做封装torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)"
  },
  
  {
    "title": "【循环神经网络】（2）BPTT误差反向传播算法推导",
    "url": "/posts/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-2-BPTT%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E6%8E%A8%E5%AF%BC/",
    "categories": "深度学习, RNN",
    "tags": "RNN",
    "date": "2022-12-14 00:00:00 +0000",
    





    
    "snippet": "BackPropagation Through Time（BPTT）是针对RNN的误差反向传播算法本文重点参考《零基础入门深度学习(5) - 循环神经网络》，此外可以参考推导小论文BackPropagation Through Time图1. RNN结构展开如图1所示，相比CNN，RNN有两个前向传播方向：从下往上是与CNN类似的层间传播，从左往右是每个中间层随时间的传播因此反向传播也分为两...",
    "content": "BackPropagation Through Time（BPTT）是针对RNN的误差反向传播算法本文重点参考《零基础入门深度学习(5) - 循环神经网络》，此外可以参考推导小论文BackPropagation Through Time图1. RNN结构展开如图1所示，相比CNN，RNN有两个前向传播方向：从下往上是与CNN类似的层间传播，从左往右是每个中间层随时间的传播因此反向传播也分为两个，一个是从上往下的层间传播，一个是从右往左沿时间线的传播BPTT循环神经网络传播为方便讨论，这里考虑一个中间层的RNN结构图2. 左：单层RNN展开结构；右：输入层到中间层的全连接结构$x_t$是$t$时刻输入向量，$s_t$是$t$时刻中间层向量，$o_t$是$t$时刻输出层向量$U$是输入层到中间层的权重矩阵，$V$是中间层到输出层的权重矩阵，$W$是中间层上一次的值作为这一次输入的权重矩阵前向计算中间层向量的前向更新如式1所示\\[s_t=f(Ux_t+Ws_{t-1}) \\tag{1}\\]现在令$x_t$维度为$m\\times 1$，$s_t$维度为$n\\times 1$，那么$U$维度为$n\\times m$，$W$维度为$n\\times n$如图2右所示，输入层的每个节点都会与中间层的所有节点相连。因此将式1展开为矩阵形式有\\[\\left [\\begin{matrix}s_1^t\\\\s_2^t\\\\\\vdots\\\\s_n^t\\end{matrix}\\right ]=f\\left(\\left[\\begin{matrix}u_{11}&amp;u_{12}&amp;\\cdots&amp;u_{1m}\\\\u_{21}&amp;u_{22}&amp;\\cdots&amp;u_{2m}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\u_{n1}&amp;u_{n2}&amp;\\cdots&amp;u_{nm}\\end{matrix}\\right]\\left[\\begin{matrix}x_1\\\\x_2\\\\\\vdots\\\\x_m\\end{matrix}\\right]+\\left[\\begin{matrix}w_{11}&amp;w_{12}&amp;\\cdots&amp;w_{1n}\\\\w_{21}&amp;w_{22}&amp;\\cdots&amp;w_{2n}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\w_{n1}&amp;w_{n2}&amp;\\cdots&amp;w_{nn}\\end{matrix}\\right]\\left[\\begin{matrix}s_1^{t-1}\\\\s_2^{t-1}\\\\\\vdots\\\\s_n^{t-1}\\end{matrix}\\right]\\right) \\tag{2}\\]每个神经元的误差计算令第$l$层$t$时刻的误差向量为$\\delta_t^l$，在RNN中它将沿两个方向传播：  一个方向是传递到上一层网络得到$\\delta_t^{l-1}$，这部分仅与权重矩阵$U$相关  一个方向是反向沿时间线传递到初始的$t_1$时刻得到$\\delta_1^l$，这部分仅与权重矩阵$W$相关令网络输出的误差为$E$，有\\[\\delta_t^l=\\frac{\\partial E}{\\partial net_t^l} \\tag{3}\\]其中$net_t$为神经元在$t$时刻的加权输入，即\\[\\begin{aligned}net_t&amp;=Ux_t+Ws_{t-1}\\\\s_{t-1}&amp;=f(net_{t-1})\\end{aligned} \\tag{4}\\]      式3类似于BP中的误差定义，参考《反向传播四个基本方程》的式2，$\\delta_t^l$反映了输出误差对$l$层$t$时刻神经元的影响        式3中$E$是一个值，而$net_t^l$尺寸是$n\\times 1$，因此$\\delta_t^l$尺寸为$1\\times n$  沿时间线误差传播先来研究$\\delta_t^l$和$\\delta_1^l$的关系先不加推导给出误差沿时间反向传播公式\\[\\delta_1^l=\\delta_t^l\\prod_{i=1}^{t-1}{Wdiag(f'(net_i^l))} \\tag{5}\\]其中$diag(x)$表示用向量$x$建立对角矩阵根据式5，第$l$层$t$时刻和$t-1$时刻的误差关系为\\[\\delta_{t-1}^l=\\delta_t^lWdiag(f'(net_{t-1}^l)) \\tag{6}\\]===================================现在对公式5做推导对式3做链式求导，有\\[\\begin{aligned}\\delta_t^l&amp;=\\frac{\\partial E}{\\partial net_t^l}\\\\&amp;=\\frac{\\partial E}{\\partial net_k^l}\\frac{\\partial net_k^l}{\\partial net_t^l}\\\\&amp;=\\frac{\\partial E}{\\partial net_k^l}\\frac{\\partial net_k^l}{\\partial net_{k-1}^l}\\frac{\\partial net_{k-1}^l}{\\partial net_{k-2}^l}...\\frac{\\partial net_{t+1}^l}{\\partial net_t^l}\\end{aligned} \\tag{7}\\]根据式7，进一步对$\\frac{\\partial net_{t}^l}{\\partial net_{t-1}^l}$做链式求导，同时结合式4，有\\[\\begin{aligned}\\frac{\\partial net_{t}^l}{\\partial net_{t-1}^l}&amp;=\\frac{\\partial net_{t}^l}{\\partial s_{t-1}^l}\\frac{\\partial s_{t-1}^l}{\\partial net_{t-1}^l}\\\\&amp;=Wdiag(f'(net_{t-1}^l))\\end{aligned} \\tag{8}\\]这里要注意式8中的第2项偏导：根据定义，$s_{t-1}^l$是维度为$n$的向量，$net_{t-1}^l$也是维度为$n$的向量，结果必然是$n\\times n$的矩阵（参考雅克比矩阵wiki）\\[\\begin{aligned}\\frac{\\partial s_{t-1}}{\\partial net_{t-1}}&amp;=\\left[\\begin{matrix}\\frac{\\partial s_1^{t-1}}{\\partial net_1^{t-1}}&amp;\\frac{\\partial s_1^{t-1}}{\\partial net_2^{t-1}}&amp;...&amp;\\frac{\\partial s_1^{t-1}}{\\partial net_n^{t-1}}\\\\\\frac{\\partial s_2^{t-1}}{\\partial net_1^{t-1}}&amp;\\frac{\\partial s_2^{t-1}}{\\partial net_2^{t-1}}&amp;...&amp;\\frac{\\partial s_2^{t-1}}{\\partial net_n^{t-1}}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\\\frac{\\partial s_n^{t-1}}{\\partial net_1^{t-1}}&amp;\\frac{\\partial s_n^{t-1}}{\\partial net_2^{t-1}}&amp;...&amp;\\frac{\\partial s_n^{t-1}}{\\partial net_n^{t-1}}\\end{matrix}\\right]\\\\&amp;=\\left[\\begin{matrix}f'(net_1^{t-1})&amp;0&amp;...&amp;0\\\\0&amp;f'(net_2^{t-1})&amp;...&amp;0\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\0&amp;0&amp;...&amp;f'(net_n^{t-1})\\end{matrix}\\right]\\\\&amp;=diag(f'(net_{t-1}))\\end{aligned} \\tag{9}\\]  式9为方便描述，忽略了层标$l$  因为$s_{t-1}^l$是$net_{t-1}^l$经函数映射（激活函数）的结果（式4，$f:n\\rightarrow n$），所以对于$i\\neq j$，有$\\frac{\\partial s_i^t}{\\partial net_j^t}=0$结合式8，式7可进一步写为\\[\\begin{aligned}\\delta_t^l&amp;=\\frac{\\partial E}{\\partial net_t^l}\\\\&amp;=\\delta_k^l*Wdiag(f'(net_{k-1}^l))*Wdiag(f'(net_{k-2}^l))*...*Wdiag(f'(net_t^l))\\\\&amp;=\\delta_k^l\\prod_{i=t}^{k-1}Wdiag(f'(net_i^l))\\end{aligned} \\tag{10}\\]===================================沿层间误差传播对式4稍加修改，有\\[\\begin{aligned}net_t^l&amp;=Ua_t^{l-1}+Ws_{t-1}^l\\\\a_t^{l-1}&amp;=f^{l-1}(net_t^{l-1})\\end{aligned} \\tag{11}\\]其中$net_t^l$是$l$层$t$时刻的加权输入，$a_t^{l-1}$是$l-1$层$t$时刻的神经元输出，$f^{l-1}$是$l-1$层的激活函数与式8类似，有\\[\\begin{aligned}\\frac{\\partial net_{t}^l}{\\partial net_t^{l-1}}&amp;=\\frac{\\partial net_{t}^l}{\\partial a_t^{l-1}}\\frac{\\partial a_t^{l-1}}{\\partial net_t^{l-1}}\\\\&amp;=Udiag(f'^{l-1}(net_t^{l-1}))\\end{aligned} \\tag{12}\\]根据式12，得到误差层间传播的推导结果\\[\\begin{aligned}\\delta_t^l&amp;=\\frac{\\partial E}{\\partial net_t^l}\\\\&amp;=\\delta_t^k*Udiag(f'^{k-1}(net_t^{k-1}))*Udiag(f'^{k-2}(net_t^{k-2}))*...*Udiag(f'^l(net_t^l))\\\\&amp;=\\delta_t^k\\prod_{i=l}^{k-1}Wdiag(f'^{i}(net_t^i))\\end{aligned} \\tag{13}\\]计算权重梯度得到每个神经元的误差向量后，就可以进一步求得网络权重的梯度了沿时间线方向的误差向量用于求解权重矩阵$W$的梯度沿层间方向的误差向量用于求解权重矩阵$U$的梯度$W$的梯度求解令$\\nabla_{W_t}E$为误差$E$对$t$时刻的权重矩阵$W$的梯度，$\\nabla_WE$为最终的梯度，有\\[\\begin{aligned}\\nabla_WE&amp;=\\sum_{i=1}^t\\nabla_{W_t}E\\\\&amp;=\\left[\\begin{matrix}\\delta_1^ts_1^{t-1}&amp;\\delta_1^ts_2^{t-1}&amp;...&amp;\\delta_1^ts_n^{t-1}\\\\\\delta_2^ts_1^{t-1}&amp;\\delta_2^ts_2^{t-1}&amp;...&amp;\\delta_2^ts_n^{t-1}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\\\delta_n^ts_1^{t-1}&amp;\\delta_n^ts_2^{t-1}&amp;...&amp;\\delta_n^ts_n^{t-1}\\end{matrix}\\right]+\\left[\\begin{matrix}\\delta_1^1s_1^0&amp;\\delta_1^1s_2^0&amp;...&amp;\\delta_1^1s_n^0\\\\\\delta_2^1s_1^0&amp;\\delta_2^1s_2^0&amp;...&amp;\\delta_2^1s_n^0\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\\\delta_n^1s_1^0&amp;\\delta_n^1s_2^0&amp;...&amp;\\delta_n^1s_n^0\\end{matrix}\\right]\\end{aligned} \\tag{14}\\]注意对于RNN，一个时刻是一次前向传播的样本中的一个词元，因此迭代算梯度时应该要计算所有时刻===================================下面对式14做推导首先研究$\\nabla_{W_t}E$将式4中第一个公式写成矩阵形式，有\\[\\begin{aligned}\\left[\\begin{matrix}net_1^t\\\\net_2^t\\\\\\vdots\\\\net_n^t\\end{matrix}\\right]&amp;=Ux_t+\\left[\\begin{matrix}w_{11}&amp;w_{12}&amp;\\cdots&amp;w_{1n}\\\\w_{21}&amp;w_{22}&amp;\\cdots&amp;w_{2n}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\w_{n1}&amp;w_{n2}&amp;\\cdots&amp;w_{nn}\\end{matrix}\\right]\\left[\\begin{matrix}s_1^{t-1}\\\\s_2^{t-1}\\\\\\vdots\\\\s_n^{t-1}\\end{matrix}\\right]\\\\&amp;=Ux_t+\\left[\\begin{matrix}w_{11}s_1^{t-1}&amp;w_{12}s_2^{t-1}&amp;\\cdots&amp;w_{1n}s_n^{t-1}\\\\w_{21}s_1^{t-1}&amp;w_{22}s_2^{t-1}&amp;\\cdots&amp;w_{2n}s_n^{t-1}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\w_{n1}s_1^{t-1}&amp;w_{n2}s_2^{t-1}&amp;\\cdots&amp;w_{nn}s_n^{t-1}\\end{matrix}\\right]\\end{aligned} \\tag{15}\\]可以看到$net_t$对$W$求导与$Ux_t$无关，而$net_j^t$只与$w_{ji}$有关因此结合式3，有\\[\\begin{aligned}\\frac{\\partial E}{\\partial w_{ji}^t}&amp;=\\frac{\\partial E}{\\partial net_j^t}\\frac{\\partial net_j^t}{\\partial w_{ji}^t}\\\\&amp;=\\delta_j^ts_i^{t-1}\\end{aligned} \\tag{16}\\]其中$\\delta_j^t$为时刻$t$误差向量的第$j$项，$s_i^{t-1}$表示$t-1$时刻中间层第$i$个神经元输出值根据式16，有\\[\\nabla_{W_t}E=\\left[\\begin{matrix}\\delta_1^ts_1^{t-1}&amp;\\delta_1^ts_2^{t-1}&amp;\\cdots&amp;\\delta_1^ts_n^{t-1}\\\\\\delta_2^ts_1^{t-1}&amp;\\delta_2^ts_2^{t-1}&amp;\\cdots&amp;\\delta_2^ts_n^{t-1}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\\\delta_n^ts_1^{t-1}&amp;\\delta_n^ts_2^{t-1}&amp;\\cdots&amp;\\delta_n^ts_n^{t-1}\\end{matrix}\\right] \\tag{17}\\]现在进一步证明$\\nabla_WE$是所有时刻的$\\nabla_{W_t}E$的累加还是从式4出发，将其整合成一个式子，有\\[net_t=Ux_t+Wf(net_{t-1}) \\tag{18}\\]因为$W$和$f(net_{t-1})$都是$W$的函数，所以有\\[\\frac{\\partial net_t}{\\partial W}=\\frac{\\partial W}{\\partial W}f(net_{t-1})+W\\frac{\\partial f(net_{t-1})}{\\partial W} \\tag{19}\\]根据上式，有\\[\\begin{aligned}\\nabla_WE&amp;=\\frac{\\partial E}{\\partial W}\\\\&amp;=\\frac{\\partial E}{\\partial net_t}\\frac{\\partial net_t}{\\partial W}\\\\&amp;=\\delta_t\\frac{\\partial W}{\\partial W}f(net_{t-1})+\\delta_tW\\frac{\\partial f(net_{t-1})}{\\partial W}\\end{aligned} \\tag{20}\\]先考虑$\\frac{\\partial W}{\\partial W}$，这是矩阵对矩阵的求导\\[\\begin{aligned}\\frac{\\partial W}{\\partial W}&amp;=\\left[\\begin{matrix}\\frac{\\partial w_{11}}{\\partial W}&amp;\\frac{\\partial w_{12}}{\\partial W}&amp;\\cdots&amp;\\frac{\\partial w_{1n}}{\\partial W}\\\\\\frac{\\partial w_{21}}{\\partial W}&amp;\\frac{\\partial w_{22}}{\\partial W}&amp;\\cdots&amp;\\frac{\\partial w_{2n}}{\\partial W}\\\\\\vdots&amp;&amp;\\ddots&amp;\\vdots\\\\\\frac{\\partial w_{n1}}{\\partial W}&amp;\\frac{\\partial w_{n2}}{\\partial W}&amp;\\cdots&amp;\\frac{\\partial w_{nn}}{\\partial W}\\end{matrix}\\right]\\\\&amp;=\\left[\\begin{matrix}\\left[\\begin{matrix}\\frac{\\partial w_{11}}{\\partial w_{11}}&amp;\\frac{\\partial w_{11}}{\\partial w_{12}}&amp;\\cdots&amp;\\frac{\\partial w_{11}}{\\partial w_{1n}}\\\\\\frac{\\partial w_{11}}{\\partial w_{21}}&amp;\\frac{\\partial w_{11}}{\\partial w_{22}}&amp;\\cdots&amp;\\frac{\\partial w_{11}}{\\partial w_{2n}}\\\\&amp;&amp;\\vdots\\\\\\frac{\\partial w_{11}}{\\partial w_{n1}}&amp;\\frac{\\partial w_{11}}{\\partial w_{n2}}&amp;\\cdots&amp;\\frac{\\partial w_{11}}{\\partial w_{nn}}\\end{matrix}\\right]&amp;\\left[\\begin{matrix}\\frac{\\partial w_{12}}{\\partial w_{11}}&amp;\\frac{\\partial w_{12}}{\\partial w_{12}}&amp;\\cdots&amp;\\frac{\\partial w_{12}}{\\partial w_{1n}}\\\\\\frac{\\partial w_{12}}{\\partial w_{21}}&amp;\\frac{\\partial w_{12}}{\\partial w_{22}}&amp;\\cdots&amp;\\frac{\\partial w_{12}}{\\partial w_{2n}}\\\\&amp;&amp;\\vdots\\\\\\frac{\\partial w_{12}}{\\partial w_{n1}}&amp;\\frac{\\partial w_{12}}{\\partial w_{n2}}&amp;\\cdots&amp;\\frac{\\partial w_{12}}{\\partial w_{nn}}\\end{matrix}\\right]&amp;\\cdots\\\\\\vdots\\end{matrix}\\right]\\\\&amp;=\\left[\\begin{matrix}\\left[\\begin{matrix}1&amp;0&amp;\\cdots&amp;0\\\\0&amp;0&amp;\\cdots&amp;0\\\\&amp;&amp;\\cdots\\\\0&amp;0&amp;\\cdots&amp;0\\end{matrix}\\right]&amp;\\left[\\begin{matrix}0&amp;1&amp;\\cdots&amp;0\\\\0&amp;0&amp;\\cdots&amp;0\\\\&amp;&amp;\\cdots\\\\0&amp;0&amp;\\cdots&amp;0\\end{matrix}\\right]&amp;\\cdots\\\\\\vdots\\end{matrix}\\right]\\end{aligned} \\tag{21}\\]因此，结合式17，式20的左边可以展开为\\[\\begin{aligned}\\delta_t\\frac{\\partial W}{\\partial W}f(net_{t-1})&amp;=\\delta_t\\frac{\\partial W}{\\partial W}s_{t-1}\\\\&amp;=\\delta_t\\left[\\begin{matrix}\\left[\\begin{matrix}1&amp;0&amp;\\cdots&amp;0\\\\0&amp;0&amp;\\cdots&amp;0\\\\&amp;&amp;\\cdots\\\\0&amp;0&amp;\\cdots&amp;0\\end{matrix}\\right]&amp;\\left[\\begin{matrix}0&amp;1&amp;\\cdots&amp;0\\\\0&amp;0&amp;\\cdots&amp;0\\\\&amp;&amp;\\cdots\\\\0&amp;0&amp;\\cdots&amp;0\\end{matrix}\\right]&amp;\\cdots\\\\\\vdots\\end{matrix}\\right]\\left[\\begin{matrix}s_1^{t-1}\\\\s_2^{t-1}\\\\\\vdots\\\\s_n^{t-1}\\end{matrix}\\right]\\\\&amp;=\\delta_t\\left[\\begin{matrix}\\left[\\begin{matrix}s_1^{t-1}\\\\0\\\\\\vdots\\\\0\\end{matrix}\\right]&amp;\\left[\\begin{matrix}s_2^{t-1}\\\\0\\\\\\vdots\\\\0\\end{matrix}\\right]&amp;\\cdots\\\\\\vdots\\end{matrix}\\right]\\\\&amp;=\\left[\\begin{matrix}\\delta_1^t&amp;\\delta_2^t\\cdots&amp;\\delta_n^t\\end{matrix}\\right]\\left[\\begin{matrix}\\left[\\begin{matrix}s_1^{t-1}\\\\0\\\\\\vdots\\\\0\\end{matrix}\\right]&amp;\\left[\\begin{matrix}s_2^{t-1}\\\\0\\\\\\vdots\\\\0\\end{matrix}\\right]&amp;\\cdots\\\\\\vdots\\end{matrix}\\right]\\\\&amp;=\\left[\\begin{matrix}\\delta_1^ts_1^{t-1}&amp;\\delta_1^ts_2^{t-1}&amp;\\cdots&amp;\\delta_1^ts_n^{t-1}\\\\\\delta_2^ts_1^{t-1}&amp;\\delta_2^ts_2^{t-1}&amp;\\cdots&amp;\\delta_2^ts_n^{t-1}\\\\&amp;&amp;\\cdots\\\\\\delta_n^ts_1^{t-1}&amp;\\delta_n^ts_2^{t-1}&amp;\\cdots&amp;\\delta_n^ts_n^{t-1}\\end{matrix}\\right]\\\\&amp;=\\nabla_{W_t}E\\end{aligned} \\tag{22}\\]现在考虑式20右边部分\\[\\begin{aligned}\\delta_tW\\frac{\\partial f(net_{t-1})}{\\partial W}&amp;=\\delta_tW\\frac{\\partial f(net_{t-1})}{\\partial net_{t-1}}\\frac{\\partial net_{t-1}}{\\partial W}\\\\&amp;=\\delta_tWf'(net_{t-1})\\frac{\\partial net_{t-1}}{\\partial W}\\\\&amp;=\\delta_t\\frac{\\partial net_t}{\\partial net_{t-1}}\\frac{\\partial net_{t-1}}{\\partial W}\\\\&amp;=\\delta_{t-1}\\frac{\\partial net_{t-1}}{\\partial W}\\end{aligned} \\tag{23}\\]根据式22和23，式20可展开为\\[\\begin{aligned}\\nabla_WE&amp;=\\frac{\\partial E}{\\partial W}\\\\&amp;=\\frac{\\partial E}{\\partial net_t}\\frac{\\partial net_t}{\\partial W}\\\\&amp;=\\nabla_{W_t}E+\\delta_{t-1}\\frac{\\partial net_{t-1}}{\\partial W}\\\\&amp;=\\nabla_{W_t}E+\\nabla_{W_{t-1}}E+\\delta_{t-2}\\frac{\\partial net_{t-2}}{\\partial W}\\\\&amp;=\\nabla_{W_t}E+\\nabla_{W_{t-1}}E+...+\\nabla_{W_1}E\\\\&amp;=\\sum_{k=1}^t\\nabla_{W_k}E\\end{aligned} \\tag{24}\\]式14得证===================================$U$的梯度求解与权重矩阵$W$求解方法类似，也可得到$t$时刻权重矩阵$U$的梯度\\[\\nabla_{U_i}E=\\left[\\begin{matrix}\\delta_1^tx_1^t&amp;\\delta_1^tx_2^t&amp;\\cdots&amp;\\delta_1^tx_m^t\\\\\\delta_2^tx_1^t&amp;\\delta_2^tx_2^t&amp;\\cdots&amp;\\delta_2^tx_m^t\\\\&amp;&amp;\\cdots\\\\\\delta_n^tx_1^t&amp;\\delta_n^tx_2^t&amp;\\cdots&amp;\\delta_n^tx_m^t\\end{matrix}\\right] \\tag{25}\\]最终权重矩阵$U$所有时刻的梯度也是各个时刻的梯度之和\\[\\nabla_UE=\\sum_{i=1}^t\\nabla_{U_i}E \\tag{26}\\]"
  },
  
  {
    "title": "【循环神经网络】（1）概念及应用",
    "url": "/posts/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-1-%E6%A6%82%E5%BF%B5%E5%8F%8A%E5%BA%94%E7%94%A8/",
    "categories": "深度学习, RNN",
    "tags": "RNN",
    "date": "2022-12-13 00:00:00 +0000",
    





    
    "snippet": "基本概念单向循环神经网络卷积神经网络CNN只能单独处理一个一个的输入，当前输出只与当前输入有关循环神经网络RNN会“缓存”之前输入的信息，使得当前输出受之前的所有输入影响图1. 左：CNN；右：RNN图1中，$X$是输入向量，$S$是隐藏层的向量，$O$是输出层的向量，$U$是输入层到隐藏层的权重矩阵，$V$是隐藏层到输出层的权重矩阵$W$是隐藏层上一次的向量$S_{t-1}$到本次的向量$...",
    "content": "基本概念单向循环神经网络卷积神经网络CNN只能单独处理一个一个的输入，当前输出只与当前输入有关循环神经网络RNN会“缓存”之前输入的信息，使得当前输出受之前的所有输入影响图1. 左：CNN；右：RNN图1中，$X$是输入向量，$S$是隐藏层的向量，$O$是输出层的向量，$U$是输入层到隐藏层的权重矩阵，$V$是隐藏层到输出层的权重矩阵$W$是隐藏层上一次的向量$S_{t-1}$到本次的向量$S_t$的权重矩阵参考《一文搞懂RNN（循环神经网络）基础篇》，将图1中的RNN展开图2. RNN一层结构从图2可以看到，其实RNN的思想很朴素：通过对前面时刻的信息做加权，实现对时序信息有记忆的输出\\[\\begin{aligned}O_t&amp;=g(V*S_t)\\\\S_t&amp;=f(U*X_t+W*S_{t-1})\\end{aligned} \\tag{1}\\]双向循环神经网络单向循环神经网络只考虑当前及之前时刻的输入，双向循环神经网络BRNN同时还考虑未来时刻的输入图3. BRNN一层结构双向循环神经网络每层循环计算可以进一步拆分为两层，这两层输入都是$X$，但是信息传递方向相反\\[\\begin{aligned}y_t&amp;=g(h_t^{(1)}+h_t^{(2)})\\\\h_t^{(1)}&amp;=f(U^{(1)}*X_t+W^{(1)}*h_{t-1}^{(1)})\\\\h_t^{(2)}&amp;=f(U^{(2)}*X_t+W^{(2)}*h_{t+1}^{(2)})\\end{aligned} \\tag{2}\\]循环神经网络的应用从上一节的讨论容易知道：CNN适合处理空间信息，RNN适合处理时序信息在《动手学深度学习》的“循环神经网络”章节作者列举了股票价格、时间曲线拟合、语言模型等应用一般来说RNN更多应用在自然语言处理领域，下面将以语言模型建模介绍RNN的使用语言模型概念语言模型就是给定一句话前面的部分，预测接下来最可能的词是什么语言模型可以用在语音转文本（STT）、图像到文本的识别（OCR）中，这类应用的输出往往是若干可能的候选词，需要语言模型从中选择一个最可能的考虑以下两个语言模型任务：  我昨天上学迟到了，老师批评了____。  我的手机坏了，我打算____一部新手机。第一个任务需要网络能够结合前面的所有信息，关联到第1个词“我”（否则网络填入“张三”也没什么不对）第二个任务需要网络能够结合前面和后面的信息，选择填入“买”（如果没有后面的“一部新手机”的信息，网络填入“哭一场”也是符合逻辑的）显然，RNN可以完成第一个任务，而第二个任务则需要BRNN语言模型数据处理《动手学深度学习》的“文本预处理”章节将语言模型数据的预处理分为读取数据集、词元化、生成词表读取数据集在NLP领域，数据集又称语料库，人类产生的任何文本都可以作为语料库供RNN学习参考文章选择了H.G.Well的《The Time Machine》小说，逐行将其加载入列表词元化对每行的语句，将其按字母或者按单词拆分为词元（token）['the', 'time', 'machine', 'by', 'h', 'g', 'wells'][][][][]['i'][][]['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him']['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']['twinkled', 'and', 'his', 'usually', 'pale', 'face', 'was', 'flushed', 'and', 'animated', 'the']可以看到，对英语文本可以按空格或字母方便的拆分，但对中文来说词元化的难度就大很多了《有哪些比较好的中文分词方案？》里对现有的中文分词方法做了总结：1）基于词典分词算法按照一定策略将待匹配的字符串和一个已建立好的“充分的”词典中的词进行匹配，若找到某个词条，则说明匹配成功常见的基于词典的分词算法有：正向最大匹配法、逆向最大匹配法、双向匹配分词法等2）基于统计的机器学习算法对汉字进行标注训练，不仅考虑词语出现的频率，还考虑上下文，具备较好的学习能力常见的基于统计的分词算法有：隐马尔科夫模型HMM、条件随机场CRF、深度学习等目前中文分词存在的难点有：1）分词标准比如对于人名，在哈工大的标准中姓和名是分开的，但在Hanlp中是合在一起的2）歧义又分为组合型歧义、交集型歧义和真歧义三种  组合型歧义：某词条的一部分也可以切分为一个独立词条，如“中华人民共和国”，粗粒度的分词就是“中华人民共和国”，细粒度的分词可能是“中华/人民/共和国”  交集型歧义：如“郑州天和服装厂”中，“天和”是厂名，但“和服”也是一个词，它们公用了“和”  真歧义：不同切分法的语义和语法都没有问题，即使人工切分也会产生歧义，只有通过上下文才能确定。如“美国会通过对台售武法案”，既可以切分成“美国/会/通过对台售武法案”，也可切分为“美/国会/通过对台售武法案”3）新词未被词典收录的词最后该答案还推荐了部分中文分词工具：NLPChina、哈工大HIT、清华THULAC、斯坦福分词器、Hanlp、结巴分词、kcws、zpar、IKAnalyzer生成词表也称向量化或者word embedding，是将词元化后的字符串转换成向量形式，方便计算机处理向量化的方式主要可以分为两种：1）One-hot representation假设词典一共包含$N$个词元，每个词元在词典里有一个唯一的编号$id\\in[1,N]$，那么one-hot向量长度就是$N$，令$v_j$是one-hot向量的第$j$个元素，则有\\[v_j=\\begin{cases}1&amp;id=j\\\\0&amp;else\\end{cases} \\tag{3}\\]one-hot的方法虽然简单，但有两个缺点：  每个词元的维度与词典容量一致，容易陷入维数灾难  无法描述词元与词元之间的相似性2）distributed representationdistributed representation认为词语之间的关联性也需要被考虑，如girl和woman都是指女性，只是年龄不同，word和words仅仅是复数和单数的区别，buy和bought都是买的意思，只是时态不同与one-hot相比，distributed有两点不同：  形式上One-hot是一种稀疏词向量，而distributed是一种固定长度的稠密词向量，一般形式如[0.792,-0.177,-0.107...]  功能上distributed使得相关或近似的词在距离上更接近了参考《YJango的Word Embedding–介绍》，下面介绍distributed的主要方法distributed的生成其实并没有明确的目标函数，可以认为是一个无监督学习任务，对这类问题，一般有两种方法：  从输入之间的关系找目标，如聚类  另外创建一个有目标函数的任务，其输入是当前问题的输入，如生成对抗网络稍微解释下第二种方法：比如当前问题是实现$y=f(x)$的映射拟合，另外创建的有监督任务为$z=g(y)$，使用$x$和$z$做有监督训练，得到$z=g(f(x))$，最后取$f:x\\rightarrow y$distributed更偏向第二种方法，其中2013年google提出的Word2Vec【1】是代表算法，该算法包含了Skip-Gram和CBOW两种模型图4. 左：CBOW；右：Skip-Gram如图4所示，Skip-Gram是以输入词元为中心词去预测周边（相似）词元；CBOW是以输入词元为周边词元去预测中心词语言模型的训练在完成词元化后一个用于训练的输入-标签例子为      输入    s    我    昨天    上学    迟到    了        标签    我    昨天    上学    迟到    了    e  其中s和e分别表示开始和结束，可以看到标签总是领先输入一个词元，比如对于输入“我”，希望网络的输出是“昨天”当然最后送入网络之前还要按上一节“生成词表”的方法完成词元的向量化对于网络的损失函数设计，就是衡量标签向量与预测向量的距离。一种常见方法是softmax + cross entroy，具体可参考《零基础入门深度学习(5) - 循环神经网络》RNN存在的问题RNN在训练过程中很容易发生梯度爆炸和梯度消失，导致梯度无法在长序列中传递从而不能捕捉长距离的影响此外图2所示的RNN将历史信息统一对待，时间越久权重越低，没有考虑信息本身的重要程度针对梯度爆炸的问题，可以通过设置梯度阈值来规避；对于梯度消失的问题，可以通过合理初始化权重、挑选合适的激活函数（比如relu）等方法来解决现代RNN中的长短时记忆网络（LTSM）和门控循环单元（GRU）对解决上述问题有比较好的效果文献【1】Mikolov T, Chen K, Corrado G, et al. Efficient estimation of word representations in vector space[J]. arXiv preprint arXiv:1301.3781, 2013."
  },
  
  {
    "title": "【图像分割】RefineNet算法",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-RefineNet%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, 图像分割",
    "tags": "图像分割",
    "date": "2022-12-12 00:00:00 +0000",
    





    
    "snippet": "RefineNet【1】发表于CVPR2017，在VOC2012数据集上获得83.4%的mIoU（同样发表于CVPR2017的PSPNet的mIoU为85.4%）RefineNet总的设计出发点为：尽量不损失任何输入信息  空洞卷积虽然可以在避免下采样的前提下增加感受野，但损失了部分输入信息，而且计算量也很大  为了在不使用空洞卷积仍然获得多尺度感受野，设计多路径的提炼网络个人认为Refin...",
    "content": "RefineNet【1】发表于CVPR2017，在VOC2012数据集上获得83.4%的mIoU（同样发表于CVPR2017的PSPNet的mIoU为85.4%）RefineNet总的设计出发点为：尽量不损失任何输入信息  空洞卷积虽然可以在避免下采样的前提下增加感受野，但损失了部分输入信息，而且计算量也很大  为了在不使用空洞卷积仍然获得多尺度感受野，设计多路径的提炼网络个人认为RefineNet的重点就在丰富每个尺度的上采样过程，希望能够最大化的利用到当前尺度下的信息作为对比，FCN、SegNet、UNet的decode模块只是简单的应用反卷积、反池化和上采样，而ENet和PSPNet甚至从理论上认为decode不重要，因此倾向设计厚encode薄decode的结构网络结构从整体上看，RefineNet使用ResNet做encode，一共输出4种尺度在decode中分别对4种尺度应用提出的RCU、CRP结构细化信息，然后用UpSample实现上采样图1. RefineNet网络结构RCU模块从图1b可以看到，RCU(residual convolution unit)是一个残差结构，基本元素是Conv2d和ReLU文章解释RCU是用来微调resnet的权重class RCUBlock(nn.Module):        def __init__(self, in_planes, out_planes, n_blocks, n_stages):        super(RCUBlock, self).__init__()        for i in range(n_blocks):            for j in range(n_stages):                setattr(self, '{}{}'.format(i+1, j+1),                        nn.Conv2d(in_planes if (i == 0) and (j == 0) else out_planes,                                out_planes, 3, stride=1, padding=1,                                 bias=(j == 0)))        self.stride = 1        self.n_blocks = n_blocks        self.n_stages = n_stages        def forward(self, x):        for i in range(self.n_blocks):            residual = x            for j in range(self.n_stages):                x = F.relu(x)                x = getattr(self, '{}{}'.format(i+1, j+1))(x)            x += residual        return xCRP模块CRP(chained residual pooling)如图1d所示，其本质上也是个残差结构，但CRP实现了将多个标准残差结构联结在一起，后续的消融实验证明了它的有效性。个人认为这是本文最大的创新点除了结构的复杂化，与RCU相比，CRP使用MaxPooling和Conv2d，文章认为CRP的主要功能是获取背景信息class CRPBlock(nn.Module):    def __init__(self, in_planes, out_planes, n_stages):        super(CRPBlock, self).__init__()        for i in range(n_stages):            setattr(self, '{}_{}'.format(i + 1, 'crp'),                    nn.Conv2d(in_planes if (i == 0) else out_planes,                            out_planes, 3, stride=1, padding=1,                            bias=False))        self.n_stages = n_stages        self.maxpool = nn.MaxPool2d(kernel_size=5, stride=1, padding=2)    def forward(self, x):        top = x        for i in range(self.n_stages):            top = self.maxpool(top)            top = getattr(self, '{}_{}'.format(i + 1, 'crp'))(top)            x = top + x        return xMulti-resolution fusion如图1c所示，该模块是将小尺度的特征图做上采样，与大尺度的特征图相加，实现多尺度信息融合# 小尺度的特征图经过crp和rcu后做上采样self.stage22 = nn.Sequential(    self._make_crp(512, 512, 4),    self._make_rcu(512, 512, 3, 2),    nn.Conv2d(512, 256, 3, padding=1, bias=False),    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))# 大尺度的特征图（从resnet出来）经过rcu做微调self.stage31 = nn.Sequential(    nn.Conv2d(1024, 256, 3, padding=1, bias=False),    self._make_rcu(256, 256, 2, 2),    nn.Conv2d(256, 256, 3, padding=1, bias=False))# 融合之后再次经过crp和rcu后做上采样，准备与更大尺度的特征图做融合self.stage32 = nn.Sequential(    self._make_crp(256, 256, 4),    self._make_rcu(256, 256, 3, 2),    nn.Conv2d(256, 256, 3, padding=1, bias=False),    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True))# forwardx4 = self.stage22(x4)   # 256, h/16, w/16x3 = self.stage31(l3)   # 256, h/16, w/16x3 = x3 + x4x3 = F.relu(x3)x3 = self.stage32(x3)   # 256, h/8, w/8RefineNet结构的级联此外文章还介绍了将上述结构做级联的方法，消融实验证明了其对精度有一定贡献但考虑到RefineNet本身已经很大了（基于ResNet-101达到了451M），个人认为这个点恐怕凑论文创新点的意义比实际应用的意义更大图2. RefineNet级联代码第三方基于pytorch实现的代码参考上面第三方代码，自己实现的代码总结RefineNet网络复杂，训练难度大。但从结果上看，精度没有明显提升个人认为目前综合性价比最高的网络是SegNet，此外对于移动设备，ENet是理想的解决方案      算法    发表时间    数据集    类别数    模型大小(MB)    训练epoch数    PA    mPA    mIoU    FWIoU        FCN    2015    voc2012    21    57    2800    0.9712    0.9452    0.899    0.9448        cityscapes    20    57    250    0.8811    0.7141    0.5671    0.8002        UNet    2015    voc2012    21    84    2060    0.9829    0.97    0.9356    0.9666        cityscapes    20    84    230    0.8888    0.6639    0.5473    0.8101        ENet    2016    voc2012    21    1.7    3000    0.9601    0.9347    0.863    0.9248        cityscapes    20    1.7    900    0.8874    0.7049    0.5591    0.8105        SegNet    2017    voc2012    21    113    1260    0.9886    0.9794    0.9539    0.9777        cityscapes    20    113    860    0.9012    0.7179    0.5996    0.8279        PSPNet    2017    voc2012    21    188    800    0.9874    0.9664    0.9457    0.9753        cityscapes    20    188    260    0.886    0.6817    0.559    0.8054        RefineNet    2017    voc2012    21    451    480    0.9909    0.9828    0.959    0.9822        cityscapes    20    451    410    0.8794    0.6409    0.5166    0.797  文献【1】Lin G, Milan A, Shen C, et al. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1925-1934."
  },
  
  {
    "title": "【图像分割】ENet算法",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-ENet%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, 图像分割",
    "tags": "图像分割",
    "date": "2022-11-29 00:00:00 +0000",
    





    
    "snippet": "ENet【1】发表于CVPR2016，文章致力于实现一个能在移动设备上实时运行的语义分割网络图1. ENet与SegNet的参数对比从图1可以看到，ENet以远小于SegNet参数量和计算量的模型获得了与后者相当甚至略优的精度网络结构总体来说可以把ENet分成Initial、Encode和Decode三部分图2. ENet网络结构Initial部分该部分将输入图像做一次下采样，同时将chan...",
    "content": "ENet【1】发表于CVPR2016，文章致力于实现一个能在移动设备上实时运行的语义分割网络图1. ENet与SegNet的参数对比从图1可以看到，ENet以远小于SegNet参数量和计算量的模型获得了与后者相当甚至略优的精度网络结构总体来说可以把ENet分成Initial、Encode和Decode三部分图2. ENet网络结构Initial部分该部分将输入图像做一次下采样，同时将channels从3提升至16文章认为视觉信息存在大量的空间冗余，而网络前面的层主要功能是提取特征，因此此时做下采样对精度的影响不大文章在这里做了个实验：对比下采样后输出channels为16和32的精度，发现相差很小。说明下采样并没有丢失多少关键信息具体实现上，Initial部分参考了InceptionV3：一个分支做stride=2的卷积，一个分支做stride=2的最大池化，最后将两个分支concat输出（图3a）图3. (a):Initial部分；(b):bottleneck模块InceptionV3认为特征图急剧衰减（只经过1层就骤降）会丢失大量信息，不利于训练，因此提出图3a的并行结构文章从另一个方面解释了使用并行结构的原因：一般来说通过卷积做降采样会相应提高输出的channels，如果按传统的方式将池化层接在卷积层后面会增加池化的计算量（因为池化是分别对每个channels做处理）文章的实验表明，如果采用卷积和池化并行的结构，能够获得10倍的加速Encode部分Encode部分最终将特征图缩小到输入的$\\frac{1}{8}$，channels从3提升至128Encode部分可以分成3个stage：  第1个stage做一次下采样，将channels从16提升至64  第2个stage做一次下采样，将channels从64提升至128，使用了空洞卷积和卷积因式分解  第3个stage不做下采样，保持channels为128，使用了空洞卷积和卷积因式分解### encoder# stage1self.stage1_downsample = DownsamplingBottleneck(16, 64, return_indices=True, dropout_prob=0.01, prelu=True)self.stage1_regular = nn.Sequential(    RegularBottleneck(64, padding=1, dropout_prob=0.01, prelu=True),    RegularBottleneck(64, padding=1, dropout_prob=0.01, prelu=True),    RegularBottleneck(64, padding=1, dropout_prob=0.01, prelu=True),    RegularBottleneck(64, padding=1, dropout_prob=0.01, prelu=True))# stage2self.stage2_downsample = DownsamplingBottleneck(64, 128, return_indices=True, dropout_prob=0.1, prelu=True)self.stage2_regular = nn.Sequential(    RegularBottleneck(128, padding=1, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=2, dilation=2, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=2, kernel_size=5, asymmetric=True, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=4, dilation=4, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=1, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=8, dilation=8, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=2, kernel_size=5, asymmetric=True, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=16, dilation=16, dropout_prob=0.1, prelu=True))# stage3self.stage3_regular = nn.Sequential(    RegularBottleneck(128, padding=1, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=2, dilation=2, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=2, kernel_size=5, asymmetric=True, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=4, dilation=4, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=1, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=8, dilation=8, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=2, kernel_size=5, asymmetric=True, dropout_prob=0.1, prelu=True),    RegularBottleneck(128, padding=16, dilation=16, dropout_prob=0.1, prelu=True))可以看到，每个stage由下采样模块DownsamplingBottleneck和常规卷积模块RegularBottleneck组成下采样模块和常规卷积模块都是基于图3b所示的bottleneck微调得到，可以看到，该结构依然借鉴了InceptionV3的并行思路下采样模块下采样最大的好处是增加感受野，但也存在2个不容忽视的问题：  丢失空间信息，如边缘形状  对图像分割任务来说，大量的下采样需要同样的上采样，这会增加模型大小和计算量1）为了尽量保证模型精度，ENet仅使用了3次下采样，同时在常规卷积模块应用空洞卷积提升感受野2）为了降低计算量和模型大小，ENet借鉴SegNet的方式，使用MaxPool2d+MaxUnpool2d稀疏采样的方式实现下采样和上采样具体来说，下采样模块将图3b中右边分支第一个$1\\times 1$的卷积换成了stride=2，kernel_size=2的普通卷积实现下采样此外左边分支的Padding是确保MaxPooling后的channels与右边分支一致常规卷积模块常规卷积模块将图3b中左边分支的MaxPooling和Padding全部去掉，直接与右边分支的输出相加，实际上这就变成ResNet残差结构了同时该模块将右边分支的conv做$n\\times 1+1\\times n$的因式分解（借鉴了InceptionV3）Decode部分Decode部分将特征图上采样到输入的尺寸，channels从128先降低到16，最后卷积到num_classesDecode部分可以分成2个stage+1个反卷积：  第1个stage做一次上采样，将channels从128降低至64  第2个stage做一次上采样，将channels从64降低至16  最后1个反卷积，实现一次上采样的同时，将channels从16调整到num_classes### decoder# stage1self.stage4_upsample = UpsamplingBottleneck(128, 64, dropout_prob=0.1, prelu=True)self.stage4_regular = nn.Sequential(    RegularBottleneck(64, padding=1, dropout_prob=0.1, prelu=True),    RegularBottleneck(64, padding=1, dropout_prob=0.1, prelu=True))# stage2self.stage5_upsample = UpsamplingBottleneck(64, 16, dropout_prob=0.1, prelu=True)self.stage5_regular = RegularBottleneck(16, padding=1, dropout_prob=0.1, prelu=True)self.transposed_conv = nn.ConvTranspose2d(16, num_classes, kernel_size=3, stride=2, padding=1, bias=False)与Encode相似，每个stage由下采样模块UpsamplingBottleneck和常规卷积模块RegularBottleneck组成可以看到与庞大的Encode相比，Decode显得很小（作为对比SegNet是对称的），文章认为Encode作为信息提取模块需要与传统的分类网络保持一致，但`Decode`的功能仅仅是上采样以及对细节做微调与之相似的是PSPNet的Decode部分也很简单其他细节1）所有的卷积都不用bias，在卷积后加BN因为cuDNN在计算带bias的卷积时是分开算的，因此会增加计算量和内存消耗2）所有bottleneck结构的右分支最后都用Dropout实现正则化文章指出目前用于语义分割的数据集规模都有限，因此使用正则化避免过拟合是必须的3）使用PReLU代替ReLU文章发现将网络前面层的大部分ReLU撤掉，精度反而有所提升，因此将所有激活函数全部换成PReLU来研究原因图4. 左：ReLU；右：PReLU  ReLU对于小于0的输入，输出是0  PReLU对于小于0的输入有一个可学习的斜率图5是文章对网络所有层中PReLU斜率参数的统计图5. PReLU在网络中的斜率分布文章发现网络前面层的斜率略微大于0（因此ReLU在这些层中表现不好）在bottleneck中斜率略微为负，作者认为这是因为相比ResNet，网络层数太少所以其实并没有形成恒等映射（恒等映射意味着斜率为1）在网络Decode层中斜率变成正，开始接近恒等映射了，作者认为这也进一步证明了上面`Decode`只是对网络做微调的理论总之，因为PReLU通过学习可以包含ReLU，因此在ENet中作为了默认激活函数总结现在来总结下ENet为了在保证精度的前提下缩小网络规模的一些手段：1）在Initial部分对输入的图像做下采样降低计算量，在实现上参考InceptionV3使用并行结构进一步实现了10倍的加速2）Encode和Decode的基本结构bottleneck依然是并行结构3）采用“厚”Encode+“薄”Decode的组合降低网络深度4）Encode中的下采样模块借鉴了SegNet的MaxPool2d+MaxUnpool2d稀疏采样方式以降低计算量5）Encode部分只做了2次下采样，相应的使用空洞卷积来保证感受野6）Decode部分借鉴了InceptionV3的卷积因式分解，降低计算量的同时提高网络非线性能力7）所有卷积不用bias，降低计算量和内存消耗8）使用Dropout做正则化避免过拟合9）使用PReLU代替ReLU提高模型精度代码第三方基于pytorch实现的代码参考上面第三方代码，自己实现的代码文献【1】Paszke A, Chaurasia A, Kim S, et al. Enet: A deep neural network architecture for real-time semantic segmentation[J]. arXiv preprint arXiv:1606.02147, 2016."
  },
  
  {
    "title": "【图像分割】PSPNet、ParseNet",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2-PSPNet-ParseNet/",
    "categories": "深度学习, 图像分割",
    "tags": "图像分割",
    "date": "2022-11-24 00:00:00 +0000",
    





    
    "snippet": "PSPNetPSPNet【1】发表于CVPR2017，是2016年ImageNet比赛中场景分析任务的冠军，以85.4%的mAP在VOC2012数据集排行榜上名列第10（作为对比，FCN的mAP为62.2%）PSPNet观察到图像分割中常出现关系错匹配、难分类别混淆、不显著类别漏检这三种误分类情况，提出金字塔结构，并引入空洞卷积以提高上下文信息处理能力此外在训练时PSPNet还设计了辅助监督...",
    "content": "PSPNetPSPNet【1】发表于CVPR2017，是2016年ImageNet比赛中场景分析任务的冠军，以85.4%的mAP在VOC2012数据集排行榜上名列第10（作为对比，FCN的mAP为62.2%）PSPNet观察到图像分割中常出现关系错匹配、难分类别混淆、不显著类别漏检这三种误分类情况，提出金字塔结构，并引入空洞卷积以提高上下文信息处理能力此外在训练时PSPNet还设计了辅助监督分支，实验表明能有效提高网络精度网络结构PSPNet将修改后的ResNet作为encoder，但decoder不像FCN、SegNet和UNet那样逐层上采样，而是一次性上采样到输入图像大小图1. PSPNet网络结构feature map 生成PSPNet使用空洞卷积（以进一步提高其感受野）修改的ResNet生成feature map具体来说是将ResNet中stage3和stage4中的普通卷积（对应文章《经典CNN设计演变的关键总结：从VGGNet到EfficientNet》图5中的conv4_x和conv5_x）换成空洞卷积self.layer1, self.layer2, self.layer3, self.layer4 = \\    resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4for n, m in self.layer3.named_modules():    if 'conv2' in n:        m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)    elif 'downsample.0' in n:        m.stride = (1, 1)for n, m in self.layer4.named_modules():    if 'conv2' in n:        m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)    elif 'downsample.0' in n:        m.stride = (1, 1)金字塔池化该模块使用4种不同尺寸的核同时对feature map做平均池化（文章对比了平均池化和最大池化，发现平均池化精度会高1个点左右），然后将4个输出上插值到feature map的尺寸文章选择的4种池化后的尺寸分别为$1\\times 1$、$2\\times 2$、$3\\times 3$、$6\\times 6$class PPM(nn.Module):    def __init__(self, in_dim, reduction_dim, bins):        # bins=[1,2,3,6]        super(PPM, self).__init__()        self.features = []        for bin in bins:            self.features.append(nn.Sequential(                nn.AdaptiveAvgPool2d(bin),                # 1x1 卷积是为了调整channels                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),                nn.BatchNorm2d(reduction_dim),                nn.ReLU(inplace=True)            ))        self.features = nn.ModuleList(self.features)    def forward(self, x):        x_size = x.size()        out = [x]        for f in self.features:            # 将4种输出都插值到输入数据的尺寸            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))        return torch.cat(out, 1)  注意到代码中的F.interpolate，其内部调用的就是torch.nn.Upsample（详细分析参考《深度学习中的上采样》）  插值后做concat时会额外加上原始输入（即feature map），这是模仿ResNet的残差结构个人觉得这个金字塔池化结构与Inception的多尺度卷积（图2）有点像，后者使用$1\\times 1$、$3\\times 3$和$5\\times 5$三种卷积提取上一层不同尺度的特征所以如果将这里的池化换成卷积会不会效果更好？毕竟池化没有可学习的参数辅助监督在训练过程中，PSPNet将ResNet中stage3出来的特征图直接上采样到原始输入尺寸，作为另一个输出计算误差文章介绍这个方法也是借鉴了ResNet的残差结构，认为这样可以帮助网络更好的收敛（文章的实验结果当辅助监督的误差权重取$\\alpha=0.4$时可以提升近1个点）图2. 加入辅助监督的对比实验def forward(self, x, y=None):    x_size = x.size()    assert (x_size[2]-1) % 8 == 0 and (x_size[3]-1) % 8 == 0    h = int((x_size[2] - 1) / 8 * self.zoom_factor + 1)    w = int((x_size[3] - 1) / 8 * self.zoom_factor + 1)    x = self.layer0(x)      # b, 128,h/4,w/4    x = self.layer1(x)      # b, 256,h/4,w/4    x = self.layer2(x)      # b, 512,h/8,w/8    x_tmp = self.layer3(x)  # b,1024,h/8,w/8    x = self.layer4(x_tmp)  # b,2048,h/8,w/8    if self.use_ppm:        # 金字塔        x = self.ppm(x)     # b,4096,h/8,w/8    x = self.cls(x)         # b,num_cls,h/8,w/8    if self.zoom_factor != 1:        x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=True)    if self.training:        # 使用stage3的输出做上采样，计算误差做辅助监督        aux = self.aux(x_tmp)     # b,num_cls,h/8,w/8        if self.zoom_factor != 1:            aux = F.interpolate(aux, size=(h, w), mode='bilinear', align_corners=True)        main_loss = self.criterion(x, y)        aux_loss = self.criterion(aux, y)        return x.max(1)[1], main_loss, aux_loss    else:        return x代码pytorch实现代码ParseNetParseNet【2】发表于ICLR2016，文章在FCN的基础上添加了全局上下文特征分支，结合L2归一化取得了与DeepLab-LargeFOV接近的精度图3. ParseNet全局特征提取、归一化、特征融合全局特征提取文章认为全局特征往往是语义分割的关键，而使用vgg的FCN虽然理论感受野是$404\\times 404$，但根据文章【3】，实际感受野只有理论值的1/4如图3所示，ParseNet对特征图额外做全局平均池化（这里我的理解是池化的输出是$1\\times 1$）全局特征融合文章介绍了全局特征和局部特征融合的2种方式：前融合、后融合：  前融合：将两种特征融合后，作为一个整体送入分类器做训练（图3的方式）  后融合：分别对全局特征和局部特征训练一个分类器，最后对两个分类器的输出做处理文章认为在某些情况下，如果局部特征需要结合全局特征才能做出正确分类时，后融合的方式就会存在不可逆转的信息损失特征归一化文章发现（图4）不同层的特征尺度是有显著差异的，如果直接融合显然会有bias图4. 蓝色：conv4_3层特征；青色：conv5_3层特征；红色：fc7层特征；绿色：pool6层特征因此分别对局部特征层和全局特征层做一次$L2$归一化，考虑到梯度尺度的因素，文章对归一化后的特征乘以一个尺度因子\\[\\begin{aligned}y_i=&amp;\\gamma_i\\hat{x}_i\\\\=&amp;\\gamma_i\\frac{x_i}{||x_i||_2}\\end{aligned} \\tag{1}\\]文献【1】Zhao H, Shi J, Qi X, et al. Pyramid scene parsing network[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2881-2890.【2】Liu W, Rabinovich A, Berg A C. Parsenet: Looking wider to see better[J]. arXiv preprint arXiv:1506.04579, 2015.【3】Zhou B, Khosla A, Lapedriza A, et al. Object detectors emerge in deep scene cnns[J]. arXiv preprint arXiv:1412.6856, 2014."
  },
  
  {
    "title": "经典CNN设计演变的关键总结：从VGGNet到EfficientNet",
    "url": "/posts/%E7%BB%8F%E5%85%B8CNN%E8%AE%BE%E8%AE%A1%E6%BC%94%E5%8F%98%E7%9A%84%E5%85%B3%E9%94%AE%E6%80%BB%E7%BB%93-%E4%BB%8EVGGNet%E5%88%B0EfficientNet/",
    "categories": "深度学习",
    "tags": "",
    "date": "2022-11-17 00:00:00 +0000",
    





    
    "snippet": "本文重点参考文章《经典CNN设计演变的关键总结：从VGGNet到EfficientNet》      网络    时间    主要贡献        VGGNet    2014    采用多个连续小卷积核代替1个大卷积核，参数降低的同时效果更好        Inception    2014    VGGNet及其之前的网络都在考虑增加网络深度，Inception开始研究增加网络广度  ...",
    "content": "本文重点参考文章《经典CNN设计演变的关键总结：从VGGNet到EfficientNet》      网络    时间    主要贡献        VGGNet    2014    采用多个连续小卷积核代替1个大卷积核，参数降低的同时效果更好        Inception    2014    VGGNet及其之前的网络都在考虑增加网络深度，Inception开始研究增加网络广度        ResNet    2015    是引用最高的论文之一，第一个成功堆叠超过100层的CNN，残差结构的引入极大改善了深层网络性能退化的问题        MobileNet    2017    能够运行在移动设备上的轻量级网络，提出了深度可分离结构        EfficientNet    2019    同时考虑网络深度、广度、分辨率，通过暴力搜索的方式找出最佳的参数  VGGNetVGG【1】是Oxford的Visual Geometry Group的组提出的相比AlexNet，其主要改动是采用连续的多个$3\\times 3$卷积取代较大的单个卷积核具体来说，采用2个$3\\times 3$卷积核取代1个$5\\times 5$卷积核，用3个$3\\times 3$卷积核取代1个$7\\times 7$卷积核很容易理解，2个$3\\times 3$卷积与1个$5\\times 5$卷积的感受野都是5，但参数量分别是$3\\times 3\\times 2\\times C_{in} \\times C_{out}$和$5\\times 5\\times C_{in} \\times C_{out}$，显然前者参数量更小通过以上分析可以看到：多个小尺寸卷积取代1个大尺寸卷积，不仅能保持相同感受野，还能降低参数量，此外由于非线性层的增加，网络能学习更复杂的模式图1. VGGNet网络结构VGGNet的结构很简单：连续$3\\times 3$卷积+最大池化做下采样，其中每次下采样都会double通道数VGGNet16（图1D）表示包含13个卷积层和3个全连接，相似的，VGGNet19（图1E）包含16个卷积层和3个全连接Inception  Inception之前的网络都在通过增加网络深度提高效果，但面临着参数多、过拟合、难训练的问题  Inception尝试增加网络宽度，通过在同一层中加入不同尺寸的卷积核提取不同尺度的特征  GoogLeNet就是使用Inception模块组成的图2. Inception层结构如图2a所示，分别使用$1\\times 1$、$3\\times 3$、$5\\times 5$的卷积和一个池化提取上一层不同尺度的特征，然后将其concat按照图2a的方式虽然可以提升网络性能，但随着深度的增加，网络参数也是很客观的，因此Inception使用$1\\times 1$卷积做改进（图2b）假设上一层输出为$100\\times 100\\times 128$，本层输出channels为256，对于$5\\times 5$的卷积，其参数量为$5\\times 5\\times 128\\times 256=819200$，如果先使用$1\\times 1$输出channels为32的卷积，再使用$5\\times 5$输出channels为256的卷积，其参数量为$1\\times 1\\times 128\\times 32+5\\times 5\\times 32\\times 256=208896$，是前者的1/4注意到这里面的一个关键：$1\\times 1$的卷积输出的channels必须要小于上层输出channels和本层输出channels的最小值此外YoloV4（《【CNN系列目标检测】（9）YOLO4算法》）中使用的transition layer就是这种处理方式  InceptionV2【3】在Inception的基础上添加了BN层，仿照VGGNet的方式将$5\\times 5$换成$3\\times 3$的连续卷积  InceptionV3【4】进一步将$3\\times 3$因式分解成$1\\times 3$和$3\\times 1$；此外文章认为相比图3b，特征图急剧衰减（只经过1层就骤降，图3a）会丢失大量信息，不利于训练，因此提出图3c所示的并行结构  InceptionV4【5】结合ResNet残差结构将网络做的更深图3. InceptionV3层结构ResNet当网络达到一定深度后，继续加深网络会导致训练误差和测试误差都变高，ResNet认为这种退化问题是因为更深的网络会导致梯度弥散和梯度爆炸，从而阻碍网络的收敛BN可以在一定程度上缓解网络退化，但依然不足以满足要求ResNet【6】解决该问题的方法是恒等映射结构图4. ResNet恒等映射结构假设对于输入$x$，希望网络输出$H(x)$，图4将上述关系修改为$H(x)=F(x)+x$，这样网络学习的就是残差$F(x)=H(x)-x$（$x$的微小波动），文章认为学习残差比直接学习$H(x)$要简单文章《ResNet及其变种的结构梳理、有效性分析与代码解读》总结了残差有效性的几种说法      假说    解释        集成学习    ResNet存在多条路径，类似于集成学习，删除部分节点并不影响网络性能        梯度传播    恒等映射的存在使得深层的梯度有路径可以直接传播到浅层，有效缓解了梯度消失的问题        差分放大    残差结构可以放大输入中微小的扰动，因此更加灵敏        自适应深度    传统的卷积很难模拟恒等，ResNet使得网络可以自己调节网络深度，不需要的层就更偏向选择恒等映射  ResNet网络结构ResNet主要包含5种结构（图5），每种结构都可分为输入部分、中间卷积部分、输出部分，不同的结构区别在于中间卷积部分卷积层数+输入卷积层+输出全连接层就是每种结构的网络深度（网络可学习的层数）图5. ResNet五种主要结构对所有5种结构，中间卷积部分都可以分为4个stage，每个stage包含若干卷积层，下一个stage特征图尺寸是上一个stage的一半，channels则翻倍可以看到，从ResNet50开始，stage内的卷积模块包含了$1\\times 1$的卷积，如Incetpion所述，这个技巧可以降低网络参数，提高网络非线性表达能力MobileNetMobileNet【7】的目标是模型轻量化，其最主要的工作是将VGGNet的标准卷积换成深度可分离卷积深度可分离卷积可分离卷积是为降低计算量而设计，包含空间可分离卷积和深度可分离卷积参考《可分离卷积及深度可分离卷积详解》，空间可分离卷积就是将$n\\times n$分解为$1\\times n$和$n\\times 1$（InceptionV3）深度可分离卷积将一次完整的卷积分解为逐深度卷积+逐点卷积图6. 上：普通卷积；下：深度可分离卷积如图6上所示，输入为$64 \\times 64\\times 3$，使用4个$3\\times 3$的常规卷积，总参数量是$3\\times 3\\times 3\\times 4=108$深度可分离卷积用与输入channel数量一致的$3\\times 3$卷积分别对每个channel做卷积（图6下左），然后用4个$1\\times 1$的普通卷积混合所有channel的信息（图6下右），总参数量是$3\\times 3\\times 3+3\\times 4=39$  注意深度可分离卷积与InceptionV1中的$1\\times 1$卷积技巧的区别：Inception中都是普通卷积，只是通过降低$1\\times 1$卷积的数量达到减少参数量的目的  此外分组卷积与深度可分离卷积有类似的地方：前者会将输入的channel分组，处于一组的使用同一个卷积，因此如果分组数与channel一致，分组卷积就是深度可分离卷积的前半部分（图6下左）torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)'''groups=1: 普通卷积groups&lt;in_channels: 分组卷积groups=in_channels: 深度可分离卷积'''MobileNet结构图7. 左：普通卷积；右：MobileNet使用深度可分离卷积MobileNetV2【8】发现使用了深度可分离卷积训练的卷积很多是空的，文章认为是因为ReLU在低维数据中容易丢失信息，因此提出了InvertedResidual单元图8. InvertedResidual单元1）首先用$1\\times 1$的普通卷积将数据映射到高维（也就是提高channels），然后做$3\\times 3$的深度可分离卷积，最后用$1\\times 1$的普通卷积将数据映射回低维2）InvertedResidual单元使用了ResNet的残差连接3）InvertedResidual中使用的激活函数为ReLU6（最大输出为6），是为了在移动端使用float16的低精度时也有好的数值分辨率：ReLU的输出范围是0到正无穷，如果激活值很大可能会超出float16的数值范围，带来精度损失4）之所以叫InvertedResidual，是因为ResNet中的Residual是先压缩维度，再放大维度的结构，正好与之相反图9. MobileNet网络结构EfficientNetEfficientNet【9】发现单独提高网络深度、广度或者输入数据的分辨率，网络精度并不会一直有正向收益。比如ResNet-1000和ResNet-100的精度其实差不多因此文章使用NAS(神经结构搜索)方法针对base网络结构搜索上述3者最佳的搭配参数\\[\\begin{aligned}depth:&amp;d=\\alpha^\\phi\\\\width:&amp;w=\\beta^\\phi\\\\resolution:&amp;r=\\gamma^\\phi\\end{aligned} \\tag{1}\\]1）首先固定$\\phi=1$，以EfficientNetB-0为基准网络搜索，发现其最佳参数为$\\alpha=1.2$、$\\beta=1.1$、$\\gamma=1.15$2）然后以上一步得到的3个参数为基础，在EfficientNetB-0上应用不同的$\\phi$，得到EfficientNetB-1到EfficientNetB-7图10. Efficient的精度与计算量对比总的来说，Efficient说明了对于任一个网络，使用搜索的方法精心搭配深度、广度和分辨率是可能进一步提高其效果的文献【1】Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.【2】Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.【3】Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[C]//International conference on machine learning. PMLR, 2015: 448-456.【4】Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2818-2826.【5】Szegedy C, Ioffe S, Vanhoucke V, et al. Inception-v4, inception-resnet and the impact of residual connections on learning[C]//Thirty-first AAAI conference on artificial intelligence. 2017.【6】He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 770-778.【7】Howard A G, Zhu M, Chen B, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J]. arXiv preprint arXiv:1704.04861, 2017.【8】Sandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted residuals and linear bottlenecks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 4510-4520.【9】Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]//International conference on machine learning. PMLR, 2019: 6105-6114."
  },
  
  {
    "title": "从FCN、SegNet、UNet谈图像分割的Tricks",
    "url": "/posts/%E4%BB%8EFCN-SegNet-UNet%E8%B0%88%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E7%9A%84Tricks/",
    "categories": "深度学习",
    "tags": "",
    "date": "2022-11-09 00:00:00 +0000",
    





    
    "snippet": "首先明确下图像分割中的几个概念，内容参考自《Image Segmentation》      术语    英文名称    说明        超像素    superpixels    是一类像素的集合，集合内的像素空间邻近，特征相似        语义分割    Semantic Segmentation    对图像中每个像素做分类，仅区分类别不区分个体（如图像中有2个人，都会被赋予同一...",
    "content": "首先明确下图像分割中的几个概念，内容参考自《Image Segmentation》      术语    英文名称    说明        超像素    superpixels    是一类像素的集合，集合内的像素空间邻近，特征相似        语义分割    Semantic Segmentation    对图像中每个像素做分类，仅区分类别不区分个体（如图像中有2个人，都会被赋予同一个label）        实例分割    Instance Segmentation    仅对某个类别的目标输出mask，区分该类别的个体        全景分割    Panoptic Segmentation    是语义分割和实例分割的结合，对每个像素做分类，区分类别并且区分个体  图1. tl:超像素, tr:语义分割, bl:实例分割, br:全景分割FCN、SegNet和UNet都属于语义分割的范畴网络结构      与图片分类相比，语义分割网络一般用卷积代替全连接，理论上网络可以输入任意尺寸的图像        FCN【1】(2015)、SegNet【2】(2017)和UNet【3】(2015)总体来说都可视为Encoder-Decoder结构，其中Encoder做下采样，Decoder做上采样，最终输出原始图像大小的heatmap        FCN、SegNet和UNet下采样都是通过最大池化MaxPool2d实现，最大的不同在于上采样的方法  图2. SegNet的Encoder-Decoder结构      算法    下采样方法    上采样方法        FCN    Conv + MaxPool2d    反卷积ConvTranspose2d        SegNet    Conv + MaxPool2d    反最大池化MaxUnPool2d        UNet    MaxPool2d + Conv    双线性插值Upsample  关于反卷积、反最大池化和双线性插值的详细分析参考文章《深度学习中的上采样》下采样FCN、SegNet和UNet的下采样都是使用卷积+最大池化的组合，但在具体实现上略有差异：  FCN和SegNet都是先卷积，再池化。但SegNet的池化会同时返回最大值的index，这样是为了在上采样中方便使用MaxUnPool2d  FCN第1层下采样pad=100是为了确保尺寸小于192的图片在最后一层下采样还有输出（《图像语义分割（1）- FCN》）  UNet是先池化再卷积，这是为了方便在上采样中使用Upsample  为了降低因为下采样造成的信息损失，每次下2采样时channels都会增加1倍# fcnself.conv1 = nn.Sequential(    BaseConv(3, 64, 3, pad=100, act='relu'),    BaseConv(64, 64, 3, act='relu'),    nn.MaxPool2d(2, stride=2, ceil_mode=True))self.conv2 = nn.Sequential(    BaseConv(64, 128, 3, act='relu'),    BaseConv(128, 128, 3, act='relu'),    nn.MaxPool2d(2, stride=2, ceil_mode=True))# segnetself.stage1 = nn.Sequential(    BaseConv(3, 64, 3, bn=True, act='relu'),    BaseConv(64, 64, 3, bn=True, act='relu'),    nn.MaxPool2d(2, stride=2, return_indices=True))# unetself.down1 = nn.Sequential(    nn.MaxPool2d(2, stride=2),    BaseConv(64, 128, 3, bn=True, act='relu'),    BaseConv(128, 128, 3, bn=True, act='relu'))上采样上采样中三个算法用的方式都不一样# fcnself.upscore2 = nn.ConvTranspose2d(    num_classes, num_classes, 4, stride=2, bias=False)# segnetself.uppool5 = nn.MaxUnpool2d(2, stride=2)self.stage5u = nn.Sequential(    BaseConv(512, 512, 3, bn=True, act='relu'),    BaseConv(512, 512, 3, bn=True, act='relu'),    BaseConv(512, 512, 3, bn=True, act='relu'))# unetself.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)self.conv = nn.Sequential(    BaseConv(in_channels, in_channels//2, 3, bn=True, act='relu'),    BaseConv(in_channels//2, out_channels, 3, bn=True, act='relu'))下采样和上采样的连接  FCN是将下一层做上采样+卷积使之与上一层的尺寸和channels一致，然后与上一层相加  SegNet是将下一层与上一层池化输出的index组合做上采样+卷积  UNet是将下一层做上采样+卷积使之与上一层的尺寸一致，然后与上一层在channel维度做concate可以看到，FCN和UNet在每次上采样时都结合了不同尺度的信息，但SegNet似乎没有考虑这点此外个人认为UNet对维度做concat的方式比FCN做加法的方式更合理下采样和上采样尺度不一致的问题虽然无全连接的网络结构理论上可以接受任意尺寸的输入，但由于下采样的舍入误差，导致上采样后的尺寸可能与上一层不一致上述网络通常使用直接截取的方法确保相加或concate的顺利进行# unetx1 = self.up(x1)diffY = x2.size()[2] - x1.size()[2]diffX = x2.size()[3] - x1.size()[3]x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,                diffY // 2, diffY - diffY // 2])x = torch.cat([x2, x1], dim=1)但即使这样处理，在给网络喂数据的时候还是会遇到一个batch里图像尺寸不一致的情况（我猜这大概是大部分代码都默认batch_size=1的原因）因此我的解决方案是：1）组织训练数据的时候首先将每个输入（一个输入包含1张图片和1张mask）paste到一个大的画布上，组成尺寸一致的batch，同时记录每个输入在画布上的bbox2）在输入网络前分析上一步batch中的所有bbox，得到最小的$H、W$，然后根据网络下采样层数$n$确定小于$H,W$的最终尺寸\\[\\begin{aligned}H'&lt;=H\\  and \\ H'=k*2^n\\\\W'&lt;=W\\ and \\ W'=p*2^n\\end{aligned} \\tag{1}\\]其中$k$和$p$是大于1的整数batch中的所有输入都根据$H’$和$W’$随机crop到该最小尺寸。这样一方面确保输入网络的batch数据尺寸统一且均为有效数据，另一方面也避免了下采样无法整除造成的舍入误差，同时也包含了数据增强的成分3）在推理时，因为要确保输出与原始图像尺寸一致，根据式2将输入pad到调整后的尺寸，最后对网络输出做crop就可以了\\[\\begin{aligned}H'&gt;=H\\  and \\ H'=k*2^n\\\\W'&gt;=W\\ and \\ W'=p*2^n\\end{aligned} \\tag{2}\\]误差loss交叉熵语义分割本质上是一个分类问题，因此一个直接的思路就是交叉熵，事实上这也是大部分代码的实现方案criterion = nn.CrossEntropyLoss(ignore_index=config.dataset.ignore_label)loss = criterion(outputs, resized_labels)加权交叉熵、focal loss、dice loss由于大部分数据（如VOC）类别的不均衡很明显，因此直接使用交叉熵会导致模型偏向优势类（如背景、人）1）加权交叉熵对每个类别计算的交叉熵损失乘以权重，显然优势类的权重应该小，劣势类的权重应该大# pytorch的CrossEntropyLoss支持weight输入torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean', label_smoothing=0.0)# cross_entroy等价于log_softmax+nll_losstorch.nn.NLLLoss(weight=None, size_average=None, ignore_index=- 100, reduce=None, reduction='mean')为了获得权重，我的解决方案是：1）遍历一遍数据集，对每张图片每个类别计算像素个数与图片像素个数之比，对所有图片累加该比值，最终得到数据集中每个类别出现的概率2）找到出现概率最大的类别，用该类别的概率除以其余所有类别，最后该类别概率置1，得到所有类别的权重# VOC计算得到的各类别权重voc_cls_rate = (1, 148.49, 380.5, 128.24, 189.84, 157.48, 64.21, 80.76, 45.38, 113.62, 109.45, 83.84,                 61.48, 114.93, 103.47, 22.47, 198.56, 135.17, 78.38, 74.36, 130.83)自适应权重实验发现上述基于统计的权重方法效果还是一般，尝试在训练后接评估，根据评估的每个类的精度动态确定下一次训练的误差权重实验发现效果很可观，并且lr也可以调大（0.01）2）focal loss参考《【CNN系列目标检测】（11）CornerNet算法》，focal loss本质上是降低易分类（正样本预测概率高，负样本预测概率低）样本的权重，增加难分类样本的权重这里只需要分别对每个类计算focal loss，再计算所有类的均值即可3）dice lossdice loss出自2016年的文章【4】，也是为了处理语义分割中正负样本不均衡的场景。文章《语义分割之dice loss深度分析（梯度可视化）》对此有详细分析dice loss的计算思路很简单，如式3所示\\[L_{dice}=1-\\frac{2|X\\cap Y|}{|X|\\cup |Y|} \\tag{3}\\]其中X是预测为类别$i$的区域，Y是GT中为类别$i$的区域  可以看到，dice loss只关注前景区域（假设同样大小的某类别区域，将其放在不同尺寸的背景上式3计算的结果一样），因此能处理正负样本不均衡的情况  对于小目标，dice loss的训练可能不稳定，假设一张图片上只有1个像素为类别$i$，其余都是背景，预测错误时loss为1，预测正确是loss为0因此一般使用cross_entropy+dice_loss 或 focal_loss+dice_loss的组合方式条件随机场CRF从上一节可以看到，目前为止语义分割网络仅对每个像素做单独预测，但没有考虑像素和像素之间的关联：如位置上接近的像素，或者颜色纹理上接近的像素，其属于同一类别的概率更高条件随机场CRF就是用来处理这件事情的（关于马尔科夫链、平均场和条件随机场的数学概念看不太明白，暂时先略过）CRF作为语义分割后处理CRF首先是作为后处理模块在文献【5】中提出的：\\[E(x)=\\sum_i{\\psi_u(x_i)}+\\sum_{i&lt;j}{\\psi_p(x_i,x_j)} \\tag{4}\\]其中$\\psi_u(x_i)$是一元势函数，就是语义分割网络对像素$x_i$的预测概率$\\psi_p(x_i,x_j)$是二元势函数，就是CRF，衡量像素$x_i$和$x_j$的关联性\\[\\begin{aligned}\\psi_p(x_i,x_j)=&amp;\\mu(x_i,x_j)k(f_i,f_j)\\\\=&amp;\\mu(x_i,x_j)\\sum_{m=1}^K{w^{(m)}k^{(m)}(f_i,f_j)}\\end{aligned} \\tag{5}\\]式5中$\\mu(x_i,x_j)$是类别兼容性函数，当预测$x_i$与$x_j$不属于同一个类别时为1，否则为0（CRF目标是将原本预测不同类的像素优化为同一类）$k(f_i,f_j)$计算像素$i$和$j$的特征向量的差异，其中$K$就是特征（空间距离、颜色等）个数，$w^{(m)}$是第$m$个特征的权重，$k^{(m)}(f_i,f_j)$是对第$m$个特征使用高斯核计算特征向量差异通常我们会选择空间距离特征和颜色特征进行计算，式5就可写为\\[k(f_i,f_j)=w^{(1)}exp\\left (-\\frac{|p_i-p_j|^2}{2\\theta_{\\alpha}^2}-\\frac{|I_i-I_j|^2}{2\\theta_{\\beta}^2}\\right )+w^{(2)}exp\\left (-\\frac{|p_i-p_j|^2}{2\\theta_{\\gamma}^2}\\right ) \\tag{6}\\]式6中$p_i$和$I_i$分别代表像素$i$的坐标和颜色向量，上式相当于用坐标和颜色组合成一个特征，又单独视坐标为一个特征，其效果是CRF更关注像素距离特征（文章【5】提到这样可以消除孤立的小区域）将CRF集合进语义分割网络  CRF作为语义分割后处理可以有效优化分割结果，但其不参与训练，不能对语义分割网络的优化带来正面影响  此外正如目标检测中two-stages到one-stage的演变一样，我们总是希望能够end2end解决问题文章CRFasRNN【6】实现了将式5用CNN的方式表达图3. CRF as CNN如图3所示，文章将其分为以下几个步骤：1）初始化对分割网络出来的预测概率（对应式5中的一元势函数）做归一化该步骤可用softmax实现2）消息传递根据像素特征向量确定初始化$m$个高斯核参数($m$为特征数，每个高斯核尺寸与图像尺寸一致)使用初始化的高斯核与上一步归一化的概率做卷积（滤波），对应式5中的$k^{(m)}(f_i,f_j)$为了提高卷积效率，文章使用了Permutohedral lattice加速方法3）滤波器加权和对上一步每种特征的滤波结果做加权和，对应式5的$\\sum_{m=1}^K{w^{(m)}k^{(m)}(f_i,f_j)}$这一步可用kernel=1，channels=m的核与滤波结果做卷积，输出channels=14）兼容性转换$\\mu(x_i,x_j)$是为了惩罚相似像素却被预测为不同类别的情况这里可用kernel=1，channels=L的核与上一步卷积结果做卷积，输出channels=L，其中L为类别数文章提到，当CRF可训练时$\\mu$的参数就可以适应性变化了，比如相邻像素分别被预测为单车和人的惩罚应该小于分别被预测为天空和单车的情况，因为前者从逻辑上更可能出现5）与一元势函数叠加对应式4最后，文章对上述5个步骤循环（文章发现循环次数为5时效果比较好，大于5次对结果没有显著提升）  CRF相当于现有模型的扩展，理论上可以被包装进任意语义分割模型中  作者在github上开源了代码，由于使用了Permutohedral lattice滤波的原因，代码仅支持CPU，且batch=1  在推理阶段也应该使用CRF包装后的模型，此时CRF相当于是一个后处理训练相比较目标检测，语义分割网络的训练困难很多loss选择很重要，但与之相比学习率更是关键，需要确保学习率比较低（可以尝试从1e-5开始测试），但如果太低又会导致精度没提升，因此要通过多组实验确定最佳值图4. tl:原图,tr:GT,bl:fcn,br:fcn+crf如图4所示，使用FCN-vgg网络(使用vgg16预训练权重)，在VOC上训练1200epoch，学习率一直保持1e-5，batch_size=16，测试结果为bl基于bl的权重，加入CRF训练4epoch测试的结果为br，可以看到CRF能够去除一些孤立的错误预测，目标的轮廓也更准确文献【1】Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.【2】Badrinarayanan V, Kendall A, Cipolla R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation[J]. IEEE transactions on pattern analysis and machine intelligence, 2017, 39(12): 2481-2495.【3】Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]//International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015: 234-241.【4】Milletari F, Navab N, Ahmadi S A. V-net: Fully convolutional neural networks for volumetric medical image segmentation[C]//2016 fourth international conference on 3D vision (3DV). IEEE, 2016: 565-571.【5】Krähenbühl P, Koltun V. Efficient inference in fully connected crfs with gaussian edge potentials[J]. Advances in neural information processing systems, 2011, 24.【6】Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1529-1537."
  },
  
  {
    "title": "深度学习中的上采样",
    "url": "/posts/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%8A%E9%87%87%E6%A0%B7/",
    "categories": "深度学习",
    "tags": "",
    "date": "2022-10-28 00:00:00 +0000",
    





    
    "snippet": "上采样是语义分割等密集输出任务的必备组件在PyTorch中Upsample、MaxUnpool2d，ConvTranspose2d均能实现上采样效果Upsample# 函数接口torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=No...",
    "content": "上采样是语义分割等密集输出任务的必备组件在PyTorch中Upsample、MaxUnpool2d，ConvTranspose2d均能实现上采样效果Upsample# 函数接口torch.nn.Upsample(size=None, scale_factor=None, mode='nearest', align_corners=None, recompute_scale_factor=None)该方法就是直接对输入做插值，类似于OpenCV的resize，因此没有训练参数该方法有两个选项需要关注：插值方法mode，角点是否对齐align_corners双线性插值参考文章《OpenCV ——双线性插值（Bilinear interpolation）》双线性插值比最近邻插值、线性插值效果好，计算量又小于三次插值等高阶插值方法，是工程中优选的方案简单来说，双线性插值首先根据插值后的坐标计算插值前的虚拟坐标（浮点数），然后根据该坐标找到其相邻的4个参考坐标（整数），同时根据虚拟坐标与4个参考坐标的位置关系计算权重，最后将4个参考坐标对应的像素值做加权和得到插值后坐标的像素值令原图$I_s$尺寸为$(W_s, H_s)$，插值后$I_d$的尺寸为$(W_d, H_d)$1）对于插值后的坐标$(x_d, y_d)$，其在原图的虚拟坐标$(x_p,y_p)$为\\[\\begin{aligned}x_p=\\frac{W_s}{W_d}x_d\\\\y_p=\\frac{H_s}{H_d}y_d\\end{aligned} \\tag{1}\\]2）根据虚拟坐标，找到其相邻的4个参考坐标$(x_c^1,y_c^1), (x_c^2,y_c^2), (x_c^3,y_c^3), (x_c^4,y_c^4)$\\[\\begin{aligned}x_c^1&amp;=\\lfloor x_p\\rfloor,&amp;y_c^1=\\lfloor y_p\\rfloor\\\\x_c^2&amp;=min(\\lfloor x_p\\rfloor+1, W_s-1), &amp;y_c^2=\\lfloor y_p\\rfloor\\\\x_c^3&amp;=\\lfloor x_p\\rfloor,&amp;y_c^3=min(\\lfloor y_p\\rfloor+1, H_s-1)\\\\x_c^4&amp;=min((\\lfloor x_p\\rfloor+1), W_s-1), &amp;y_c^4=min(\\lfloor y_p\\rfloor+1, H_s-1)\\end{aligned} \\tag{2}\\]3）计算每个参考点的权重双线性插值首先沿X轴做2次插值，得到2个中间值，然后沿Y轴对这2个中间值做1次插值得到$(x_d,y_d)$的取值在本例中首先根据$(x_c^1,y_c^1), (x_c^2,y_c^2)$得到1个中间点，根据$(x_c^3,y_c^3), (x_c^4,y_c^4)$得到另1个中间点，然后根据这两个中间点得到最终的值\\[\\begin{aligned}w_x^1=x_c^2-x_p\\\\w_x^2=x_p-x_c^1\\\\w_y^1=y_c^3-y_p\\\\w_y^2=y_p-y_c^1\\end{aligned} \\tag{3}\\]4）计算插值后坐标对应的值如上所述，首先计算X轴方向2个中间值$v_x^1$、$v_x^2$\\[\\begin{aligned}v_x^1=w_x^1\\times I_s(x_c^1,y_c^1)+w_x^2\\times I_s(x_c^2,y_c^2)\\\\v_x^2=w_x^1\\times I_s(x_c^3,y_c^3)+w_x^2\\times I_s(x_c^4,y_c^4)\\end{aligned} \\tag{4}\\]然后根据这2个中间值沿Y轴方向计算最终值\\[I_d(x_d,y_d)=w_y^1\\times v_x^1+w_y^2\\times v_x^2 \\tag{5}\\]角点对齐现在我们考虑从$(3\\times 3)$插值到$(5\\times 5)$，插值前的中心点为$(1,1)$，插值后的中心点为$(3,3)$按照式(1)，插值后中心点在原图的虚拟坐标为$(\\frac{3}{5}3,\\frac{3}{5}3)\\rightarrow (1.2,1.2)$这意味着插值后的中心点取值是以原图$(1.2,1.2)$为中心找的4个参考点计算的，但显然原图$(1,1)$才是理想的虚拟点为了满足上述对应关系，可以改写式(1)\\[\\begin{aligned}x_p=\\frac{W_s}{W_d}(x_d+0.5)-0.5\\\\y_p=\\frac{H_s}{H_d}(y_d+0.5)-0.5\\end{aligned} \\tag{6}\\]图1. 角点对齐图1源自PyTorch论坛，其中align_corners=True对应的是式(1)，align_corners=False对应的是式(6)align_corners其实描述的是如何看待像素：  align_corners=False：将像素视为$1\\times 1$的区域，对齐的是区域边缘  align_corners=True： 将像素视为区域的角点，对齐的是角点补充：1）OpenCV的双线性插值函数默认使用align_corners=False的方式2）PyTorch中的Upsample，当选择align_corners=False时，图像边缘值的插值方式不是严格的双线性算法，细节可参考《一文看懂align_corners》3）一般认为align_corners=False虽然方便了坐标值计算，但会影响边缘分割的精度（暂时不理解）MaxUnpool2d# 函数接口torch.nn.MaxUnpool2d(kernel_size, stride=None, padding=0)与MaxPool2d配合使用  最大池化操作通过选取kernel内最大值实现下采样，该函数同时可输出最大值的位置  MaxUpool2d同时接受下采样后的tensor和记录最大值位置的tensor，将输入还原到最大值的位置上，其余位置补0，这样可以最大限度还原信息图2. MaxPool2d+MaxUpool2d# PyTorch官方示例pool = nn.MaxPool2d(2, stride=2, return_indices=True)unpool = nn.MaxUnpool2d(2, stride=2)input = torch.tensor([[[[ 1.,  2.,  3.,  4.],                            [ 5.,  6.,  7.,  8.],                            [ 9., 10., 11., 12.],                            [13., 14., 15., 16.]]]])output, indices = pool(input)up=unpool(output, indices)# uptensor([[[[  0.,   0.,   0.,   0.],          [  0.,   6.,   0.,   8.],          [  0.,   0.,   0.,   0.],          [  0.,  14.,   0.,  16.]]]])ConvTranspose2d# 函数接口torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)该方法也是一种卷积，因此与Upsample不同，有训练参数反卷积简单说就是首先将原图做插值，元素之间插0，然后用普通卷积核做卷积，这样卷积后的结果比原图大，实现上采样的效果对比膨胀卷积，后者是首先将普通卷积做插值，元素之间插0，达到提高感受野的效果反卷积和膨胀卷积可以参考《卷积、反卷积、膨胀卷积》反卷积的棋盘格效应谷歌大脑的研究者在博客上分析了反卷积的棋盘格问题（图3）图3. 反卷积的棋盘格效应文章发现神经网络生成的图片往往有明显的棋盘格效应，经过分析是由于生成网络使用的反卷积层导致的图4. 反卷积导致棋盘格效应如图4，当反卷积层的`kernel_size`不能被`stride`整除的时候，会导致不均匀重叠的效应，这个效应在二维图像上更加明显事实上，由于网络往往包含多层反卷积，即使确保kernel_size与stride的整除关系，也不可避免会产生不均匀重叠因此文章推荐Upsample插值+卷积代替反卷积实现上采样"
  },
  
  {
    "title": "PyTorch分布式训练",
    "url": "/posts/PyTorch%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/",
    "categories": "深度学习",
    "tags": "",
    "date": "2022-09-15 00:00:00 +0000",
    





    
    "snippet": "本文重点参考《GPU多卡并行训练总结（以pytorch为例）》、《Pytorch 单机并行训练》、《PyTorch 多卡分布式训练》使用多GPU并行训练可以分为模型并行和数据并行2种模式：1）模型并行模型很大，一个GPU显存不够，需要将网络不同模块放在不同的GPU上2）数据并行数据本身比较大或者需要比较大的batch size（一般来说增加batch_size可以获得更加可靠的梯度从而提高训...",
    "content": "本文重点参考《GPU多卡并行训练总结（以pytorch为例）》、《Pytorch 单机并行训练》、《PyTorch 多卡分布式训练》使用多GPU并行训练可以分为模型并行和数据并行2种模式：1）模型并行模型很大，一个GPU显存不够，需要将网络不同模块放在不同的GPU上2）数据并行数据本身比较大或者需要比较大的batch size（一般来说增加batch_size可以获得更加可靠的梯度从而提高训练精度）。训练过程中不同GPU的模型保持一致，但使用不同的训练数据图1. 模型并行与数据并行数据并行的两种方式pyTorch实现了2种数据并行的训练方法1) DataParallel优点是对现有代码改造极少，缺点是仅支持单机单进程多线程，负载不均衡的问题很严重# 只需要将现有的模型套进函数再指定GPU就可以了net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])DataParallel的原理为：1）将模型加载进主设备，一般默认为cuda:02）每次迭代时主设备将模型复制到其他GPU（从设备）上，同时将训练数据进行均分，分配给其他从设备进行前向传播3）每个从设备独立做前向传播和反向传播，但是只求取梯度4）各从设备的梯度汇总到主设备后进行求和，更新主设备的模型参数完成一轮迭代可以看到，这种方式主设备的计算量会远大于从设备（负载不均衡），此外每轮迭代各从设备有大量的数据需要与主设备通信，当GPU数量增加到一定值后通信的瓶颈会越来越明显因此现在一般不推荐DataPrallel的方式2) DistributedDataParallelDistributedDataParallel支持多机多卡，可以实现真正的分布式计算DistributedDataParallel也会有一个主设备(cuda:0)，训练开始时负责将模型分发给各从设备，每个设备开启一个进程与DataPrallel最大的不同是，每个从设备反向传播得到的梯度不再汇总到主设备，而是使用RingAllReduce的方法环形更新所有设备的梯度，由于训练一开始各设备的模型参数是一样的，因此环形更新完后所有设备参数仍保持一致RingAllReduce环形梯度传递如图2所示，每个从设备只会接收其上游邻居设备的数据，同时只会发送数据给下游邻居设备图2. RingAllReduce环形梯度传递如图2右所示，假设一共有$N=5$个设备，则一共需要传递（当前设备梯度+上游设备梯度）$N-1=4$次可以确保所有设备的梯度一致进一步的，方法还会将每块设备上的数据做分块（假设分为$D$块），对每个设备，每$N$轮传递只有$D-1$块数据发送和接收，这样可以进一步降低通信量BatchNormal的同步训练时假设每个设备的batch size为$B$，显然BN层均值和方差仅针对batch_size=B的数据如果能够综合考虑所有设备的数据计算的均值和方差会更接近真实数据分布，训练效果会更好因此可以考虑每轮训练时同步所有设备的BN（会损失部分训练速度）分布式训练相关名词            名词      说明                  WORLD SIZE      参与训练的总设备（进程）数，如一共2台主机每台主机4张显卡，那么WORLD SIZE就是$2\\times 4=8$              Rank      标识所有主机所有设备（进程）的序号，从0开始              Local Rank      标识当前主机所有设备（进程）的序号，从0开始              Master Adress      标识主进程所在主机IP，每个进程都填一样的              Master Port      标识主进程应该监听的端口号      数据并行DistributedDataParallel的使用方法PyTorch支持两种启动方法，其中torch.distributed.launch是对torch.multiprocessing的封装，对用户来说代码量更少使用更方便图3. 两种启动方法下面以torch.distributed.launch的启动方式介绍代码组织方式0）启动代码假设一共2台主机，每台主机4块显卡# 主机1的启动脚本export MASTER_ADDR=gpu-server1export MASTER_PORT=10086CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --use_env --nproc_per_node=4 --nnodes=2 --node_rank=0 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT train.py# 主机2的启动脚本export MASTER_ADDR=gpu-server1export MASTER_PORT=10086CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --use_env --nproc_per_node=4 --nnodes=2 --node_rank=1 --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT train.py  CUDA_VISIBLE_DEVICES是设备Local Rank  nproc_per_node是每台主机的进程（设备）数  nnodes是有几台主机  node_rank是当前启动的是哪一台主机  master_addr和master_port是主进程所在主机的地址和端口  use_env当使用了该选项时（参考《Pytorch 分布式训练的坑（use_env, loacl_rank)》），launch会将local_rank添加到环境变量，否则执行train.py --local_rank=xx（需要train.py中实现argparse.ArgumentParser）1）初始化进程组&amp;设置使用的显卡import osimport argparseimport torch.distributed as distfrom torch.nn.parallel import DistributedDataParallel as DDP# 如果在启动脚本中设置了use_envlocal_rank = os.environ['LOCAL_RANK']# 如果没有在启动脚本中设置use_envparser = argparse.ArgumentParser()parser.add_argument(\"--local_rank\", default=-1, type=int)args = parser.parse_args()local_rank = args.local_rankdist.init_process_group(backend='nccl') # 使用nccl实现GPU通信torch.cuda.set_device(local_rank)device = torch.device('cuda:%d'%local_rank)2）创建模型&amp;使用DDP包装创建模型后，如果没有权重文件，为了确保各设备的起始参数完全一致，需要主设备首先保存参数，然后其他设备读取该参数self.model = CornerNet(self.num_classes).to(self.device)tmp_weights = ''if cfg.TRAIN.RESUME_FLAG:    if get_rank()==0:        logger.info('loading weights from %s'%cfg.TRAIN.RESUME_WEIGHT)    self.model = load_weight(self.model, cfg.TRAIN.RESUME_WEIGHT, auto_adaptive=True, device=self.device)    # DDP模式下为确保各进程模型完全一致(模型权重初始化)，需要主进程保存模型权重供其他进程读取    tmp_weights = os.path.join(root_dir, 'tmp.pth')    if get_rank()==0:        torch.save(self.model.state_dict(), tmp_weights)    # 阻塞，直到所有进程都运行到这里    dist.barrier()    self.model = load_weight(self.model, tmp_weights, auto_adaptive=True, device=self.device)# 同步BNself.model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(self.model).to(self.device)self.model = DDP(self.model, device_ids=[get_rank()])3）包装分布式数据采样器每个设备都会加载同样的数据集，但需要配置采样器确保不同的设备读取的数据不重叠self.train_sampler = DistributedSampler(self.corner_dataset)train_batch_sampler = BatchSampler(    self.train_sampler,    cfg.TRAIN.BATCH_SIZE,    drop_last=True)self.dataloader = DataLoader(    self.corner_dataset,    batch_sampler=train_batch_sampler,    pin_memory=True, # 锁页内存    num_workers=cfg.TRAIN.NUM_WORKS)  DistributedSampler负责将数据乱序，为不同的设备指定不同的数据索引  BatchSampler则根据数据索引为当前设备生成batch图4. DistributedSampler如图4，假设数据集一共11个样本，使用2块GPU计算首先打乱数据（可选），然后确定每个设备的数据大小$\\lceil \\frac{11}{2} \\rceil=6$，由于$11-2\\times 6=-1$，因此将第1个数据补到末尾确保每块GPU数据量一致，最后按$step=2$分配数据索引到不同设备此外，如果DistributedSampler是乱序模式，在训练过程每次epoch开始时设置一次self.train_sampler.set_epoch(epoch)，确保每轮epoch会重新打乱一次数据4）其他      所有的数据保存、log输出要确保只在一个进程中执行（一般指定local_rank==0），否则会出现重复输出、多进程数据读写错误        显示误差时需要将所有设备的误差求和取平均才能反映整体误差（backward计算梯度因为仅针对当前设备因此不需要）  def reduce_loss(self, loss, average=False):    ''' DDP模式将所有device的Loss求和取平均    '''    world_size = dist.get_world_size()    with torch.no_grad():        dist.all_reduce(loss)        if average: loss /= world_size        return loss  并行训练时学习率可以增加为$设备参数数量=N$倍因为每个设备在更新参数时使用的梯度是所有设备汇总的，相当于batch_size增加了N倍，而batch_size的增加相当于将梯度方差减少了（更精确），因此可以增加lr提高训练速度"
  },
  
  {
    "title": "【CNN系列目标检测】（11）CornerNet算法",
    "url": "/posts/CNN%E7%B3%BB%E5%88%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-11-CornerNet%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, CNN系列目标检测",
    "tags": "CNN系列目标检测",
    "date": "2022-09-07 00:00:00 +0000",
    





    
    "snippet": "CornerNet【1】是ECCV2018的一篇目标检测算法，在COCO上获得了42.2%的AP，超过了同一年发表的YoloV3  CornerNet是one-stage的anchor-free算法  CornerNet不直接预测bbox，而是将目标检测转化为bbox左上、右下两个关键点检测问题  CornerNet池化后的特征图上每个点代表一个预测，这与YoloV3和YoloX类似，但是Y...",
    "content": "CornerNet【1】是ECCV2018的一篇目标检测算法，在COCO上获得了42.2%的AP，超过了同一年发表的YoloV3  CornerNet是one-stage的anchor-free算法  CornerNet不直接预测bbox，而是将目标检测转化为bbox左上、右下两个关键点检测问题  CornerNet池化后的特征图上每个点代表一个预测，这与YoloV3和YoloX类似，但是YoloV3每个点上会预测多个不同anchor的bbox，而YoloX每个点也会预测多个bbox（由于没有anchor所以需要做SimOTA匹配）。从这一点看，CornerNet预测重叠度大的不同目标的能力可能会差一些  文章最后的消融实验表明，影响模型AP最主要的还是corner的预测精度  CornerNet的池化算子是在cpp中实现的，对学习c++调用libtorch然后编译为so文件与pytorch配合编程是一个很好的借鉴  另外值得一提的是，CornerNet后处理按每个类别分别做nms，这样的好处是重叠在一起的不同类别目标不会被nms掉图1. 按类别做nms可以更好的检测iou较大的不同类别目标官方提供了pytorch实现的代码本人基于官方代码整理重构了代码，修复了一些因为pytorch版本差异导致的bug本文额外参考了博文《CornerNet 算法笔记》核心问题对于目标检测任务，直接预测bbox的top-left和bottom-right点存在2个问题：1) 目标bbox的corner往往并不在目标上面，如何提取corner与目标的关联特征？2) top-left点和bottom-right点是分别提取的，如何确定其对应关系？对于第一个问题，文章提出corner pooling层对于第二个问题，网络同时输出每个点的embedding，属于同一个目标的tl和br，其embedding向量距离最小网络结构  CornerNet首先用7x7的卷积层+resdual模块将输入尺寸缩小为原来的1/4（训练输入的尺寸为511，网络最终输出的尺寸为128）  网络的backbone使用2个串联的hourglass network，该网络先做降采样再做上采样，输出与输入尺寸一致  backbone后接top-left和bottom-right预测模块，每个模块包含一个corner pool和heatmaps、embeddings、offsets输出图2. CornerNet网络结构pre模块对图像做1/4降采样，特征通道提高到256# 普通卷积和残差网络的组合self.pre = nn.Sequential(            BaseConv(3, 128, 7, stride=2, act='relu'),            Resdual(128, 256, stride=2)        )hourglass network模块hourglass【2】是一个用来做姿态估计的网络，使用残差网络做基础层，先做一系列降采样再做上采样恢复尺寸，同时网络的输入和上采样的输出做一个加法整体上形成一个类似残差网络的结构class kp_module(nn.Module):    def __init__(self, n, dims, modules):        ''' hourglass网络        '''        super().__init__()        self.n = n        self.dims = dims                curr_mod, next_mod = modules[0], modules[1]        curr_dim, next_dim = dims[0],    dims[1]        self.up1 = make_layer(curr_dim, curr_dim, curr_mod)        self.max1 = nn.Sequential()        self.low1 = make_hg_layer(curr_dim, next_dim, curr_mod)        self.low2 = kp_module(            n-1, dims[1:], modules[1:]        ) if n &gt; 1 else \\            make_layer(next_dim, next_dim, next_mod)        self.low3 = make_layer_revr(next_dim, curr_dim, curr_mod)        self.up2 = nn.Upsample(scale_factor=2)        self.merge = MergeUp()    def forward(self, x):        up1 = self.up1(x)              max1 = self.max1(x)        low1 = self.low1(max1)       # 下采样        low2 = self.low2(low1)       # 嵌套hourglass        low3 = self.low3(low2)          up2  = self.up2(low3)        # 上采样        return self.merge(up1, up2)  # 输入和输出相加，整体类似resdualcorner预测模块backbone输出的cnv首先经过tl_pool和br_pool，然后经过3种卷积层输出corner的heatmap、emdedding和offsettl_cnv = tl_cnv_(cnv)br_cnv = br_cnv_(cnv)tl_heat, br_heat = tl_heat_(tl_cnv), br_heat_(br_cnv) # heatmapstl_tag,  br_tag  = tl_tag_(tl_cnv),  br_tag_(br_cnv)  # embeddingstl_regr, br_regr = tl_regr_(tl_cnv), br_regr_(br_cnv) # offsets假设池化后的特征图(tl_cnv、br_cnv)尺寸为H x W，上面每个点代表一个bbox估计其中  heatmaps尺寸为B x cls x H x W表示每个类别下每个位置corner点的响应值  embeddings尺寸为B x 1 x H x W表示每个位置corner点的特征  offsets尺寸为B x 2 x H x W表示每个位置corner点的下采样误差corner pool如图3所示，一个理想的紧致的bbox，其corner所在的水平虚线和垂直虚线总是会与目标相接触如果中间特征层对目标敏感(heatmap)，那么对于top-left点，从该点往右和往下遍历总是会找到一个突变点图3. 理想的bbox基于上述考虑，文章设计了一种新的池化层图4. 左：池化层结构，右：池化计算思路如图4所示，对于每个点，首先从该点往右找最大的特征值$f_x$，然后从该点往下找最大的特征值$f_y$，最后将该点赋值为$f_x+f_y$。可以理解，对于gt中的top-left位置，按上述方法赋值后肯定比其他位置响应度大具体的池化计算思路类似冒泡排序：以水平方向为例，从最右到待计算的点，依次比较当前点和右边一个点的最大值并赋值到当前点，这样最终的结果是，每个位置的值都是从该位置往右所有位置的最大值# TopPool、LeftPool池化操作在cpp里实现class pool(nn.Module):    def __init__(self, dim, pool1, pool2):        super().__init__()        self.p1_conv1 = BaseConv(dim, 128, 3, act='relu')        self.p2_conv1 = BaseConv(dim, 128, 3, act='relu')        self.p_conv1 = BaseConv(128, dim, 3)        self.conv1 = BaseConv(dim, dim, 1)        self.relu1 = get_activation('relu')        self.conv2 = BaseConv(dim, dim, 3, act='relu')        self.pool1 = pool1()        self.pool2 = pool2()    def forward(self, x):        p1_conv1 = self.p1_conv1(x)        pool1 = self.pool1(p1_conv1)        p2_conv1 = self.p2_conv1(x)        pool2 = self.pool2(p2_conv1)        p_conv1 = self.p_conv1(pool1 + pool2)                conv1 = self.conv1(x)        relu1 = self.relu1(p_conv1 + conv1)        conv2 = self.conv2(relu1)        return conv2class tl_pool(pool):    def __init__(self, dim):        super().__init__(dim, TopPool, LeftPool)corner pool 的前向与后向计算实现以top_pool为例，前向计算就是从最后一行开始向上一行对每列冒泡交换最大值std::vector&lt;at::Tensor&gt; top_pool_forward(    at::Tensor input) {    // Initialize output    at::Tensor output = at::zeros_like(input);    // Get height    int64_t height = input.size(2);    // Copy the last row    at::Tensor input_temp  = input.select(2, height - 1); // input[:,:,height-1,:]    at::Tensor output_temp = output.select(2, height - 1);    output_temp.copy_(input_temp);    at::Tensor max_temp;    for (int64_t ind = 1; ind &lt; height; ++ind) {        input_temp  = input.select(2, height - ind - 1);        output_temp = output.select(2, height - ind);        max_temp    = output.select(2, height - ind - 1);        at::max_out(max_temp, input_temp, output_temp);    }    return {         output    };}后向计算是将下一层网络的梯度反向传播到本层的输入，函数的输入是本层网络的输入和下一层网络反向传播回来的梯度由于本层的前向操作就是比较大小并交换输入的位置，因此是从倒数第2行开始，依次比较每列数据：如果本行输入比下一行输入大（前向传播时该输入没有被替换），那么保留这个位置的梯度，否则这个位置的梯度置0需要注意的是，在官方代码中会将本行输入比下一行小的位置的梯度累加到下一行位置我认为从理论上来说累加是不对的，可能作者只是想通过这种方法增大梯度值// 官方累加版的梯度反向传播std::vector&lt;at::Tensor&gt; top_pool_backward(    at::Tensor input,        // 本层前向传播时的输入    at::Tensor grad_output   // 下一层反向传播回来的梯度) {    auto output = at::zeros_like(input);    int32_t batch   = input.size(0);    int32_t channel = input.size(1);    int32_t height  = input.size(2);    int32_t width   = input.size(3);    auto max_val = at::zeros({batch, channel, width}, torch::CUDA(at::kFloat));    auto max_ind = at::zeros({batch, channel, width}, torch::CUDA(at::kLong));    auto input_temp = input.select(2, height - 1);    max_val.copy_(input_temp);    max_ind.fill_(height - 1);    auto output_temp      = output.select(2, height - 1);    auto grad_output_temp = grad_output.select(2, height - 1);    output_temp.copy_(grad_output_temp);    auto un_max_ind = max_ind.unsqueeze(2);    auto gt_mask    = at::zeros({batch, channel, width}, torch::CUDA(at::kBool));    auto max_temp   = at::zeros({batch, channel, width}, torch::CUDA(at::kFloat));    for (int32_t ind = 1; ind &lt; height; ++ind) {        input_temp = input.select(2, height - ind - 1);        at::gt_out(gt_mask, input_temp, max_val);        max_temp.resize_(0);        at::masked_select_out(max_temp, input_temp, gt_mask);        max_val.masked_scatter_(gt_mask, max_temp);        max_ind.masked_fill_(gt_mask, height - ind - 1);        grad_output_temp = grad_output.select(2, height - ind - 1).unsqueeze(2);        output.scatter_add_(2, un_max_ind, grad_output_temp); // 当前输入最大值位置的梯度会被累加    }    return {        output    };}// 修改为不累加梯度的版本std::vector&lt;at::Tensor&gt; top_pool_backward(    at::Tensor input,    at::Tensor grad_output) {    auto output = at::zeros_like(input);    int32_t batch   = input.size(0);    int32_t channel = input.size(1);    int32_t height  = input.size(2);    int32_t width   = input.size(3);    auto max_val = at::zeros({batch, channel, width}, torch::CUDA(at::kFloat));    auto input_temp = input.select(2, height - 1);    max_val.copy_(input_temp);    auto max_temp   = at::zeros({batch, channel, width}, torch::CUDA(at::kFloat));    auto gt_mask    = at::zeros({batch, channel, height, width}, torch::CUDA(at::kBool));    auto gt_mask_temp = gt_mask.select(2, height - 1);    gt_mask_temp.fill_(1);    for (int32_t ind = 1; ind &lt; height; ++ind) {        input_temp = input.select(2, height - ind - 1);        gt_mask_temp = gt_mask.select(2, height - ind - 1);        at::gt_out(gt_mask_temp, input_temp, max_val);        max_temp.resize_(0);        at::masked_select_out(max_temp, input_temp, gt_mask_temp);        max_val.masked_scatter_(gt_mask_temp, max_temp);    }    output = at::mul(grad_output, gt_mask);    return {        output    };}经过对比实验，以上两个版本在训练中表现没有特别大的差异heatmaps、emdeddings和offsets1）heatmaps从corner pooling出来的特征会分别经过3种卷积层（图1）生成heatmaps、embeddings和offsets# keypoint heatmaps, 每个类别下每个位置top-left点的响应值# [B, num_cls, H, W]self.tl_heats = nn.ModuleList([    nn.Sequential(        BaseConv(self.cnv_dim, self.dims[0], 3, bn=False, act='relu'),        BaseConv(self.dims[0], self.num_cls, 1, bn=False)    ) for _ in range(self.nstack)])在训练数据准备阶段会以每个gt的top-left和bottom-right点为中心，以bbox的尺寸为参考计算一个半径，在这个圆内对gt_heatmaps做高斯分布赋值2）embeddings# embeddings, 每个位置top-left点的特征值# [B, 1, H, W]self.tl_tags = nn.ModuleList([    nn.Sequential(        BaseConv(self.cnv_dim, self.dims[0], 3, bn=False, act='relu'),        BaseConv(self.dims[0], 1, 1, bn=False)    ) for _ in range(self.nstack)])一般来说embedding应该是一个多维的向量，但这里输出却是一个值可以把BaseConv(self.cnv_dim, self.dims[0], 3, bn=False, act='relu')的输出理解为原始的embedding，后面经过1x1x1的卷积只是进一步将特征收缩，方便最终比较3）offsets# offsets, 每个位置top-left点的xy下采样精度误差# [B, 2, H, W]self.tl_regrs = nn.ModuleList([    nn.Sequential(        BaseConv(self.cnv_dim, self.dims[0], 3, bn=False, act='relu'),        BaseConv(self.dims[0], 2, 1, bn=False)    ) for _ in range(self.nstack)])从网络模型可以知道，输入图像是网络最终输出特征图尺寸的4倍，也就是说输出的bbox需要放大4倍才能正确对应输入图像的目标，但下采样过程是存在精度损失的，这个损失会影响最终bbox的精度，因此文章对该偏差也单独设计了一个输出在文章的消融实验中添加了ground-truth精度误差补偿后AP上升了13%，说明该项是有意义的这里我有一点疑问: 下采样误差应该是一个固定的值，对所有目标都是一样的，为什么这里需要学习？网络loss定义1）对heatmaps，文章使用focal loss计算误差\\[L_h=-\\frac{1}{N}\\sum_{c=1}^C\\sum_{i=1}^H\\sum_{j=1}^W\\begin{cases}(1-p_{cij})^\\alpha log(p_{cij})&amp;y_{cij}=1\\\\(1-y_{cij})^\\beta (p_{cij})^\\alpha log(1-p_{cij})&amp;else\\end{cases} \\tag{1}\\]可以看到，在(1)中对正确预测且概率较大的值会乘以一个较小的权重，focal loss通过这种方式降低容易样本的loss，增加困难样本的loss2）对embeddings，文章设计了pull和push两种误差pull误差计算gt中本属于同一个目标的top-left和bottom-right的embeddings差异(越小越好)\\[L_{pull}=\\frac{1}{N}\\sum_{k=1}^N((e_{t_k}-e_k)^2+(e_{b_k}-e_k)^2) \\tag{2}\\]式2中$e_{t_k}$和$e_{b_k}$分别表示第$k$个embeddings值对，$e_k$表示该值对的均值push误差计算gt中不属于同一目标的top-left和bottom-right的embeddings差异(越大越好)\\[L_{push}=\\frac{1}{N(N-1)}\\sum_{k=1}^N\\sum_{j=1,j\\neq k}^Nmax(0,\\Delta-|e_k-e_j|) \\tag{3}\\]式3中$e_k$和$e_j$表示两组值对的均值3）对offsets，文章使用smooth L1 loss\\[L_{off}=\\frac{1}{N}\\sum_{k=1}^NSmoothL1Loss(o_k, \\hat{o}_k) \\tag{4}\\]其中$o_k$是gt中每个点的下采样误差\\[o_k=(\\frac{x_k}{n}-\\lfloor \\frac{x_k}{n}\\rfloor, \\frac{y_k}{n}-\\lfloor \\frac{y_k}{n}\\rfloor) \\tag{5}\\]式5中的$n$在这个网络中是4文献【1】 Law H, Deng J. Cornernet: Detecting objects as paired keypoints[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 734-750.【2】 Newell A, Yang K, Deng J. Stacked hourglass networks for human pose estimation[C]//European conference on computer vision. Springer, Cham, 2016: 483-499."
  },
  
  {
    "title": "【人脸识别】（4）RetinaFace算法",
    "url": "/posts/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-4-RetinaFace%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, 人脸识别",
    "tags": "人脸识别",
    "date": "2022-07-22 00:00:00 +0000",
    





    
    "snippet": "RetinaFace【1】是2019年InsightFace提出的人脸检测算法与MTCNN的多级检测机制不同，RetinaFace是单阶检测器，通过引入FPN和SSH增强网络多尺度(尤其是小尺度)检测能力，借鉴SSD实现anchor和误差计算RetinaFace认为人脸检测与通用目标检测的不同是：人脸目标往往宽高比比较固定，但尺度变化很大。因此网络设计要集中在多尺度能力上，而损失函数中的an...",
    "content": "RetinaFace【1】是2019年InsightFace提出的人脸检测算法与MTCNN的多级检测机制不同，RetinaFace是单阶检测器，通过引入FPN和SSH增强网络多尺度(尤其是小尺度)检测能力，借鉴SSD实现anchor和误差计算RetinaFace认为人脸检测与通用目标检测的不同是：人脸目标往往宽高比比较固定，但尺度变化很大。因此网络设计要集中在多尺度能力上，而损失函数中的anchors宽高比则可以简单设为1需要说明的是RetinaFace的正式发表版本(CVPR2020)【2】有些许不同，可能是为了提高中稿率，正式版本更侧重3D face重建的方向，文章认为landmarks(5个点)和更多的其他关键点能提高人脸检测的精度(但这也意味着样本获取难度的上升)图1. RetinaFace检测RetinaFace网络结构图2. RetinaFace网络结构如图2所示，RetinaFace首先用backbone提取图像特征，然后接入FPN融合前者不同尺度的特征，三个SSH分别对FPN出来的三种尺度特征做通道融合，每个SSH都对应一个检测头，最终的输出融合每个检测头的输出BackBoneRetinaFace分别测试了MobileNet和ResNet50作为backbone提取图像特征RetinaFace将backbone中3个不同层次（尺度）上的特征作为输出给FPNFPNFPN(feature pyramid network)【3】通过对尺寸小的高层特征做上采样，与尺寸大的低层特征融合获得高分辨率、强语义的特征，提升小目标的检测能力图3. FPN网络结构其中1x1 conv的作用是统一不同层特征的channels可以看到FPN的输出仅改变了channels，并没有改变输入特征的HW尺寸SSHSSH(Single Stage Headless Face Detector)【4】是ICCV2017录用的一篇人脸检测算法文章SSH丢弃了head(往往包含大量参数的全连接层)，直接从分类网络的前几层卷积层进行人脸检测，速度快，精度也很不错图4. SSH检测模块网络结构SSH中使用了3个如图4所示的检测模块，3个检测模块分别从backbone的不同层输出接入，用于检测大脸、中脸、小脸(从这个角度看，RetinaFace就是在SSH的基础上做的补充：backbone和SSH检测模块中加入了FPN，增强多尺度检测能力)      从图4可以看到，SSH检测模块没有改变输入特征的尺寸(包括channels)        FPN是在HW上做文章，融合不同尺度的信息；SSH是在channels上做文章，融合不同通道的信息(增加感受野)  损失函数设计RetinaFace的损失参考了SSD的思路：首先为3种尺度的输出生成对应的anchors，网络的输出是以这些anchors为基础的偏移量；然后结合anchors、网络输出和gt计算lossanchors生成图5. SSD的anchors生成如图5所示(参考SSD)，feature map的尺寸是5x5，将其对应到图像上，以feature map中的每个红点为中心，生成不同尺寸和宽高比的anchorsRetinaFace与此类似，首先为3种尺度的输出定义feature map的间隔steps（红点之间的距离）和anchors的尺寸min_sizes（每个红点包含2种尺寸，宽与高一致）# resnet50的参数'steps': [8, 16, 32]'min_sizes': [[16, 32], [64, 128], [256, 512]]在anchors生成时再根据输入图像的尺寸对其做归一化anchors = []for k, f in enumerate(self.feature_maps):    min_sizes = self.min_sizes[k]    for i, j in product(range(f[0]), range(f[1])):        # 对第k个feature_map的第(i,j)个红点，生成不同尺寸的anchors        for min_size in min_sizes:            # anchor尺寸的归一化            s_kx = min_size / self.image_size[1]            s_ky = min_size / self.image_size[0]            # self.steps[k]是红点之间的距离，dense_cx就是归一化后的anchor中心            dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]            dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]            for cy, cx in product(dense_cy, dense_cx):                anchors += [cx, cy, s_kx, s_ky]损失计算在代码中这一部分由MultiBoxLoss完成，其接受predictions, priors, targets为输入，返回loss_l, loss_c, loss_landm总体上MultiBoxLoss可以分为两个部分：  groundtruth(targets)和anchors(priors)的匹配，同时对targets做encode  根据匹配结果计算predictions与encode后的targets的误差anchors匹配对每张图像，假设groundtruth中有n个bbox，anchors数量为m1) 计算每个bbox和anchor的iou（overlaps=n*m）2) 对每个bbox，找到与其iou最大的anchor(best_prior_overlap=n*1);对每个anchor，找到与其iou最大的bbox(best_truth_overlap=1*m)3) 保留best_prior_overlap中iou大于0.2的项，根据best_prior_overlap为每个anchor匹配最佳的gt: matches=m*4、matches_landm=m*10、conf=m4) 将最佳匹配中iou小于0.35的conf置0（作为负样本）5) 分别对匹配后的bbox和landm做encodebbox encode1) 计算每个bbox与匹配的anchor的中心偏差g_cxcy，将其除以anchor的尺寸做归一化，对其再除以误差放大比例因子2) 计算bbox与anchor长宽之比作为归一化的尺度偏差g_wh，对其再取对数除以误差放大比例因子3) 将g_cxcy和g_wh拼接得到encoded_loc=m*4def encode(matched, priors, variances):    # variances是误差放大系数    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]    g_cxcy /= (variances[0] * priors[:, 2:])    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]    g_wh = torch.log(g_wh) / variances[1]    return torch.cat([g_cxcy, g_wh], 1)landmark encode计算每个ldmk的5个点与匹配的anchor的中心偏差g_cxcy，再将其除以anchoe的尺寸做归一化，最后对其再除以误差放大比例因子def encode_landm(matched, priors, variances):    # variances是误差放大系数    matched = torch.reshape(matched, (matched.size(0), 5, 2))    priors_cx = priors[:, 0].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)    priors_cy = priors[:, 1].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)    priors_w = priors[:, 2].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)    priors_h = priors[:, 3].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)    priors = torch.cat([priors_cx, priors_cy, priors_w, priors_h], dim=2)    g_cxcy = matched[:, :, :2] - priors[:, :, :2]    g_cxcy /= (variances[0] * priors[:, :, 2:])    g_cxcy = g_cxcy.reshape(g_cxcy.size(0), -1)    return g_cxcy基于匹配结果的误差计算anchors匹配的输出是loc_t、conf_t、landm_t，分别表示每个anchor最佳匹配的gt中的框位置、置信度、landmark因此对一个batch，上述三个量的尺寸分别为batch_size*num_anchors*4、batch_size*num_anchors*10、batch_size*num_anchors1) 根据conf_t&gt;0（正样本）筛选对应的loc_t和landm_t，分别计算与predictions的L1 losslandm_p = landm_data[pos_idx1].view(-1, 10) # 预测值landm_t = landm_t[pos_idx1].view(-1, 10)loss_landm = F.smooth_l1_loss(landm_p, landm_t, reduction='sum')loc_p = loc_data[pos_idx].view(-1, 4) # 预测值loc_t = loc_t[pos_idx].view(-1, 4)loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')2) 根据conf_t&gt;0（正样本）从置信度预测值batch_conf中筛选对应的值，结合log_sum_exp计算分类误差loss_closs_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))其中log_sum_exp是对softmax+cross entropy误差计算方法的改进方法，可以避免因数据溢出造成的错误。详细的介绍见《关于LogSumExp》3) 根据上一步得到的loss_c，做难负样本挖掘，更新loss_c难负样本挖掘就是：忽略loss较小的负样本，仅对loss较大的负样本和所有正样本重新计算loss# Hard Negative Miningloss_c[pos.view(-1, 1)] = 0 # filter out pos boxes for nowloss_c = loss_c.view(num, -1)_, loss_idx = loss_c.sort(1, descending=True)_, idx_rank = loss_idx.sort(1)num_pos = pos.long().sum(1, keepdim=True)num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)neg = idx_rank &lt; num_neg.expand_as(idx_rank)# Confidence Loss Including Positive and Negative Examplespos_idx = pos.unsqueeze(2).expand_as(conf_data)neg_idx = neg.unsqueeze(2).expand_as(conf_data)conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)targets_weighted = conf_t[(pos+neg).gt(0)]loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')总结      与MTCNN相比，RetinaFace训练数据准备更简单，前者需要根据label提前做crop，喂给网络的是人脸/非人脸/part人脸图像        与MTCNN相比，RetinaFace的landmark表达理论上更好。MTCNN中landmark的encode是计算其与人脸框左上角的距离，这样landmark与人脸bbox耦合度太高，标注时bbox的误差会影响landmark的值；而RetinaFace中landmark的encode是计算其与anchor中心的偏差，跟bbox不相关了        MTCNN和RetinaFace在正面近距离人脸检测场景中都能获得非常不错的结果，但后者在侧面/多尺度检测场景中精度更高        RetinaFace借鉴了通用目标检测算法的很多技巧，比如FPN、anchors和MultiBoxLoss，获得了不错的提升。当然在侧面/多尺度检测场景下仍有进一步提升的空间  官方代码是mxnet实现的，自己实现了pytorch版本文献【1】https://arxiv.org/pdf/1905.00641.pdf【2】Deng J, Guo J, Ververas E, et al. Retinaface: Single-shot multi-level face localisation in the wild[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020: 5203-5212.【3】Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2117-2125.【4】Najibi M, Samangouei P, Chellappa R, et al. Ssh: Single stage headless face detector[C]//Proceedings of the IEEE international conference on computer vision. 2017: 4875-4884."
  },
  
  {
    "title": "【人脸识别】（3）ArcFace算法",
    "url": "/posts/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-3-ArcFace%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, 人脸识别",
    "tags": "人脸识别",
    "date": "2022-06-30 00:00:00 +0000",
    





    
    "snippet": "ArcFace人脸识别算法【1】出自CVPR2019，是SphereFace与CosFace基础上的改进方法，在十个公开数据集上取得了SOTA效果ArcFace认为当前基于深度学习的人脸识别算法可以分为2类：      通过一组已知身份的人脸数据训练softmax分类层，然后从网络的中间层backbone获取特征向量embedding，在模型推理时通过比较embedding判断相似度。代表算...",
    "content": "ArcFace人脸识别算法【1】出自CVPR2019，是SphereFace与CosFace基础上的改进方法，在十个公开数据集上取得了SOTA效果ArcFace认为当前基于深度学习的人脸识别算法可以分为2类：      通过一组已知身份的人脸数据训练softmax分类层，然后从网络的中间层backbone获取特征向量embedding，在模型推理时通过比较embedding判断相似度。代表算法是ArcFace、CosFace、SphereFace、Center loss等        利用训练数据的配对策略，省掉类似softmax的分类层，直接学习抽象特征embedding，在模型推理时通过比较embedding判断相似度。代表算法是FaceNet  这两类算法在训练集上都能取得不错的精度，但也存在各自的问题：      第1类算法softmax层维度会随着样本中类别数量的增加而线性增加，学习到的特征泛化能力可能未必好        第2类算法随着样本中类别数量的增加，训练样本组合数会爆炸性的增加，并且中难度样本挖掘也是一大难点    此外注意到以上2类方法最终推理阶段都是使用backbone输出的embedding特征向量，我的理解是因为人脸识别应用通常分类的量是非常大的，而且在使用过程中可能会不断变化  因此如果还是用softmax之类作为网络输出就需要不断修改终端推理模型，并且模型参数会随着类别数的增加显著增加，最终可能超出终端设备承载能力  由此人脸识别任务的设计思路是：训练一个映射到固定维度空间的backbone，确保这种映射能最大化类间距离最小化类内距离（如此对于推理终端，模型大小是恒定的，每次只需要更新模型权重即可）ArcFace算法ArcFace属于上述分析中的第1类方法，在CosFace的基础上做了修改，改善了特征的泛化能力ArcFace的网络架构可以概括为：backbone + Arcface + CrossEntropyLoss，其中Arcface本质上是对全连接层的一个修改首先看softmax-crossEntropy的公式\\[L_1=-\\frac{1}{N}\\sum_{i=1}^Nlog\\frac{e^{W_{y_i}^Tx_i+b_{y_i}}}{\\sum_{j=1}^ne^{W_j^Tx_i+b_j}}\\tag{1}\\]其中$x_i\\in \\Bbb R^d$表示第$i$个样本的特征向量，特征维度为$d$，在本算法中默认为512$W\\in \\Bbb R^{d\\times n}$表示上一层到当前层的权重矩阵，$n$为类别数，$W_j\\in \\Bbb R^n$表示第$j$类输出层的权重注意到$W_j^Tx_i=|W_j|*|x_i|cos\\theta_j$，因此如果令$b_j=0$，并且对$W_j$做$L_2$归一化，则有\\[L_2=-\\frac{1}{N}\\sum_{i=1}^Nlog\\frac{e^{s*cos\\theta_{y_i}}}{e^{s*cos\\theta_{y_i}}+\\sum_{j=1,j\\neq y_i}^ne^{s*cos\\theta_{y_i}}}\\tag{2}\\]其中$s$是$x_i$的模长，即$x_i=s*|x_i|$，文章推荐取64$theta_{y_i}$是输出层类别$y_i$的权重向量与特征向量的夹角余弦，显然，如果特征向量代表的样本类别就是$y_i$，那么我们希望这个夹角为0，输出概率（即余弦值）为1到这里还只是对softmax公式的近似变形，ArcFace为了提高类内的紧实度和类间的离散度，在式2的基础上增加了角度上的惩罚$m$(**文章推荐$m$取0.5**)\\[L_3=-\\frac{1}{N}\\sum_{i=1}^Nlog\\frac{e^{s*cos(\\theta_{y_i}+m)}}{e^{s*cos(\\theta_{y_i}+m)}+\\sum_{j=1,j\\neq y_i}^ne^{s*cos\\theta_{y_i}}}\\tag{3}\\]在这里默认角度区间是[0,pi]，余弦函数在该区间单调递减，因此角度上加$m$相当于增加了一个margin图1. margin对比此外文章还结合SphereFace、CosFace、ArcFace提出了混合版的公式4\\[L_4=-\\frac{1}{N}\\sum_{i=1}^Nlog\\frac{e^{s*(cos(m_1\\theta_{y_i}+m_2)-m_3)}}{e^{s*(cos(m_1\\theta_{y_i}+m_2)-m_3)}+\\sum_{j=1,j\\neq y_i}^ne^{s*cos\\theta_{y_i}}}\\tag{4}\\]Arcface + CrossEntropyLoss 的代码实现class CombinedMarginLoss(nn.Module):    def __init__(self, embedding_size, num_cls,                  s, m1, m2, m3, intercls_filtering_thre=0):        ''' arcFace论文中(式4)结合SphereFace、ArcFace和CosFace的Loss        '''        super(CombinedMarginLoss, self).__init__()        self.s  = s        self.m1 = m1        self.m2 = m2        self.m3 = m3        self.intercls_filtering_thre = intercls_filtering_thre        self.cos_m = math.cos(self.m2)        self.sin_m = math.sin(self.m2)        self.theta = math.cos(math.pi - self.m2)        self.sinmm = self.m2*self.sin_m        self.easy_margin = False        # Wx中的W, x是backbone出来的向量        self.kernel = nn.Parameter(torch.Tensor(num_cls, embedding_size))        self.kernel.data.uniform_(-1, 1).renorm_(2,0,1e-5).mul_(1e5)        self.ce_loss = nn.CrossEntropyLoss()    def forward(self, logits, labels):        ''' logits: [B, num_cls]            labels: [B, 1]        '''        # Wx = |W||x|cos_theta        kernel_norm = F.normalize(self.kernel, p=2, dim=1)        cos_theta = F.linear(logits, kernel_norm).clamp(-1,1)        index_positive = torch.where(labels &gt;= 0)[0]        target_logit = cos_theta[index_positive, labels[index_positive]]        if self.m1 == 1.0 and self.m3 == 0.:            # arcFace loss            sin_theta = torch.sqrt(1. - torch.pow(target_logit, 2))      # 1-cos^2 = sin^2            cos_theta_m = target_logit*self.cos_m - sin_theta*self.sin_m # cos(target+margin)            if self.easy_margin:                final_target_logit = torch.where(                    target_logit&gt;0, cos_theta_m, target_logit                )            else:                # target_logit&gt;self.theta确保0&lt;=theta+m&lt;=pi，此时用cos(theta+m)                # 否则用修正的CosFace：cos(theta)-m*sin(m)                final_target_logit = torch.where(                    target_logit&gt;self.theta, cos_theta_m, target_logit-self.sinmm                )            cos_theta[index_positive, labels[index_positive]] = final_target_logit            cos_theta = cos_theta * self.s        elif self.m3 &gt; 0:            # cosFace loss            final_target_logit = target_logit - self.m3            cos_theta[index_positive, labels[index_positive]] = final_target_logit            cos_theta = cos_theta * self.s        # cross entropy loss        loss = self.ce_loss(cos_theta, labels)        return loss代码里self.kernel是公式1里的$W$，在训练过程中是可学习的logits是公式1里的$x$，从bacbone出来的时候已经做了$L_2$归一化特征可分性在faces_emore数据库里取前100个类别做训练，使用TSNE降维并做数据可视化图2. 使用TSNE将特征转为3维代码InsightFace官网开源了一系列人脸分析算法，其中arcface主页提供了MXNet、PyTorch、PaddlePaddle和OneFlow的官方实现官方的PyTorch版本是多卡并行版的，我自己参考其实现了一个简单的单机版PyTorch版本代码文献【1】Deng J, Guo J, Xue N, et al. Arcface: Additive angular margin loss for deep face recognition[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 4690-4699."
  },
  
  {
    "title": "【人脸识别】（2）FaceNet算法",
    "url": "/posts/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-2-FaceNet%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, 人脸识别",
    "tags": "人脸识别",
    "date": "2022-06-21 00:00:00 +0000",
    





    
    "snippet": "人脸相关任务可以分为以下3类                                             face verification      人脸验证      1:1的比对，看该人脸是否为特定人脸，属于二分类问题（如登录、身份确认等）              face identification      人脸识别      1:N的比对，判断该人脸是谁，属于多分...",
    "content": "人脸相关任务可以分为以下3类                                             face verification      人脸验证      1:1的比对，看该人脸是否为特定人脸，属于二分类问题（如登录、身份确认等）              face identification      人脸识别      1:N的比对，判断该人脸是谁，属于多分类问题              face recognition      人脸识别      广义上是vertification，狭义上是identification      FaceNet【1】是google在2015年提出的通用框架，试图同时解决人脸验证、识别和聚类的问题FaceNet的效果非常好，在LFW数据集上测试的准确率达到99.63%，在YouTube Faces DB数据集上，准确率为95.12%，并且测试发现算法对图像尺寸也要求不高，$80\\times 80$的图像也能有效分类FaceNet算法图1. FaceNet结构FaceNet算法思路很简单：通过CNN网络将人脸图像映射到归一化的特征空间（embedding，文中默认128维），确保在这个特征空间中类内距离尽可能小，类间距离尽可能大其中CNN网络可以用各种已有的backbone（如Inception、Mobilenet等）做很小改动得到，比较麻烦的是如何确保训练得到的映射空间分类性能好。对此，文章提出了Triplet Loss，并对训练数据的组织方式做了研究Triplet Loss对于基准人脸Anchor、与基准人脸属于同一类的人脸Positive和其他类的人脸Negative，计算三者的关系，要满足\\[||x_i^a-x_i^p||_2^2+\\alpha&lt;||x_i^a-x_i^n||_2^2\\tag{1}\\]其中$x_i^a$是某个基准人脸的embedding向量，$x_i^p$是Positive人脸的embedding向量，$x_i^n$是Negative人脸的embedding向量为了确保训练的有效性，一个直接的想法是每次选取那个与Anchor距离最大，与Negative距离最小的Positive（类比SVM中的支撑点），这样每次的梯度最大。但这意味着每次都需要对全部数据做一次推理+L2距离，计算量太大于是文章提出在每个mini-batch中做距离比较，而在后续的实验中文章又发现其实不做hardest positive挑选直接将所有的样本一起计算loss反而更鲁棒（这让我想到DL本身的mini-batch SGD，它们之所以有效可能根本原因都差不多：批量的loss在一定程度上做了平滑）训练数据的组织  为了每个batch都能计算Triplet Loss，需要训练数据同时包含Anchor、Positive和Negative  为了提升每次训练的效果，每个batch需要有足够多的种类，文章每个batch确保大约40种人脸，这样每个batch=3*40就会比较大（文章在LFW上达到的99.6%的精度使用的bacth为1800）def __getitem__(self, index):    images = np.zeros((3, 3, self.input_shape[0], self.input_shape[1]))    labels = np.zeros((3))    #   先获得两张同一个人的人脸    #   用来作为anchor和positive    c               = random.randint(0, self.num_classes - 1)    selected_path   = self.paths[self.labels[:] == c]    while len(selected_path) &lt; 2:        c               = random.randint(0, self.num_classes - 1)        selected_path   = self.paths[self.labels[:] == c]    #   随机选择两张    image_indexes = np.random.choice(range(0, len(selected_path)), 2)    image = cvtColor(Image.open(selected_path[image_indexes[0]]))            image = resize_image(image, [self.input_shape[1], self.input_shape[0]], letterbox_image = True)    image = preprocess_input(np.array(image, dtype='float32'))    image = np.transpose(image, [2, 0, 1])    images[1, :, :, :] = image    labels[1] = c    #   取出另外一个人的人脸    different_c         = list(range(self.num_classes))    different_c.pop(c)    different_c_index   = np.random.choice(range(0, self.num_classes - 1), 1)    current_c           = different_c[different_c_index[0]]    selected_path       = self.paths[self.labels == current_c]    while len(selected_path)&lt;1:        different_c_index   = np.random.choice(range(0, self.num_classes - 1), 1)        current_c           = different_c[different_c_index[0]]        selected_path       = self.paths[self.labels == current_c]    #   随机选择一张    image_indexes       = np.random.choice(range(0, len(selected_path)), 1)    image               = cvtColor(Image.open(selected_path[image_indexes[0]]))            image = resize_image(image, [self.input_shape[1], self.input_shape[0]], letterbox_image = True)    image = preprocess_input(np.array(image, dtype='float32'))    image = np.transpose(image, [2, 0, 1])    images[2, :, :, :]  = image    labels[2]           = current_c    return images, labels这样每次从dataloader拿到的数据是[B,3,C,H,W]，为了与backbone适配，会做一个concat，变成[B*3,C,H,W]def dataset_collate(batch):    ''' 将anchor、pos、neg拼到一起        [batch,3,c,h,w] =&gt; [3*batch,c,h,w]    '''    images = []    labels = []    for img, label in batch:        images.append(img)        labels.append(label)    images1 = np.array(images)[:, 0, :, :, :]    images2 = np.array(images)[:, 1, :, :, :]    images3 = np.array(images)[:, 2, :, :, :]    images = np.concatenate([images1, images2, images3], 0)        labels1 = np.array(labels)[:, 0]    labels2 = np.array(labels)[:, 1]    labels3 = np.array(labels)[:, 2]    labels = np.concatenate([labels1, labels2, labels3], 0)        images  = torch.from_numpy(np.array(images)).type(torch.FloatTensor)    labels  = torch.from_numpy(np.array(labels)).long()    return images, labels代码官方的代码是tensorflow实现的，这里推荐这个第三方实现pytorch，代码很规范值得学习总结      FaceNet的思路很简洁，人脸图像只需要做crop理论上就可以做有效的训练，换用表达能力更好的backbone几乎肯定能进一步提升精度        但FaceNet也存在训练时间太长，对特殊样本（如夸张的笑）效果不佳的问题，这些都会对算法落地带来挑战        以FaceNet为代表的深度学习框架极大提升了人脸识别的精度，但同时也让我有一种担忧：人脸是否其实存在某种特征范式？为什么会有脸盲症（如果人脸不存在独特的特征范式，脸盲症的病人应该会丧失大部分物体的分辨能力才对）？CNN得到的embedding可能包含了部分这种特征范式，但如果我们能从数学上更精确的逼近它，那么人脸识别肯定会更精确。从这个角度来说，深度学习会不会在未来让人类科技陷入某种瓶颈或者混沌（毕竟人是懒惰的，可以用就好了）？  文献【1】Schroff F, Kalenichenko D, Philbin J. Facenet: A unified embedding for face recognition and clustering[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 815-823."
  },
  
  {
    "title": "【人脸识别】（1）MTCNN算法",
    "url": "/posts/%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB-1-MTCNN%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, 人脸识别",
    "tags": "人脸识别",
    "date": "2022-06-20 00:00:00 +0000",
    





    
    "snippet": "MTCNN【1】是人脸检测领域应用较广的算法，也是很多人脸识别算法的前置算法MTCNN基于级联的3个网络实现人脸框和眼、口、鼻的定位图1. MTCNN人脸检测网络结构图2. MTCNN网络结构      MTCNN由PNet、RNet和ONet级联组成，三个网络的输出都包含人脸prob、人脸rect、人脸landmark（眼、鼻、口定位）        RNet的输入是根据PNet输出的人脸...",
    "content": "MTCNN【1】是人脸检测领域应用较广的算法，也是很多人脸识别算法的前置算法MTCNN基于级联的3个网络实现人脸框和眼、口、鼻的定位图1. MTCNN人脸检测网络结构图2. MTCNN网络结构      MTCNN由PNet、RNet和ONet级联组成，三个网络的输出都包含人脸prob、人脸rect、人脸landmark（眼、鼻、口定位）        RNet的输入是根据PNet输出的人脸rect做crop得到的图像，ONet的输入是根据RNet输出的人脸rect做crop得到的图像  从图2可以看到，从PNet到RNet再到ONet，网络逐渐变深，级联的意义就在于：前面的网络做初筛得到大概的值，后面的网络做精调输出精确的结果图3. MTCNN级联效果对比图3(a)是仅使用PNet输出的结果，(b)是PNet+RNet输出的结果，(c)是PNet+RNet+ONet输出的结果PNetPNet网络（图2）被设计为全卷积网络，因此输入图像的尺寸不需要限定class PNetModel(tf.keras.Model):    ''' pnet define        默认尺寸: [B, 12, 12, 3]    '''    def __init__(self):        super(PNetModel, self).__init__()        self.conv3_10 = common.convolutional([3, 3, 10])        self.conv3_16 = common.convolutional([3, 10, 16])        self.conv3_32 = common.convolutional([3, 16, 32])                                           self.prob     = common.convolutional([1, 32, 2], activate=False)         self.bbox     = common.convolutional([1, 32, 4], activate=False)         self.landmark = common.convolutional([1, 32, 10], activate=False)                                           self.pool    = tf.keras.layers.MaxPool2D(pool_size=(3,3),                                      strides=(2,2),                                      padding='same')        self.softmax = tf.keras.layers.Softmax(axis=-1)    def call(self, inputs):        conv1 = self.conv3_10(inputs)        pool1 = self.pool(conv1)        conv2 = self.conv3_16(pool1)        conv3 = self.conv3_32(conv2)        prob = self.prob(conv3)        prob = self.softmax(prob)        bbox = self.bbox(conv3)        landmark = self.landmark(conv3)        return prob, bbox, landmark      需要注意的是，为了实现不同输入尺寸的兼容，激活函数的参数数量要与卷积核数量保持一致（在tensorflow中通过shared_axes设定，在pytorch中通过num_parameters指定）        对于$12\\times 12$的输入，PNet的输出是prob:[1,1,2]、bbox:[1,1,4]、ldmk:[1,1,10]，这里prob表示$12\\times 12$的图像是不是人脸，初始的人脸框就是[0,0,11,11]，bbox就是对初始人脸框的修正值  def generate_bbox(prob, xyxy, scale, choosed_pos):    ''' generate bbox based on prob        prob: [H, W]        xyxy: [H, W, 4]        scale: float32        return: [nums, 9]    '''    STRIDE   = 2    CELLSIZE = 12        # offset    dx1, dy1, dx2, dy2 = [xyxy[choosed_pos[0], choosed_pos[1], i] for i in range(4)]    xyxy = np.array([dx1, dy1, dx2, dy2]) # 4 x nums    score = prob[choosed_pos[0], choosed_pos[1]]    boundingbox = np.vstack([np.round((STRIDE * choosed_pos[1]) / scale),                             np.round((STRIDE * choosed_pos[0]) / scale),                             np.round((STRIDE * choosed_pos[1] + CELLSIZE) / scale),                             np.round((STRIDE * choosed_pos[0] + CELLSIZE) / scale),                             score,                             xyxy])    return boundingbox.Tdef convolutional(filters_shape, activate=True):    ''' PNet接受任意尺寸的输入        因此激活函数只针对每个卷积核(shared_axes)，同时不使用batchnormal        filters_shape: (kernel_sz, C, kernel_num)    '''    if activate: fn_activate = tf.keras.layers.PReLU(shared_axes=[1,2])    else:        fn_activate = None    conv = tf.keras.layers.Conv2D(filters=filters_shape[-1],                                   kernel_size=filters_shape[0],                                  strides=1,                                  padding='valid',                                  activation=fn_activate,                                  use_bias=True,                                  kernel_regularizer=tf.keras.regularizers.l2(0.0005),                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),                                  bias_initializer=tf.constant_initializer(0.))        return conv在训练的时候，PNet的输入尺寸被统一到$12\\times 12$；在推理的时候会将原始图片做缩放组成图像金字塔，实现多尺度检测RNetRNet网络包含全连接层，要求输入为$24\\times 24$def RNet(input_data):    ''' rnet define        input_data: [B, 24, 24, 3]    '''    conv = common.convolutional((3, 3, 28))(input_data)    pool = tf.keras.layers.MaxPool2D(pool_size=(3,3),                                      strides=(2,2),                                      padding='same')(conv)    conv = common.convolutional((3, 3, 48))(pool)    pool = tf.keras.layers.MaxPool2D(pool_size=(3,3),                                      strides=(2,2),                                      padding='valid')(conv)    conv = common.convolutional((2, 2, 64))(pool)    fc   = common.fully_connect(conv, 128, True)    prob = common.fully_connect(fc, 2)    prob = tf.keras.layers.Softmax(axis=-1)(prob)    bbox = common.fully_connect(fc, 4)    landmark = common.fully_connect(fc, 10)    return prob, bbox, landmarkONetONet网络要求输入为$48\\times 48$def ONet(input_data):    ''' onet define        input_data: [B, 48, 48, 3]    '''    conv = common.convolutional((3, 3, 32))(input_data)    pool = tf.keras.layers.MaxPool2D(pool_size=(3,3),                                      strides=(2,2),                                      padding='same')(conv)    conv = common.convolutional((3, 3, 64))(pool)    pool = tf.keras.layers.MaxPool2D(pool_size=(3,3),                                      strides=(2,2),                                      padding='same')(conv)    conv = common.convolutional((3, 3, 64))(pool)    pool = tf.keras.layers.MaxPool2D(pool_size=(2,2),                                      strides=(2,2),                                      padding='same')(conv)    conv = common.convolutional((2, 2, 128))(pool)    fc   = common.fully_connect(conv, 256, True)    prob = common.fully_connect(fc, 2)    prob = tf.keras.layers.Softmax(axis=-1)(prob)    bbox = common.fully_connect(fc, 4)    landmark = common.fully_connect(fc, 10)    return prob, bbox, landmarkMTCNN训练及推理这里以wider-face数据集和facepoint数据集为基础（前者仅包含人脸rect信息，后者包含landmark信息），生成MTCNN的训练数据wider-face &amp; facepoint数据集wider-face数据集图片素材选自WIDER数据集，对其中的人脸做了标注。wider-face根据事件场景的类型分为了61个类,根据每个类别按照40%、10%、50%的比例划分到训练集，验证集以及测试集中wider-face的标注文件wider_face_train_bbx_gt.txt内容为# 行1：图片路径# 行2：该图片标注的人脸数量# 行3：一个人脸标注#      x, y, w, h, blur, expression, illumination, invalid, occlusion, pose0--Parade/0_Parade_marchingband_1_849.jpg1449 330 122 149 0 0 0 0 0 0facepoint数据集包含了landmark标注信息和人脸rect信息，在MTCNN训练中仅使用其landmark信息facepoint的标注文件trainImageList.txt内容为# 图片路径, x1, x2, y1, y2, leye_x, leye_y, reye_x, reye_y, nose_x, nose_y, lmouse_x, lmouse_y, rmouse_x, rmouse_ylfw_5590\\Aaron_Eckhart_0001.jpg 84 161 92 169 106.250000 107.750000 146.750000 112.250000 125.250000 142.750000 105.250000 157.750000 139.750000 161.750000需要注意的是landmark数据需要根据人脸rect做尺度归一化def normalize_landmark(landmark, bbox):    ''' 人脸5个点根据bbox做归一化        landmark: np 5*2        bbox:     np 4 (x,y,w,h)    '''    landmark_normed = np.zeros((5,2), dtype=np.float32)    for idx, mark in enumerate(landmark):        landmark_normed[idx] = ((mark[0]-bbox[0])/bbox[2],                                 (mark[1]-bbox[1])/bbox[3])    return landmark_normed训练数据生成PNet训练时从wider-face中拿人脸rect数据，从facepoint中拿人脸landmark数据（rect数据和landmark数据是不相关的，loss也是分别计算的），对其做随机crop、shift、rotate等数据增强，然后与gt做iou，大于既定阈值1的为pos，处于阈值1和阈值2之间的为part，小于阈值2的为neg，最后对图片resize到$12\\times 12$RNet训练时还是以wider-face和facepoint为基础，但wider-face数据首先通过训练好的PNet，生成一系列rect后做nms+crop再喂给RNet做训练，landmark数据还是从facepoint中拿原始的ONet训练时与RNet类似，wider-face数据首先通过训练好的PNet，生成一系列rect后做nms+crop再给RNet，后者生成一系列rect，再做nms+crop喂给ONet做训练loss定义MTCNN中一共backward三种loss：      其中loss_cls仅计算pos和neg样本（忽略part样本可以降低分类难度），使用交叉熵loss        其中loss_bbox仅计算pos和part样本（只有这2类样本有bbox），使用L2 loss        其中loss_ldmk仅针对landmark数据样本，使用L2 loss  需要注意的是，以上3种Loss的最终输出都是原始loss的topk，这样在训练中更多的关注大的损失def loss_net(prob, prob_t, bbox, bbox_t, landmark, landmark_t):   ''' calc pnet/rnet/onet loss    '''   loss_cls  = get_cls_loss(prob, prob_t)   loss_bbox = get_bbox_loss(bbox, bbox_t, prob_t)   loss_ldmk = get_landmark_loss(landmark, landmark_t, prob_t)   acc_cls = get_cls_accuracy(prob, prob_t)   return loss_cls, loss_bbox, loss_ldmk, acc_cls推理推理过程与训练数据生成类似，RNet的输入是PNet对原始图像做金字塔后的bbox输出def infer(self, np_img, step=3):    ''' mtcnn infer                np_img: [H, W, 3]        step:   执行pnet / pnet+rnet / pnet+rnet+onet    '''    bboxes_pnt, landmark = pnet_scales(self.model_pnet, np_img)    if bboxes_pnt is None: return None, None    # 仅看pnet出来的结果    if step &lt;= 1: return bboxes_pnt, landmark    bboxes_pnt, landmark = rnet_scales(self.model_rnet, np_img, bboxes_pnt)    if bboxes_pnt is None: return None, None    # 仅看pnet+rnet出来的结果    if step &lt;= 2: return bboxes_pnt, landmark    # 注释掉onet部分可以看pnet+rnet的效果    bboxes_pnt, landmark = onet_scales(self.model_onet, np_img, bboxes_pnt)    if bboxes_pnt is None: return None, None    return bboxes_pnt, landmark从上述代码可以看到，PNet、RNet和ONet的输出都有landmark，但实际上PNet和Rnet的landmark输出没有意义：有文章认为PNet和RNet的landmark可以辅助bbox的训练，但从loss_net函数可以看到网络的三种loss是不相关的，参与训练的数据也各不相同，因此理论上landmark不参与训练也不影响最终精度在实际的训练中本文测试了将PNet和RNet的landmark loss权重改为0，最终MTCNN的表现不受影响代码实现官方代码参考第三方的tensorflow(v1)实现，实现了tensorflow2 + torch版本文献【1】Zhang K, Zhang Z, Li Z, et al. Joint face detection and alignment using multitask cascaded convolutional networks[J]. IEEE signal processing letters, 2016, 23(10): 1499-1503."
  },
  
  {
    "title": "【CNN系列目标检测】（10）YOLOX算法",
    "url": "/posts/CNN%E7%B3%BB%E5%88%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-10-YOLOX%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, CNN系列目标检测",
    "tags": "CNN系列目标检测",
    "date": "2022-04-20 00:00:00 +0000",
    





    
    "snippet": "YOLOX【1】是2021年旷视提出的改进算法，其精度和速度都超过了YOLOv5（YOLOX-L在单张Tesla V100上能达到68.9FPS）官方开源了代码，并提供了ONNX、TensorRT、NCNN及OpenVINO版本本人基于官方代码整理实现了自己的版本图1. YOLOX与YOLOv5性能对比  与YOLOv[1-5]相比，YOLOX最大的不同在：解耦合头部、Anchor-free...",
    "content": "YOLOX【1】是2021年旷视提出的改进算法，其精度和速度都超过了YOLOv5（YOLOX-L在单张Tesla V100上能达到68.9FPS）官方开源了代码，并提供了ONNX、TensorRT、NCNN及OpenVINO版本本人基于官方代码整理实现了自己的版本图1. YOLOX与YOLOv5性能对比  与YOLOv[1-5]相比，YOLOX最大的不同在：解耦合头部、Anchor-free  此外YOLOX使用Mosaic和MixUp两种数据增强，认为可以不再需要ImageNet的预训练了  YOLOX认为YOLOv4和YOLOv3针对Anchor-based做了很多优化，因此以YOLOv3为基础做改进参考《简明扼要理解YOLOX》网络结构图2. YOLOX-Darknet53backbone &amp; neck      YOLOX的backbone采用darknet53，使用SPP实现多尺度特征的融合，扩大感受野        YOLOX的neck与YOLOv4一样使用FPN和PAFPN提升多尺度检测性能        YOLOX的默认激活函数是SiLU（也称为Swish），在深层网络中效果优于YOLOv3和YOLOv4使用的Leaky ReLU  # yolox/exp/yolox_base.pyin_channels = [256, 512, 1024]backbone = YOLOPAFPN(self.depth, self.width, in_channels=in_channels, act=self.act)head = YOLOXHead(self.num_classes, self.width, in_channels=in_channels, act=self.act)self.model = YOLOX(backbone, head)head解耦合头部1）YOLOX的输出也是3种分辨率的特征图（$20\\times 20$、$40\\times 40$、$80\\times 80$）2）YOLOv3和YOLOv4对每个“格子”直接预测耦合在一起的信息：\\[\\begin{aligned}(cls+x+y+w+h+prob)\\times anchors=&amp;(80+1+1+1+1+1)\\times 3\\\\=&amp;255\\end{aligned}\\]因此输出的是\\[\\begin{aligned}20\\times 20\\times 255\\\\40\\times 40\\times 255\\\\80\\times 80\\times 255\\end{aligned}\\]3）YOLOX则对3种分辨率将预测信息分开输出（cls：80，reg：4，obj：1）图3. 对每种分辨率分开输出预测信息因此输出的是\\[\\begin{aligned}20\\times 20\\times 80、20\\times 20\\times 1、20\\times 20\\times 4\\\\40\\times 40\\times 80、40\\times 40\\times 1、40\\times 40\\times 4\\\\80\\times 80\\times 80、80\\times 80\\times 1、80\\times 80\\times 4\\end{aligned}\\]图4是文中YOLOX与YOLOv3的头部对比图4. 解耦合头部4）之所以YOLOX将输出解耦合，是因为在YOLOv3上实验发现解耦合后AP从38.5上升到39.6，并且收敛速度也加快了图5. YOLOv3上解耦合性能对比测试5）与YOLOv3不一样，YOLOX最后会将3种分辨率的输出按比例还原并concat到一起，最终是一个输出$8400\\times 85$图6. YOLOv3(左)与YOLOX(右)最终输出的对比Anchor-free关于Anchor-based和Anchor-free的讨论见《anchor-free存在什么缺点？》  Anchor-based和Anchor-free都是基于网格的思路，没有解决目标检测中正负样本严重失衡的问题  Anchor-based通过统计的方法提前确定目标的尺寸(anchor)，网络学习基于这些anchor的偏移量，因此训练起来更稳定  Anchor-free在没有任何先验知识的情况下学习，训练难度更大  Anchor-based需要提前对场景数据做统计，超参数多，Anchor-free超参数更少  对于训练数据中同一个网格有多个目标的情况，Anchor-based可以将其分别放在不同的anchor里，Anchor-free则只能靠FPN等方式被动学习Anchor-based方法Anchor-based的方法首先会通过聚类等方法确定场景数据涉及的目标尺度及宽高# YOLOv3# scale YOLO.STRIDES              = [8, 16, 32]# width, height1.25,1.625,   2.0,3.75,     4.125,2.875, 1.875,3.8125, 3.875,2.8125, 3.6875,7.4375, 3.625,2.8125, 4.875,6.1875, 11.65625,10.1875YOLOv3设置了3种尺度（8, 16, 32），每种尺度下通过聚类得到了3组宽高(anchors)对于训练数据，会计算label中框的尺寸与各scale对应anchor的IOU，如果大于既定阈值则认为该label属于对应anchor# YOLOv3 dataset处理for i in range(3):    anchors_xywh = np.zeros((self.anchor_per_scale, 4))    anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5    # self.anchors 对应上面的3组宽高    anchors_xywh[:, 2:4] = self.anchors[i]    # 对label做scale后与anchor做iou    iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)    iou.append(iou_scale)    iou_mask = iou_scale &gt; 0.3    if np.any(iou_mask):        xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32)        # 如果iou满足要求，则将label赋给对应anchor        # 这里label的size为: [3][out_h, out_w, 3, 255]        #                 3种scale           3组宽高        label[i][yind, xind, iou_mask, :] = 0        label[i][yind, xind, iou_mask, 0:4] = bbox_xywh        label[i][yind, xind, iou_mask, 4:5] = 1.0        label[i][yind, xind, iou_mask, 5:] = smooth_onehot        bbox_ind = int(bbox_count[i] % self.max_bbox_per_scale)        bboxes_xywh[i][bbox_ind, :4] = bbox_xywh        bbox_count[i] += 1        exist_positive = TrueAnchor-free方法  Anchor-based方法在计算loss的时候直接计算对应网格对应anchor的损失# anchor-based(yolov3)方法label和pred形式都是对应网格对应anchor的信息pred_xywh     = pred[:, :, :, :, 0:4]pred_conf     = pred[:, :, :, :, 4:5]label_xywh    = label[:, :, :, :, 0:4]respond_bbox  = label[:, :, :, :, 4:5]  Anchor-free方法因为没有anchor限定，因此首先需要将预测的框与所有label做匹配，最后计算损失# anchor-free(yolox)方法中label和pred形式不一致bbox_preds = outputs[:, :, :4]               # [batch, 8400,  4]obj_preds  = outputs[:, :, 4].unsqueeze(-1)  # [batch, 8400,  1]cls_preds  = outputs[:, :, 5:]               # [batch, 8400, 80]# labels: [batch, 120, 5] cls, xc,yc,w,hlabel匹配1) 粗筛选根据ground truth中的位置获得输出结果中可能的有效位置这一步仅考虑ground truth，跟预测的结果没关系# fg_mask中对应图片上所有标注框(OR)包含的点为true# fg_mask: [8400]# bboxes_preds_per_image: [8400, 4]fg_mask, is_in_boxes_and_center = self.get_in_boxes_info(                                 gt_bboxes_per_image,                                 expanded_strides,                                 x_shifts,                                 y_shifts,                                 total_num_anchors,                                 num_gt,                                )get_in_boxes_info中用了两种衡量方式：  输出特征图中处于gt box内的点，认为是可能的有效位置，定义为is_in_boxes  输出特征图中，以gt box中心为原点，半径2.5内的点，认为是可能的有效位置，定义为is_in_centers函数输出为fg_mask                = is_in_boxes | is_in_centersis_in_boxes_and_center = is_in_boxes &amp; is_in_centers2) 计算基于粗筛选结果的costcost = (        pair_wise_cls_loss        + 3.0 * pair_wise_ious_loss        + 100000.0 * (~is_in_boxes_and_center)       )      其中~is_in_boxes_and_center是背景区域        其中pair_wise_ious_loss是ground truth的框与预测结果经过粗筛选后的iou损失  # 根据fg_mask对预测结果做粗筛bboxes_preds_per_image = bboxes_preds_per_image[fg_mask]# 对每个gt，计算其与每个pred的iou# gt_bboxes_per_image:    [N_gt, 4]# bboxes_preds_per_image: [N_pd, 4]# pair_wise_ious:         [N_gt, N_pd]pair_wise_ious = bboxes_iou(gt_bboxes_per_image, bboxes_preds_per_image, False)3) SimOTA匹配根据第1步得到的粗筛fg_mask、第2步得到的cost和pair_wise_ious得到最终的匹配结果# 匹配的结果没有改变pred的顺序（gt第多少个与pred第[0,1,2,...]匹配）# 会修改fg_mask的值及尺寸，首先根据本身的mask压缩尺寸（fg_mask[fg_mask.clone()]），然后将匹配后的mask赋值(    num_fg,                   # gt与pred匹配上的框的数量    gt_matched_classes,       # gt与pred匹配上的类别（长度num_fg）    pred_ious_this_matching,  # gt与pred匹配上的框的ious值（长度num_fg）    matched_gt_inds,          # 匹配上的gt的序号（长度num_fg）) = self.dynamic_k_matching(cost, pair_wise_ious, gt_classes, num_gt, fg_mask)  STEP1: 设置候选框数量# matching_matrix: [N_gt, N_pd] 表示gt中第i个与pred中第j个匹配# pair_wise_ious:  [N_gt, N_pd] 表示gt中第i个与pred中第j个的iou# topk_ious:       [N_gt,   10] 表示gt与pred中iou前10matching_matrix = torch.zeros_like(cost, dtype=torch.uint8)ious_in_boxes_matrix = pair_wise_iousn_candidate_k = min(10, ious_in_boxes_matrix.size(1))topk_ious, _ = torch.topk(ious_in_boxes_matrix, n_candidate_k, dim=1)  STEP2: 根据COST挑选候选框# 对每个gt，挑选与其cost最小的前K个pred结果# 对每个gt框，根据其在topk_ious中与所有10个候选框iou的总和动态确定K的数量dynamic_ks = torch.clamp(topk_ious.sum(1).int(), min=1)dynamic_ks = dynamic_ks.tolist()for gt_idx in range(num_gt):    _, pos_idx = torch.topk(        cost[gt_idx], k=dynamic_ks[gt_idx], largest=False    )    matching_matrix[gt_idx][pos_idx] = 1图7. 图示根据COST挑选候选框上图来源《简明扼要理解YOLOX》，假定gt有3个目标，第1步粗筛选得到10个预测目标将topk_ious按第1维相加后确定gt的3个目标分别在cost找前3、4、3个最小的预测目标这种确定K的方式逻辑是：如果某个gt目标有更多的预测框与其iou较大，说明该gt目标可能与更多的预测结果关联，需要在更大范围内查找  STEP3: 过滤共用的候选框经过第2步得到的matching_matrix，可能存在某个预测结果同时与多个gt目标关联图8. 图示多个pred与同一个gt关联假定gt有3个目标，pred总共1000个，按照第2步的结果，分别在pred里找了3、4、3个cost最小的结果可以发现pred里第5个结果同时与gt的第1和第2个关联因此需要比较pred中第5个结果分别与gt第1和第2个目标的cost，保留更小的那个cost作为最终关联# anchor_matching_gt: [N_pd]，大于1的那一列表示对应的pred有共用情况anchor_matching_gt = matching_matrix.sum(0)if (anchor_matching_gt &gt; 1).sum() &gt; 0:    _, cost_argmin = torch.min(cost[:, anchor_matching_gt &gt; 1], dim=0)    matching_matrix[:, anchor_matching_gt &gt; 1] *= 0    matching_matrix[cost_argmin, anchor_matching_gt &gt; 1] = 1# 最后根据匹配的结果更新下fg_maskfg_mask_inboxes = matching_matrix.sum(0) &gt; 0num_fg = fg_mask_inboxes.sum().item()fg_mask[fg_mask.clone()] = fg_mask_inboxes输出loss计算YOLOX一共考虑了3种loss：loss_iou、loss_obj、loss_clsreg_weight = 5.0loss = reg_weight * loss_iou + loss_obj + loss_cls + loss_l1从代码来看，loss_iou和loss_cls应用了匹配结果fg_mask，但loss_obj还是用的原始输出loss_iou = (            # self.iou_loss实现了iou和giou两种            self.iou_loss(bbox_preds.view(-1, 4)[fg_masks], reg_targets)           ).sum() / num_fgloss_obj = (            # self.bcewithlog_loss与F.binary_cross_entropy_with_logits一样            self.bcewithlog_loss(obj_preds.view(-1, 1), obj_targets)           ).sum() / num_fgloss_cls = (            self.bcewithlog_loss(                 cls_preds.view(-1, self.num_classes)[fg_masks], cls_targets            )           ).sum() / num_fg数据增强Mosaic增强每次将4张图像随机缩放、随机裁减、随机排列，组合成一张图像可以有效提升小样本的检测效果图9. Mosaic数据增强MixUp增强将两张图像做加权融合，组合成一张图像图10. MixUp数据增强文献【1】Ge Z, Liu S, Wang F, et al. Yolox: Exceeding yolo series in 2021[J]. arXiv preprint arXiv:2107.08430, 2021."
  },
  
  {
    "title": "目标检测回归损失",
    "url": "/posts/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1/",
    "categories": "深度学习",
    "tags": "",
    "date": "2022-04-07 00:00:00 +0000",
    





    
    "snippet": "      损失类型    提出时间    特点        Smooth L1 Loss【1】    2015[Fast R-CNN]    将目标框的4个参数视为独立的，累加成一个值计算loss        IoU Loss【2】    2016    将目标框的4个参数看成一个整体回归，是Smooth L1的改进        GIoU Loss【3】    2019    增加最...",
    "content": "      损失类型    提出时间    特点        Smooth L1 Loss【1】    2015[Fast R-CNN]    将目标框的4个参数视为独立的，累加成一个值计算loss        IoU Loss【2】    2016    将目标框的4个参数看成一个整体回归，是Smooth L1的改进        GIoU Loss【3】    2019    增加最小包围框的计算，是Iou Loss的改进        DIoU Loss【4】    2020    增加中心距离的计算，是GIou Loss的改进        CIoU Loss【4】    2020    增加长宽比的计算，是DIou Loss的改进  Smooth L1 Loss这是Fast R-CNN【1】中提出的方法令预测框为$A=(t_x,t_y,t_w,t_h)$，真实框为$B=(v_x,v_y,v_w,v_h)$对参数$x$，有\\[\\begin{aligned}L_1=&amp;|t_x-v_x|\\\\L_2=&amp;(t_x-v_x)^2\\\\smooth_{L1}=&amp;\\begin{cases}0.5(t_x-v_x)^2&amp;|t_x-v_x|&lt;1\\\\|t_x-v_x|-0.5&amp;otherwise\\end{cases}\\end{aligned} \\tag{1}\\]根据式1可得其导数为\\[\\begin{aligned}\\delta L_1=&amp;\\begin{cases}1&amp;(t_x-v_x)\\leq 0\\\\-1&amp;otherwise\\end{cases}\\\\\\delta L_2=&amp;2(t_x-v_x)\\\\\\delta smooth_{L1}=&amp;\\begin{cases}t_x-v_x&amp;|t_x-v_x|&lt;1\\\\\\pm 1&amp;otherwise\\end{cases}\\end{aligned} \\tag{2}\\]可以看到  $L_1$损失的导数为常数，在训练后期$abs(t_x-v_x)$很小时，如果lr不变，$L_1$会在稳定值附近波动，无法收敛到更高精度  $L_2$损失的导数当$abs(t_x-v_x)$很大时也比较大，导致训练初期不稳定  $smooth_{L1}$相当于结合了上述两种损失的长处，避开了其缺点对一个框，$smooth_{L1}$会依次计算$(x,y,w,h)$的损失，然后将其相加得到最终的回归损失缺点  同一个累加的损失可能对应不同的框重叠情况，导致优化的方向不确定  $(x,y,w,h)$具有一定相关性，但$smooth_{L1}$忽略了这点  基于$L_1$和$L_2$的距离损失对尺度不具有不变性IoU LossIoU Loss【2】是旷视2016年提出的方法，取值范围为$[0,1]$简单来说IoU就是计算两个框交集与并集之比\\[IoU=-ln(\\frac{A\\cap B}{A\\cup B}) \\tag{3}\\]为了降低计算量可以将其写作\\[IoU=1-\\frac{A\\cap B}{A\\cup B} \\tag{4}\\]缺点  当预测框与真实框不相交时$IoU=0$，此时无法反映两者的距离远近，并且损失函数不可导  不同的相交形式可能对应同一个IoU值图1. 以上三种重叠方式IoU均一致GIoU LossGIoU Loss【3】解决了IoU中不相交框无法度量的问题，取值范围为$[-1,1]$\\[GIoU=1-IoU+\\frac{C-A\\cup B}{C} \\tag{5}\\]其中$C$是$A$与$B$的最小包围框，显然当两框不相交时GIoU是负数缺点  当两个框为包含关系时，GIoU退化为IoU图2. GIoU无法处理包含情况DIoU LossDIoU(Distance-IoU) Loss【4】解决了GIoU中框包含情况退化的问题\\[DIoU=1-IoU+\\frac{\\rho^2(A,B)}{d^2} \\tag{6}\\]其中$\\rho^2(A,B)$表示两个框中心的距离平方，$d$表示两个框最小包围框（也就是$C$）的对角线长度图3. DIoU考虑中心点距离CIoU LossCIoU(Complete-IoU) Loss【4】在DIoU的基础上考虑了长宽比\\[DIoU=1-IoU+\\frac{\\rho^2(A,B)}{d^2}+\\alpha b \\tag{7}\\]其中\\[\\begin{aligned}b=&amp;\\frac{4}{\\pi^2}\\left(arctan\\frac{t_w}{t_h}-arctan\\frac{v_w}{v_h}\\right)^2\\\\\\alpha =&amp;\\frac{b}{(1-IoU)+b}\\end{aligned} \\tag{8}\\]评估文章【4】在VOC2007上对各种loss做了精度评估可以看到，GIoU比IoU提升了2.49%，DIoU比IoU提升了3.29%，CIoU比IoU提升了5.67%图4. VOC2007的精度评估其中CIoU(D)是DIoU-NMS，在YoloV4中代替常规的NMS解决相邻或者重叠的多个目标被误抑制的问题  常规的NMS是比较不同框的IoU，如果大于某阈值，则只保留得分最高的那个框  但如果不同目标在图像中很近或者重叠，常规NMS会抑制掉得分低的那个造成漏检  DIoU-NMS的思路是不仅考虑IoU，还考虑中心距离(而DIoU恰好考虑了两者)，如果IoU较大且距离较大则认为是两个不同的目标，不再过滤文献【1】Girshick R, Donahue J, Darrell T, et al. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation[C]// Computer Vision and Pattern Recognition. IEEE, 2013:580-587.【2】Yu J ,  Jiang Y ,  Wang Z , et al. UnitBox: An Advanced Object Detection Network[J]. ACM, 2016.【3】Rezatofighi H, Tsoi N, Gwak J Y, et al. Generalized intersection over union: A metric and a loss for bounding box regression[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 658-666.【4】Zheng Z, Wang P, Liu W, et al. Distance-IoU loss: Faster and better learning for bounding box regression[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(07): 12993-13000."
  },
  
  {
    "title": "【点云深度学习】(4) PointPillars(2019) 代码分析",
    "url": "/posts/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-PointPillars%E4%BB%A3%E7%A0%81%E5%88%86%E6%9E%90/",
    "categories": "点云处理",
    "tags": "点云深度学习",
    "date": "2022-03-24 00:00:00 +0000",
    





    
    "snippet": "本文参考基于tensorflow的第三方实现数据集代码使用kitti 3D目标检测数据集，数据集使用方式见《KITTI 3D目标检测数据集使用》网络输入数据的生成  作者在C++里实现原始点云数据到pillar形式数据[1, maxPillars, maxPointsPerPillar, 7]的转换  使用pybind11将C++代码编译为python可以调用的库文件（见《C++ &amp;...",
    "content": "本文参考基于tensorflow的第三方实现数据集代码使用kitti 3D目标检测数据集，数据集使用方式见《KITTI 3D目标检测数据集使用》网络输入数据的生成  作者在C++里实现原始点云数据到pillar形式数据[1, maxPillars, maxPointsPerPillar, 7]的转换  使用pybind11将C++代码编译为python可以调用的库文件（见《C++ &amp; Python 混合编程（6）– Python 调用 C++（pybind11）》）  在python中对所有pillars按x、y顺序稀疏映射到XYC空间中形成伪图像pillars数据生成pybind11::tuple createPillars(pybind11::array_t&lt;float&gt; points,                              int maxPointsPerPillar,                              int maxPillars,                              float xStep,                              float yStep,                              float xMin,                              float xMax,                              float yMin,                              float yMax,                              float zMin,                              float zMax,                              bool printTime = false);1）对落在[xMin, xMax]、[yMin, yMax]、[zMin, zMax]范围内的点云，按[xStep, yStep]步长确定其所属的“鸟瞰二维网格”（在KITTI激光雷达坐标系中Z方向是垂直地面的方向）2）使用std::unordered_map记录有效的pillars，这样得到的是一个“稠密”的结果，节省空间。同时用indices记录每个pillars原本的索引，方便后面”稠密-&gt;稀疏”的操作3）对落在同一个“二维网格”的点，计算其坐标均值for (auto&amp; p: pair.second){    p.xc = p.x - xMean;    p.yc = p.y - yMean;    p.zc = p.z - zMean;}4）计算每个pillar的7组特征tensor.mutable_at(0, pillarId, pointId, 0) = p.x - (xIndex * xStep + xMin);tensor.mutable_at(0, pillarId, pointId, 1) = p.y - (yIndex * yStep + yMin);tensor.mutable_at(0, pillarId, pointId, 2) = p.z - zMid;tensor.mutable_at(0, pillarId, pointId, 3) = p.intensity;tensor.mutable_at(0, pillarId, pointId, 4) = p.xc;tensor.mutable_at(0, pillarId, pointId, 5) = p.yc;tensor.mutable_at(0, pillarId, pointId, 6) = p.zc;函数输出tensor.resize({1, maxPillars, maxPointsPerPillar, 7});indices.resize({1, maxPillars, 3});原始label数据到网络输出数据的生成如《【点云深度学习】(3) PointPillars(2019)》所述，网络输出的结果为occ:     [batch_size, Xn/2, Yn/2, C]表示该结果为目标的概率loc：    [batch_size, Xn/2, Yn/2, C, 3]表示该目标的xyz坐标size：   [batch_size, Xn/2, Yn/2, C, 3]表示该目标的length、width、heightangle：  [batch_size, Xn/2, Yn/2, C]表示该目标的偏航角heading：[batch_size, Xn/2, Yn/2, C]表示该目标的朝向clf：    [batch_size, Xn/2, Yn/2, C, nb_classes]表示该目标属于各个类别的概率而原始的kitti label数据包含的是目标位置、长宽高、yaw角函数createPillarsTarget实现二者的转换std::tuple&lt;pybind11::array_t&lt;float&gt;, int, int&gt; createPillarsTarget(const pybind11::array_t&lt;float&gt;&amp; objectPositions,                                             const pybind11::array_t&lt;float&gt;&amp; objectDimensions,                                             const pybind11::array_t&lt;float&gt;&amp; objectYaws,                                             const pybind11::array_t&lt;int&gt;&amp; objectClassIds,                                             const pybind11::array_t&lt;float&gt;&amp; anchorDimensions,                                             const pybind11::array_t&lt;float&gt;&amp; anchorZHeights,                                             const pybind11::array_t&lt;float&gt;&amp; anchorYaws,                                             float positiveThreshold,                                             float negativeThreshold,                                             int nbClasses,                                             int downscalingFactor,                                             float xStep,                                             float yStep,                                             float xMin,                                             float xMax,                                             float yMin,                                             float yMax,                                             float zMin,                                             float zMax,                                             bool printTime = false)1）根据位置确定其所属的pillar// 其实就是Xn/2, Yn/2const auto xSize = static_cast&lt;int&gt;(std::floor((xMax - xMin) / (xStep * downscalingFactor)));const auto ySize = static_cast&lt;int&gt;(std::floor((yMax - yMin) / (yStep * downscalingFactor)));这里downscalingFactor=2，是因为网络的输出相对于原始“伪图像”做了下二采样occ: [batch_size, Xn/2, Yn/2, C]float objectDiameter = std::sqrt(std::pow(labelBox.width, 2) + std::pow(labelBox.length, 2));// Offset = Number of grid boxes that can fit on the object diameterconst auto x_offset = static_cast&lt;int&gt;(std::ceil(objectDiameter / (xStep * downscalingFactor)));const auto y_offset = static_cast&lt;int&gt;(std::ceil(objectDiameter / (yStep * downscalingFactor)));// Xc = Number of grid boxes that can fit between Xmin (Ymin) and Label's x (y) coordinateconst auto xC = static_cast&lt;int&gt;(std::floor((labelBox.x - xMin) / (xStep * downscalingFactor)));const auto yC = static_cast&lt;int&gt;(std::floor((labelBox.y - yMin) / (yStep * downscalingFactor)));// X(Y)Start = Start from Xc (Yc) - Number of boxes in object's diameter.// For example the object is located at 5 unites and is 2 unites long. Then X(Y)start will begin// the search from 3const auto xStart = clip(xC - x_offset, 0, xSize);const auto yStart = clip(yC - y_offset, 0, ySize);// Similarly end the search at 8 units. Because the object cannot extend beyond that.const auto xEnd = clip(xC + x_offset, 0, xSize);const auto yEnd = clip(yC + y_offset, 0, ySize);这里是通过框的对角线长度确定搜索半径x_offset，后续在[xStart, yStart] =&gt; [xEnd, yEnd]范围内搜索与anchors的最佳匹配2）搜索与anchors的最佳匹配，生成网络输出格式的label      与SSD一样，PointPillars也是anchor-based的，网络输出的框是以某一个最佳匹配的anchor为基础预测比例和偏移得到的        对当前搜索的pillar，首先确定其对应的anchors，然后计算每个anchor和label框的iou  anchorBox.x = x; // Assign the real world x and y coordinate to the anchor boxanchorBox.y = y; // Note that anchor boxes originally didn't have Xs and Ys.                 // This is because we need ot check them along the X-Y grid.                 // However, they did have a z value attached to them. const float iouOverlap = iou(anchorBox, labelBox); // Get IOU between two 3D boxes.if (maxIou &lt; iouOverlap){    maxIou = iouOverlap;    bestAnchor = anchorBox;    bestAnchorId = anchorCount;}  根据iou结果生成相应的labelif (iouOverlap &gt; positiveThreshold) // Accept the Anchor. Add the anchor details to the tensor.{    // Tensor at CurrentObject Id, xth grid cell, yth grid cell, currentAnchor, 0    tensor.mutable_at(objectCount, xId, yId, anchorCount, 0) = 1;     auto diag = anchorDiagonals[anchorCount];    tensor.mutable_at(objectCount, xId, yId, anchorCount, 1) = (labelBox.x - anchorBox.x) / diag; // delta x,y,z    tensor.mutable_at(objectCount, xId, yId, anchorCount, 2) = (labelBox.y - anchorBox.y) / diag;    tensor.mutable_at(objectCount, xId, yId, anchorCount, 3) = (labelBox.z - anchorBox.z) / anchorBox.height;    tensor.mutable_at(objectCount, xId, yId, anchorCount, 4) = std::log(labelBox.length / anchorBox.length); // delta l,w,h    tensor.mutable_at(objectCount, xId, yId, anchorCount, 5) = std::log(labelBox.width / anchorBox.width);    tensor.mutable_at(objectCount, xId, yId, anchorCount, 6) = std::log(labelBox.height / anchorBox.height);    tensor.mutable_at(objectCount, xId, yId, anchorCount, 7) = std::sin(labelBox.yaw - anchorBox.yaw); //delta yaw    if (labelBox.yaw &gt; 0) // Is yaw &gt; 0    {        tensor.mutable_at(objectCount, xId, yId, anchorCount, 8) = 1;     }    else    {        tensor.mutable_at(objectCount, xId, yId, anchorCount, 8) = 0;    }    tensor.mutable_at(objectCount, xId, yId, anchorCount, 9) = labelBox.classId;}else if (iouOverlap &lt; negativeThreshold){    tensor.mutable_at(objectCount, xId, yId, anchorCount, 0) = 0;}else{    tensor.mutable_at(objectCount, xId, yId, anchorCount, 0) = -1;}网络模块网络的输入为tensor.resize({1, maxPillars, maxPointsPerPillar, 7});indices.resize({1, maxPillars, 3});首先对pillar做一次(1,1,C)尺寸的卷积 + (1,maxPointsPerPillar)的最大池化  卷积的作用是对每个pillar中的每个点的7个原始特征做一次提取，其中C为预设anchor的数量  池化的作用是对每个pillar内所有特征做一次提取，得到pillar内的全局特征  经过上述操作后pillar尺寸变为[1, maxPillars, 1, C]然后利用tf.scatter_nd函数将其按顺序稀疏映射到(Yn,Xn)大小的伪图像上# 从稠密到稀疏# pillars =[batch_size, Xn, Yn, nb_channels] 虚拟鸟瞰图pillars = tf.keras.layers.Lambda(lambda inp: tf.scatter_nd(inp[0], inp[1],                                (batch_size,) + image_size + (nb_channels,)),                                name=\"pillars/scatter_nd\")([corrected_indices, x])接下来是网络层的处理，不赘述损失函数针对网络的6个输出，设计了对应的6种损失def losses(self):    return [self.focal_loss, self.loc_loss, self.size_loss, self.angle_loss, self.heading_loss, self.class_loss]  其中除了focal_loss，其他的损失都只针对正样本（就是label中有目标的位置）self.mask = tf.equal(y_true, 1)masked_loss = tf.boolean_mask(loss, self.mask)  在loc_loss、size_loss和angle_loss中用到了huber lossloss = tf.compat.v1.losses.huber_loss(y_true,                                    y_pred,                                    reduction=\"none\")huber loss是均方根误差(MSE)和平均绝对误差(MAE)的结合体MSE训练难度低，误差大时下降快，误差小时下降慢，但受明显偏离正常范围的离群样本的影响大MAE受离群样本影响小，但因为下降速度与误差大小无关，因此求导困难Huber会计算样本均值，当某个样本偏离均值太大则用MAE，否则用MSE  focal loss决定预测的目标是否有效对于正样本，代码计算其交叉熵下的focal loss，做为loss1对于负样本，代码取focal loss从小到大排列的后10%，做为loss2最后将loss1+loss2，取均值neg_mask = tf.equal(y_true, 0)thr = tfp.stats.percentile(tf.boolean_mask(focal_loss, neg_mask), 90.)hard_neg_mask = tf.greater(focal_loss, thr)mask = tf.logical_or(self.mask, tf.logical_and(neg_mask, hard_neg_mask))masked_loss = tf.boolean_mask(focal_loss, mask)  其中的tfp.stats.percentile函数来自于tensorflow_probability as tfp  其作用是对输入做从小到大排序，通过插值取第p%的数据TF训练框架# STEP1: 模型输入输出的声明#        输入和输出之间就是网络层的设计input_pillars = tf.keras.layers.Input(input_shape, batch_size=batch_size, name=\"pillars/input\")input_indices = tf.keras.layers.Input((max_pillars, 3), batch_size=batch_size, name=\"pillars/indices\",                                       dtype=tf.int32)occ, loc, size, angle, heading, clf = PillarsNet(input_pillars, input_indices)pillar_net = tf.keras.models.Model([input_pillars, input_indices], [occ, loc, size, angle, heading, clf])# STEP2： 损失函数、优化器的声明#         使用compile连结模型、损失函数、优化器pillar_net.load_weights(os.path.join(MODEL_ROOT, \"model.h5\"))loss = PointPillarNetworkLoss(params)optimizer = tf.keras.optimizers.Adam(lr=params.learning_rate, decay=params.decay_rate)pillar_net.compile(optimizer, loss=loss.losses())# STEP3: 训练&amp;验证数据迭代器的声明#        callback形式的学习率调整、tensorboard显示、模型文件保存training_gen = SimpleDataGenerator(...)validation_gen = SimpleDataGenerator(...)epoch_to_decay = int(        np.round(params.iters_to_decay / params.batch_size / int(np.ceil(float(len(label_files)) / params.batch_size))))callbacks = [        tf.keras.callbacks.TensorBoard(log_dir=log_dir),        tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(log_dir, \"model.h5\"),                                           monitor='val_loss', save_best_only=True),        tf.keras.callbacks.LearningRateScheduler(            lambda epoch, lr: lr * 0.8 if ((epoch % epoch_to_decay == 0) and (epoch != 0)) else lr, verbose=True),        tf.keras.callbacks.EarlyStopping(patience=20, monitor='val_loss'),    ]# STEP4: 使用fit函数开始训练pillar_net.fit(training_gen,               validation_data = validation_gen,               steps_per_epoch=len(training_gen),        # 每个epoch包含的steps                                                         # 一个epoch覆盖所有训练样本，一般为 total_sz/batch_sz               callbacks=callbacks,               use_multiprocessing=True,               epochs=int(params.total_training_epochs), # 总的epoch数               workers=1)使用compile+callback+fit的方式可以不用自己显式写for循环fit内部会每个epoch调用一次注册的callback函数"
  },
  
  {
    "title": "KITTI 3D目标检测数据集使用",
    "url": "/posts/KITTI-3D%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8/",
    "categories": "自动驾驶",
    "tags": "",
    "date": "2022-03-24 00:00:00 +0000",
    





    
    "snippet": "KITTI 3D数据集内容代码使用kitti 3D目标检测数据集图1. KITTI坐标系3D数据集包含的目录为：calib、image_2、label_2、velodynelabel_2表示标注结果是以2号相机坐标系为基准label_2数据//    id0      id1 id2  id3   id4    id5    id6     id7   id8  id9   id10 id1...",
    "content": "KITTI 3D数据集内容代码使用kitti 3D目标检测数据集图1. KITTI坐标系3D数据集包含的目录为：calib、image_2、label_2、velodynelabel_2表示标注结果是以2号相机坐标系为基准label_2数据//    id0      id1 id2  id3   id4    id5    id6     id7   id8  id9   id10 id11 id12 id13 id14   Pedestrian 0.00  0  -0.20 712.40 143.00 810.73 307.92 1.89  0.48  1.20 1.84 1.47 8.41 0.01      id    含义    取值        0    目标类别    Car、Van、Truck、Pedestrian、Person_sitting、Cyclist、Tram、Misc、DontCare        1    目标是否被截断（离开图像边界）    [0,1]，0表示非截断，目标是完整的，1表示完全截断        2    物体是否被遮挡    0：没被遮挡、1：小部分被遮挡、2：大部分被遮挡、3：完全被遮挡        3    物体观察角度    [-pi, pi]，在相机坐标系下，相机原点到目标中心的连线与相机X轴的夹角        4    目标二维边界框xmin    图像坐标系下        5    目标二维边界框ymin    图像坐标系下        6    目标二维边界框xmax    图像坐标系下        7    目标二维边界框ymax    图像坐标系下        8    目标三维边界框尺寸height    2号相机坐标系下，单位米        9    目标三维边界框尺寸width    2号相机坐标系下，单位米        10    目标三维边界框尺寸length    2号相机坐标系下，单位米        11    目标三维中心x    2号相机坐标系下，单位米        12    目标三维中心y    2号相机坐标系下，单位米        13    目标三维中心z    2号相机坐标系下，单位米        14    目标yaw角    [-pi, pi]，2号相机坐标系下，目标前进方向与相机X轴的夹角  参考官方的label数据解析代码（readLabels.m）：function objects = readLabels(label_dir,img_idx)% parse input filefid = fopen(sprintf('%s/%06d.txt',label_dir,img_idx),'r');C   = textscan(fid,'%s %f %d %f %f %f %f %f %f %f %f %f %f %f %f','delimiter', ' ');fclose(fid);% for all objects doobjects = [];for o = 1:numel(C{1})  % extract label, truncation, occlusion  lbl = C{1}(o);                   % for converting: cell -&gt; string  objects(o).type       = lbl{1};  % 'Car', 'Pedestrian', ...  objects(o).truncation = C{2}(o); % truncated pixel ratio ([0..1])  objects(o).occlusion  = C{3}(o); % 0 = visible, 1 = partly occluded, 2 = fully occluded, 3 = unknown  objects(o).alpha      = C{4}(o); % object observation angle ([-pi..pi])  % extract 2D bounding box in 0-based coordinates  objects(o).x1 = C{5}(o); % left  objects(o).y1 = C{6}(o); % top  objects(o).x2 = C{7}(o); % right  objects(o).y2 = C{8}(o); % bottom  % extract 3D bounding box information  objects(o).h    = C{9} (o); % box height  objects(o).w    = C{10}(o); % box width  objects(o).l    = C{11}(o); % box length  objects(o).t(1) = C{12}(o); % location (x)  objects(o).t(2) = C{13}(o); % location (y)  objects(o).t(3) = C{14}(o); % location (z)  objects(o).ry   = C{15}(o); % yaw angleendcalib数据P0: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 0.000000000000e+00 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 0.000000000000e+00P1: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 -3.797842000000e+02 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 0.000000000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 0.000000000000e+00P2: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 4.575831000000e+01 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 -3.454157000000e-01 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 4.981016000000e-03P3: 7.070493000000e+02 0.000000000000e+00 6.040814000000e+02 -3.341081000000e+02 0.000000000000e+00 7.070493000000e+02 1.805066000000e+02 2.330660000000e+00 0.000000000000e+00 0.000000000000e+00 1.000000000000e+00 3.201153000000e-03R0_rect: 9.999128000000e-01 1.009263000000e-02 -8.511932000000e-03 -1.012729000000e-02 9.999406000000e-01 -4.037671000000e-03 8.470675000000e-03 4.123522000000e-03 9.999556000000e-01Tr_velo_to_cam: 6.927964000000e-03 -9.999722000000e-01 -2.757829000000e-03 -2.457729000000e-02 -1.162982000000e-03 2.749836000000e-03 -9.999955000000e-01 -6.127237000000e-02 9.999753000000e-01 6.931141000000e-03 -1.143899000000e-03 -3.321029000000e-01Tr_imu_to_velo: 9.999976000000e-01 7.553071000000e-04 -2.035826000000e-03 -8.086759000000e-01 -7.854027000000e-04 9.998898000000e-01 -1.482298000000e-02 3.195559000000e-01 2.024406000000e-03 1.482454000000e-02 9.998881000000e-01 -7.997231000000e-01标注数据中的坐标信息需要根据Tr_velo_to_cam转换到雷达坐标系def transform_labels_into_lidar_coordinates(labels: List[Label3D], R: np.ndarray, t: np.ndarray):    transformed = []    for label in labels:        # R、T就是Tr_velo_to_cam的值        # python3.5之后支持 @ 矩阵乘法        label.centroid = label.centroid @ np.linalg.inv(R).T - t        # 官方label中的顺序为height、width、length，这里将其换成LWH        label.dimension = label.dimension[[2, 1, 0]]        # 官方label中的yaw是图1中相机2的X轴顺时针的角度，与激光雷达X轴相差90度        label.yaw -= np.pi / 2        # 确保yaw处于[-90,90]，因为目标的朝向会被单独分出来        while label.yaw &lt; -np.pi:            label.yaw += (np.pi * 2)        while label.yaw &gt; np.pi:            label.yaw -= (np.pi * 2)        transformed.append(label)    return labels利用label数据在velodyne中画3Dbox  首先将中心点$(x,y,z)$和框的尺寸$(l,w,h)$转换到雷达坐标系  以框中心为原点，绕Z轴旋转yaw度  将框偏移$(x,y,z)$官方提供的代码（computeBox3D.m）% compute rotational matrix around yaw axisR = [+cos(object.ry), 0, +sin(object.ry);                   0, 1,               0;     -sin(object.ry), 0, +cos(object.ry)];% 3D bounding box dimensionsl = object.l;w = object.w;h = object.h;% 3D bounding box cornersx_corners = [l/2, l/2, -l/2, -l/2, l/2, l/2, -l/2, -l/2];y_corners = [0,0,0,0,-h,-h,-h,-h];z_corners = [w/2, -w/2, -w/2, w/2, w/2, -w/2, -w/2, w/2];% rotate and translate 3D bounding boxcorners_3D = R*[x_corners;y_corners;z_corners];corners_3D(1,:) = corners_3D(1,:) + object.t(1);corners_3D(2,:) = corners_3D(2,:) + object.t(2);corners_3D(3,:) = corners_3D(3,:) + object.t(3);  需要注意的是，上述代码是基于相机坐标系下的，因此Y方向是高度方向  x_corners长度为8，表示3D框的8个顶点图2. 俯瞰视角下3D框顶点计算如图2，以$P_1$点的x坐标为例（雷达坐标系下）\\[P_1(x) = \\frac{l}{2}cos\\alpha-\\frac{w}{2}sin\\alpha\\tag{1}\\]图3. kitti数据绘制效果"
  },
  
  {
    "title": "【点云深度学习】(3) PointPillars(2019)",
    "url": "/posts/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-PointPillars/",
    "categories": "点云处理",
    "tags": "点云深度学习",
    "date": "2022-03-23 00:00:00 +0000",
    





    
    "snippet": "PointPillars【1】是SECOND【2】的改进版本不同于体素化的方法（VoxelNet、SECOND）,PointPillars首先将点云在X-Y平面（地平面）上组成一个个的柱体(Pillar)，转化为伪图片，然后用SSD网络做目标检测（anchor-based），输出的结果是目标3D框和类别PointPillars取得了速度和精度的平衡，平均检测速度为62Hz，是apollo和a...",
    "content": "PointPillars【1】是SECOND【2】的改进版本不同于体素化的方法（VoxelNet、SECOND）,PointPillars首先将点云在X-Y平面（地平面）上组成一个个的柱体(Pillar)，转化为伪图片，然后用SSD网络做目标检测（anchor-based），输出的结果是目标3D框和类别PointPillars取得了速度和精度的平衡，平均检测速度为62Hz，是apollo和autoware中的默认方法图1. PointPillars性能对比上图是CIA-SSD【3】中介绍的精度-速度对比PointPillars网络结构图2. PointPillars网络结构伪图像数据生成1）首先根据预设的[xMin, xMax]、[yMin, yMax]和[zMin, zMax]对原始点云做ROI过滤这一步的意义在于：确定pillars的范围；将地面障碍物上方的点云过滤掉（如隧道顶），否则之后转为鸟瞰视角会有影响2）根据预设的xStep和yStep和点云的x、y坐标确定每个点云所属的pillar最后得到的所有pillar尺寸为[1, maxPillars, maxPointsPerPillar, 7]  maxPillars是预设的每帧点云的pillar数目  maxPointsPerPillar是预设的每个pillar包含的点云数目  7是pillar中每个点云的特征数目每个点云的特征为：[dx, dy, dz, intensity, xc, yc, zc]  dx、dy是点云坐标(x,y)与所属pillar位置的偏差：$dx = p.x - (xIndex * xStep + xMin)$  dz是点云坐标z与[zMin, zMax]中心的偏差  intensity是点云的反射强度  xc、yc、zc是点云坐标(x,y,z)与所属pillar中所有点云坐标均值的偏差：$p.xc = p.x - xMean$3）最后将所有pillar按x、y顺序稀疏映射到XYC空间中形成伪图像首先会对pillar做一次(1,1,C)尺寸的卷积 + (1,maxPointsPerPillar)的最大池化  卷积的作用是对每个pillar中的每个点的7个原始特征做一次提取，其中C为预设anchor的数量  池化的作用是对每个pillar内所有特征做一次提取，得到pillar内的全局特征经过上述操作后pillar尺寸变为[1, maxPillars, 1, C]，将其按pillar所在的x,y放到XYC伪图像空间在XYC空间中  预设该伪图像的宽高为Xn、Yn  每个”像素点”的尺度为xStep、yStep，也就是一个pillar的尺寸  该空间是稀疏的，不是每个”像素点”上都有pillar经过稀疏映射，最终得到的伪图像尺寸为[1, Xn, Yn, C]二维CNN backbone &amp; detection head1）如图2所示，首先对输入通过卷积做3次下采样，然后通过反卷积做3次上采样，最后将3个不同尺度的特征做concatbackbone输出的尺寸为[batch_size, Xn/2, Yn/2, 6*C]2）在detection head里分别对backbone的输出做卷积生成6项输出  occ: [batch_size, Xn/2, Yn/2, C]表示该结果为目标的概率  loc：[batch_size, Xn/2, Yn/2, C, 3]表示该目标的xyz坐标  size：[batch_size, Xn/2, Yn/2, C, 3]表示该目标的length、width、height  angle：[batch_size, Xn/2, Yn/2, C]表示该目标的偏航角  heading：[batch_size, Xn/2, Yn/2, C]表示该目标的朝向  clf：[batch_size, Xn/2, Yn/2, C, nb_classes]表示该目标属于各个类别的概率如前面所述，C是预设anchor的数量，也就是说输出的结果是以预设的anchor为基础，预测相应的偏移得到最终的目标框anchor一般由一组不同尺度的目标参数组成:# length, width, height, z-center, orientation    anchor_dims = np.array([[3.9, 1.6, 1.56, -1, 0],                            [3.9, 1.6, 1.56, -1, 1.5708],                            [0.8, 0.6, 1.73, -0.6, 0],                            [0.8, 0.6, 1.73, -0.6, 1.5708],                            ], dtype=np.float32).tolist()损失函数论文将损失分为3类：位置损失、类别损失、朝向损失总损失为\\[L_{total}=\\frac{1}{N_{pos}}(\\beta_{loc}L_{loc}+\\beta_{cls}L_{cls}+\\beta_{dir}L_{dir})\\tag{1}\\]1）位置损失与SECOND【2】一致，检测框包含$(x,y,z,w,l,h,\\theta)$\\[L_{loc}=\\sum_{b\\in (x,y,z,w,l,h,\\theta)}{SmoothL1(\\Delta b)}\\tag{2}\\]其中\\[\\begin{aligned}\\Delta x=\\frac{x^{gt}-x^a}{d^a},\\Delta y&amp;=\\frac{y^{gt}-y^a}{d^a},\\Delta z=\\frac{z^{gt}-z^a}{d^a}\\\\\\Delta w=log\\frac{w^{gt}}{w^a},\\Delta l&amp;=log{l^{gt}}{l^a},\\Delta h=\\frac{h^{gt}}{h^a}\\\\\\Delta\\theta&amp;=sin(\\theta^{gt}-\\theta^a)\\end{aligned}\\tag{3}\\]其中$d^a=\\sqrt{(w^a)^2+(l^a)^2}$是预测框的对角线长度（忽略高度）2）朝向损失在位置损失中，角度被限定在$[0,90]$，将目标朝向单独抽出来可以降低优化的难度朝向问题实际就是一个二分类问题，因此应用softmax即可3）分类损失文章在这里应用了focal loss（改善样本不均衡带来的问题）\\[L_{cls}=-\\alpha_a(1-p^a)^\\gamma logp^a\\tag{4}\\]代码实现PointPillars的官方代码是基于SECOND修改的，pytorch实现这里参考基于tensorflow的第三方实现，个人感觉有很多细节值得学习详细的分析见《【点云深度学习】(4) PointPillars(2019)代码分析》文献【1】Lang A H, Vora S, Caesar H, et al. Pointpillars: Fast encoders for object detection from point clouds[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 12697-12705.【2】Yan Y, Mao Y, Li B. Second: Sparsely embedded convolutional detection[J]. Sensors, 2018, 18(10): 3337.【3】Zheng W, Tang W, Chen S, et al. CIA-SSD: Confident IoU-aware single-stage object detector from point cloud[J]. arXiv preprint arXiv:2012.03015, 2020."
  },
  
  {
    "title": "C++ & Python 混合编程（6）-- Python 调用 C++（pybind11）",
    "url": "/posts/C++-&-Python-%E6%B7%B7%E5%90%88%E7%BC%96%E7%A8%8B-6-Python-%E8%B0%83%E7%94%A8-C++-pybind11/",
    "categories": "python",
    "tags": "",
    "date": "2022-03-15 00:00:00 +0000",
    





    
    "snippet": "pybind11是一个轻量级的只包含头文件的库，用于 Python 和 C++ 之间接口转换，可以为现有的 C++ 代码创建 Python 接口绑定。其目的和语法类似于Boost.Python库，但是Boost库十分庞大而复杂，而pybind11十分轻量级pybind11对C++11支持很好（仅支持C++11以上的编译器 + python2.7以上）pybind11目前已经实现了所有主要的P...",
    "content": "pybind11是一个轻量级的只包含头文件的库，用于 Python 和 C++ 之间接口转换，可以为现有的 C++ 代码创建 Python 接口绑定。其目的和语法类似于Boost.Python库，但是Boost库十分庞大而复杂，而pybind11十分轻量级pybind11对C++11支持很好（仅支持C++11以上的编译器 + python2.7以上）pybind11目前已经实现了所有主要的Python类型到C++的转换，包括handle, object, bool_, int_, float_, str, bytes, tuple, list, dict, slice, none, capsule, iterable, iterator, function, buffer, array, array_tpybind11环境配置pybind11是 header-only的，因此不需要编译动态链接库，直接解压使用即可直接从官方github上下载源代码到文件夹pybind11函数编写与绑定  CPP文件中完成函数实现  CPP中通过PYBIND11_MODULE宏完成函数绑定  实现CmakeLists.txt  编译生成python可用的链接库参考《pybind11编译和使用》、《Python/C++混合编程利器Pybind11实践》CPP中函数实现示例两个函数add、add_c，前者实现两个int相加，后者实现两个double矩阵相加#include &lt;pybind11/pybind11.h&gt;#include &lt;pybind11/numpy.h&gt;#include &lt;omp.h&gt;#include &lt;iostream&gt;namespace py = pybind11;// func add int add(int i = 1, int j = 2) {    return i + j;}// func add_cpy::array_t&lt;double&gt; add_c(py::array_t&lt;double&gt; arr1, py::array_t&lt;double&gt; arr2) {    py::buffer_info buf1 = arr1.request(), buf2 = arr2.request();    if (buf1.shape != buf2.shape)        throw std::runtime_error(\"Input shapes must match\");    /* No pointer is passed, so NumPy will allocate the buffer */    auto result = py::array_t&lt;double&gt;(buf1);    py::buffer_info buf3 = result.request();    double* ptr1 = (double*)buf1.ptr,        * ptr2 = (double*)buf2.ptr,        * ptr3 = (double*)buf3.ptr;#pragma omp parallel for    for (ssize_t idx = 0; idx &lt; buf1.size; idx++)        ptr3[idx] = ptr1[idx] + ptr2[idx];    return result;}  #include &lt;pybind11/pybind11.h&gt; + namespace py = pybind11; 是基本操作，包含pybind11的基础部分  #include &lt;pybind11/numpy.h&gt;是为了使用pybind11::array_t类接收numpy.ndarray数组重点分析下add_c：pybind11::array_t类与numpy.ndarray数组连接，再通过`request`方法将数组解析成py::buffer_info结构体struct buffer_info {    void *ptr = nullptr;          // 指向数组数据的指针    ssize_t itemsize = 0;         // 数组元素类型占的字节数，double数组为8    ssize_t size = 0;             // 数组元素的数量，如2x3的数组，size=6    std::string format;           // 数组元素类型，double数组为'd'    ssize_t ndim = 0;             // 数组的维度，如2x3的数组，ndim=2    std::vector&lt;ssize_t&gt; shape;   // 数组每个维度的长度，行优先，如2x3的数组，shape=(2,3)    std::vector&lt;ssize_t&gt; strides; // 数组每个维度的字节长度，行优先，如2x3的double数组，strides=(8*3, 8)                                  // 每行占用24字节，每行中相邻列偏移8字节    bool readonly = false;        // flag to indicate if the underlying storage may be written to};  其中py::ssize_t就是std::ssize_t，在32位机器里就是int，在64位机器就是long int，也就是说其是带符号的size_t  numpy内数组都是行优先的，在buffer_info中也应以行优先顺序遍历  通过包含&lt;pybind11/eigen.h&gt;，pybind11也支持不大于二维的eigen，但需要显式说明引用模式，否则会默认做依次深拷贝。个人认为还不如直接用buffer_infoCPP中函数绑定一般直接在函数实现的CPP后面对想要暴露给python的函数接口做绑定PYBIND11_MODULE(model_add, m) {    m.doc() = \"pybind11 model_add-1 plugin\"; // optional module docstring    m.def(\"add\", &amp;add, \"A function which adds two numbers\",        py::arg(\"i\") = 1, py::arg(\"j\") = 2);    m.def(\"add_c\", &amp;add_c, \"A function which adds two arrays with c type\");}  model_add是要导出给python的模块名称  m.doc()是该模块的功能描述，可以省略  m.def()实现c++函数与python接口的绑定，其中py::arg()可以实现python端的默认参数功能实现CmakeLists.txt &amp; 编译cmake_minimum_required(VERSION 3.5)project(model_add)add_subdirectory(pybind11)pybind11_add_module(model_add SHARED add.cpp)  pybind11_add_module内的model_add就是生成链接库的名称，需要与`PYBIND11_MODULE`中声明的模块名保持一致，否则会导入失败  add_subdirectory指定pybind11源代码的文件夹，需要将从github下载的代码放在这里，当然也可以通过软链接实现  最后通过cmake make就可以生成可供python直接import的链接库文件了对于复杂的功能，CPP里往往要使用其他库，可以通过target_link_libraries实现以PCL为例cmake_minimum_required(VERSION 3.5)project(point_drawer)# find_package(PCL REQUIRED)find_package(Boost REQUIRED)set(PCL_INCLUDE_DIRS \"/usr/local/pcl-1.8.1/include/pcl-1.8\")set(PCL_LIBRARY_DIRS \"/usr/local/pcl-1.8.1/lib\")set(PCL_LIBRARIES pcl_common pcl_io)include_directories(${PCL_INCLUDE_DIRS})link_directories(${PCL_LIBRARY_DIRS})set(EIGEN_INCLUDE_DIRS \"/usr/include/eigen3\")include_directories(${EIGEN_INCLUDE_DIRS})add_subdirectory(pybind11)pybind11_add_module(point_drawer SHARED src/point_drawer.cpp)target_link_libraries(point_drawer PUBLIC pybind11::module ${PCL_LIBRARIES})注：这里直接用find_package(PCL REQUIRED)会导致python import 失败，原因未知：  ImportError: dynamic module does not define module export function (PyInit_libxxxx)高维数据操作参考PointPillars第三方实现，假设需要创建并赋值一个四维数据// 创建一个高维数组pybind11::array_t&lt;float&gt; tensor;tensor.resize({1, maxPillars, maxPointsPerPillar, 7});// 对这个高维数组赋值tensor.mutable_at(0, pillarId, pointId, 0) = p.x - (xIndex * xStep + xMin);tensor.mutable_at(0, pillarId, pointId, 1) = p.y - (yIndex * yStep + yMin);tensor.mutable_at(0, pillarId, pointId, 2) = p.z - zMid;tensor.mutable_at(0, pillarId, pointId, 3) = p.intensity;tensor.mutable_at(0, pillarId, pointId, 4) = p.xc;tensor.mutable_at(0, pillarId, pointId, 5) = p.yc;tensor.mutable_at(0, pillarId, pointId, 6) = p.zc;// 读取高维数组的值float px = tensor.at(0, pillarId, pointId, 0);"
  },
  
  {
    "title": "【点云深度学习】(2) PointNet++(2017)",
    "url": "/posts/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-PointNet++/",
    "categories": "点云处理",
    "tags": "点云深度学习",
    "date": "2022-03-09 00:00:00 +0000",
    





    
    "snippet": "PointNet++【1】是PointNet作者同年提出的改进方法：1）PointNet是对每个点做单独卷积，最后通过max_pool做特征聚合，结果表明其能有效提取全局特征，但忽略了不同位置的局部特征PointNet++使用sampling和grouping对点云做采样和聚类，对每个聚类点云用PointNet的方式做特征提取，最后再做池化下一层对上一层sampling的点再重复做sampl...",
    "content": "PointNet++【1】是PointNet作者同年提出的改进方法：1）PointNet是对每个点做单独卷积，最后通过max_pool做特征聚合，结果表明其能有效提取全局特征，但忽略了不同位置的局部特征PointNet++使用sampling和grouping对点云做采样和聚类，对每个聚类点云用PointNet的方式做特征提取，最后再做池化下一层对上一层sampling的点再重复做sampling和grouping（增加group半径）以增加感受野（个人认为这里会有同一个点的重复提取，而有些点又有可能没照顾到）2）PointNet++考虑了不同区域点云密度不一致的现状，提出MSG和MRG来优化grouping的效果3）PointNet的分割网络直接将max_pool后的全局特征与每个点的特征做concat，缺少不同尺度的特征PointNet++在特征提取阶段是downsample的模式，在分割中设计了upsample步骤，并且每个updasample都跟同一尺度的特征做concat，有点PANet的味道图1. PointNet++精度对比网络结构分析图2. PointNet++网络结构在特征提取部分，PointNet++设计了2层的set abstraction，每个set abstraction包含1个samping+grouping和1个pointnet；第2层的set abstraction以第1层samping到的点为输入在分类部分，PointNet++沿用PointNet的全连接层方案在分割部分，因为要对每个点做预测，因此需要对特征提取的结果做上采样，同时concat特征提取中的相应层提升多尺度的效果set abstractionsamping &amp; grouping首先对当前的所有点云做采样，然后分别以这些采样的点为中心做聚类假设输入为[B,N,3,1]，那么samping+grouping出来的是[B,S,G,3]，其中S是samping的数量，G是grouping的数量def sample_and_group(input_layer, points_convd, num_sample, num_group,                      radius, with_knn=False, with_xyz=True):    \"\"\"    最远点采样+聚类\\n    input_layer  =[B, N, 3]    num_sample   =uint32                        采样点数目\\n    num_group    =uint32                        每个聚类的点数目\\n    points_convd =[B, N, C]                     经过卷积层之后的点（不再包含原始的坐标XYZ）\\n    radius       =float32                       query ball方法聚类的半径    with_knn     =bool                          是否使用knn聚类    with_xyz     =bool                          是否同时使用上一次sample的点    out          =[B, num_sample, 3], \\n                  [B, num_sample, num_group, 3+C]\\n    \"\"\"    xyz    = gather_point(input_layer, farthest_point_sample(input_layer, num_sample)) # [B, num_sample, 3]    if with_knn:        grouped_ind = knn_group(input_layer, xyz, num_group)                           # [B, num_sample, num_group]    else:        grouped_ind = query_ball_group(input_layer, xyz, num_group, radius)    grouped_xyz = group_point(input_layer, grouped_ind)                                # [B, num_sample, num_group, 3]    grouped_xyz -= tf.tile(tf.expand_dims(xyz, 2), [1,1,num_group,1])                  # translation normalization    if points_convd is not None:        grouped_pts = group_point(points_convd, grouped_ind)              # [B, num_sample, num_group]        if with_xyz:            grouped_pts_ = tf.keras.layers.Concatenate(axis=-1)([grouped_xyz, grouped_pts])            # grouped_pts_ = tf.concat([grouped_xyz, grouped_pts], axis=-1) # [B, num_sample, num_group, 3+C]        else:            grouped_pts_ = grouped_pts    else:        grouped_pts_ = grouped_xyz                return xyz, grouped_pts_  samping过程使用最远距离采样算法随机选择一个点为采样点，计算未采样点与当前所有采样点的距离，选择最远的那个加入采样点，一直重复直到采样点数目满足要求  grouping使用knn或者query_ball_group算法经过作者测试，两种方法效果差不多，但后者因为可以控制搜索半径，在MSG中更方便使用grouping中的不均匀点云处理因为点云是不均匀的，在每个地方都采用同样大小的半径/点数来做grouping是不合理的，作者提出了两种方案：MSG、MRG图2. a)MSG、b)MRG1）多尺度组合MSG对每一个采样点，使用多个半径/点数来做grouping，最后做concat这样做对于稀疏的点可以确保有足够大的感受野，但对于比较密的点会有重复采样造成重复计算2）多分辨率组合MRG对不同特征层的输出做concat来扩大感受野，计算量比MSG小此外要注意，对grouping的XYZ点，需要在group范围内做均值归一化PointNet从官方代码看，跟在sampling+grouping后面的PointNet模块实际就是单点卷积+池化，PointNet中的两次T-net旋转被舍弃了（原因不知道，按道理至少在原始点云输入的时候做1次旋转还是可以的）分类网络# PointNet++ SSG 分类网络def PointNet2(input_layer):    \"\"\"    input_layer=[B, N, 3, 1]    \"\"\"    input_layer_   = tf.squeeze(input_layer, [3])    xyz1, points1  = common.sample_and_group(input_layer_, None, 512, 32, 0.2)    conv = common.convolutional(points1, (1, 1,  3,  64))    conv = common.convolutional(conv,    (1, 1, 64,  64))    conv = common.convolutional(conv,    (1, 1, 64, 128))    pool = common.max_pool(conv, (1, conv.shape[2]))    pool = tf.squeeze(pool, [2])        xyz2, points2  = common.sample_and_group(xyz1, pool, 128, 64, 0.4)    conv = common.convolutional(points2, (1, 1,   3, 128))    conv = common.convolutional(conv,    (1, 1, 128, 128))    conv = common.convolutional(conv,    (1, 1, 256, 256))    pool = common.max_pool(conv, (1, conv.shape[2]))    pool = tf.squeeze(pool, [2])    points3  = common.sample_and_group_all(xyz2, pool)    conv = common.convolutional(points3, (1, 1,   3,  256))    conv = common.convolutional(conv,    (1, 1, 256,  512))    conv = common.convolutional(conv,    (1, 1, 512, 1024))    pool = common.max_pool(conv, (1, conv.shape[2]))    full = common.fully_connect(pool, 512, drop_rate=0.5)    full = common.fully_connect(full, 128, drop_rate=0.5)    full = common.fully_connect(full, cfg.CLASSES, activate=False, bn=False)    return full分割网络分割网络首先对特征提取网络出来的结果做上采样，然后与特征网络中相应层的特征做concat，最后插值到与原始点云数量相同的时候输出分割结果# PointNet++分割网络def PointNet2Seg(input_layer):    \"\"\"    input_layer=[B, N, 3, 1]    output     =[B, N, cfg.CLASSES]    \"\"\"    N = input_layer.shape[1]    input_layer_   = tf.squeeze(input_layer, [3])    xyz1, points1  = common.sample_and_group(input_layer_, None, 1024, 32, 0.1)    conv  = common.convolutional(points1, (1, 1,  3, 32))    conv  = common.convolutional(conv,    (1, 1, 32, 32))    conv  = common.convolutional(conv,    (1, 1, 32, 64))    pool1 = common.max_pool(conv, (1, conv.shape[2]))    pool1 = tf.squeeze(pool1, [2])    xyz2, points2  = common.sample_and_group(xyz1, pool1, 256, 32, 0.2)    conv  = common.convolutional(points2, (1, 1,  3,  64))    conv  = common.convolutional(conv,    (1, 1, 64,  64))    conv  = common.convolutional(conv,    (1, 1, 64, 128))    pool2 = common.max_pool(conv, (1, conv.shape[2]))    pool2 = tf.squeeze(pool2, [2])    xyz3, points3  = common.sample_and_group(xyz2, pool2, 64, 32, 0.4)    conv  = common.convolutional(points3, (1, 1,  3,  64))    conv  = common.convolutional(conv,    (1, 1, 64,  64))    conv  = common.convolutional(conv,    (1, 1, 64, 128))    pool3 = common.max_pool(conv, (1, conv.shape[2]))    pool3 = tf.squeeze(pool3, [2])    xyz4, points4  = common.sample_and_group(xyz3, pool3, 16, 32, 0.8)    conv  = common.convolutional(points4, (1, 1,  3,  64))    conv  = common.convolutional(conv,    (1, 1, 64,  64))    conv  = common.convolutional(conv,    (1, 1, 64, 128))    pool4 = common.max_pool(conv, (1, conv.shape[2]))    pool4 = tf.squeeze(pool4, [2])    fp1  = common.feather_propogation(xyz3, pool3, xyz4, pool4)    conv = common.convolutional(fp1,  (1, 1,   3, 256))    conv = common.convolutional(conv, (1, 1, 256, 256))    conv = tf.squeeze(conv, [2])    fp2  = common.feather_propogation(xyz2, pool2, xyz3, conv)    conv = common.convolutional(fp2,  (1, 1,   3, 256))    conv = common.convolutional(conv, (1, 1, 256, 256))    conv = tf.squeeze(conv, [2])    fp3  = common.feather_propogation(xyz1, pool1, xyz2, conv)    conv = common.convolutional(fp3,  (1, 1,   3, 256))    conv = common.convolutional(conv, (1, 1, 256, 128))    conv = tf.squeeze(conv, [2])    fp4  = common.feather_propogation(input_layer_, None, xyz1, conv)    conv = common.convolutional(fp4,  (1, 1,   3, 128))    conv = common.convolutional(conv, (1, 1, 128, 128))    conv = common.convolutional(conv, (1, 1, 128, 128))    conv = common.convolutional(conv, (1, 1, 128, 128))    dp   = tf.keras.layers.Dropout(0.5)(conv)    conv = common.convolutional(dp, (1, 1, 128, cfg.CLASSES+1), activate=False, bn=False)    conv = tf.keras.layers.Reshape((N, cfg.CLASSES+1))(conv)    return conv上采样def feather_propogation(xyz1, point1, xyz2, point2):    \"\"\"    上采样\\n    N&gt;M, 对xyz1每个点，在xyz2中找最近的3个    xyz1   =[B, N, 3]    point1 =[B, N, C1]    xyz2   =[B, M, 3]    point2 =[B, M, C2]    out    =[B, N, 1, C1+C2],    \"\"\"    weight, idx  = three_nn(xyz1, xyz2)    interpolated = three_interpolate(point2, weight, idx)    if point1 is not None:        interpolated = tf.keras.layers.Concatenate(axis=-1)([point1, interpolated])    return tf.keras.layers.Reshape((interpolated.shape[1], 1, interpolated.shape[2]))(interpolated)插值是对上一层（点数量多）的每个点，在下一层（点数量少）中查找XYZ距离最近的3个点，并根据距离分配权重，基于权重计算出1个插值点，这样就可以保证1）下一层插值后的点数量与上一层一致2）最终用于分割的点的顺序与输入一致c++实现TF中未定义的OPPointNet++中的samping+grouping和upsample是特殊操作，在TF中没有定义参考文章《Tensorflow上手3: 实现自己的Op》1、在C++中实现OP的注册2、在C++中重写OP的compute函数（基于CPU或者CUDA都可以）3、在C++中实现该OP的梯度计算OP（也包含OP注册和compute函数重写）4、将C++文件编译为动态链接库，在python文件中加载库后实现函数接口OP注册这一步是为了声明函数的输入输出类型以及尺寸TF在加载网络模型时根据这个函数推断每一层的数据，compute函数的实现在TF开始训练/推断时才会运行#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/shape_inference.h\"#include \"tensorflow/core/framework/common_shape_fns.h\"#include \"cstdlib\"#include \"cstring\"#include \"ctime\"#include \"iostream\"#include \"fstream\"using namespace tensorflow;REGISTER_OP(\"FarthestPointSample\")  .Attr(\"nsampled: int\")  .Input(\"inp: float32\")  .Output(\"out: int32\")  .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c){    ::tensorflow::shape_inference::ShapeHandle dims1;    TF_RETURN_IF_ERROR(c-&gt;WithRank(c-&gt;input(0), 3, &amp;dims1)); // batch_size * npoints * 3    int nsampled;    TF_RETURN_IF_ERROR(c-&gt;GetAttr(\"nsampled\", &amp;nsampled));    ::tensorflow::shape_inference::ShapeHandle output = c-&gt;MakeShape({c-&gt;Dim(dims1, 0), nsampled}); // batch_size * nsampled    c-&gt;set_output(0, output);    return Status::OK();  });重写OP的compute函数从OpKernel继承，主要重写Compute函数一般是检查输入tensor的尺寸，申请输出tensor的空间，然后实现具体功能class FarthestPointSampleCpuOp: public OpKernel{public:    explicit FarthestPointSampleCpuOp(OpKernelConstruction* context):OpKernel(context){        OP_REQUIRES_OK(context, context-&gt;GetAttr(\"nsampled\", &amp;nsampled_));        OP_REQUIRES(context, nsampled_ &gt; 0, errors::InvalidArgument(\"FarthestPointSample needs positive nsampled\"));    }    void Compute(OpKernelContext* context)override{        const Tensor&amp; inp_tensor = context-&gt;input(0);        OP_REQUIRES(context,                     inp_tensor.dims()==3 &amp;&amp; inp_tensor.shape().dim_size(2)==3,                    errors::InvalidArgument(\"FarthestPointSample expects (batch_size,num_points,3) inp shape\"));        int b = inp_tensor.shape().dim_size(0);        int n = inp_tensor.shape().dim_size(1);        auto inp_flat = inp_tensor.flat&lt;float&gt;(); // type: Eigen        const float* inp = &amp;(inp_flat(0));        int m = nsampled_;        Tensor* out_tensor = NULL;        OP_REQUIRES_OK(context, context-&gt;allocate_output(0, TensorShape{b,m}, &amp;out_tensor));        auto out_flat = out_tensor-&gt;flat&lt;int&gt;();        int* out = &amp;(out_flat(0));        farthestpointsamplingLauncher(b, n, m, inp, out);    }private:    int nsampled_;};REGISTER_KERNEL_BUILDER(Name(\"FarthestPointSample\").Device(DEVICE_CPU), FarthestPointSampleCpuOp);梯度计算OP梯度计算是额外的一个OP，它考察的是从$n+1$层梯度back回当前$n$层到达$n-1$层的梯度变化对于sample、group和upsample，基本思路如果某个点在$n+1$层和$n-1$层都存在，则复制$n+1$层对应的梯度，否则置0官方代码在具体实现中将sample操作分成两步实现，第一步得到采样的索引，第二步根据索引gather点云，这样第一步是没有梯度的，只有第二步因为选择产生梯度变化。这样做的好处是，在group中可以根据索引concat特征点void gatherpointgradLauncher(int b,int n,int m,const float * grad,const int * idx,float * inp_grad){    for(int ind_batch=0; ind_batch&lt;b; ++ind_batch){        for(int ind_pt=0; ind_pt&lt;m; ++ind_pt){            int ind_inp_grad = ind_batch*n*3+idx[ind_batch*m+ind_pt]*3;            int ind_grad     = ind_batch*m*3+ind_pt*3;            inp_grad[ind_inp_grad]   = grad[ind_grad];            inp_grad[ind_inp_grad+1] = grad[ind_grad+1];            inp_grad[ind_inp_grad+2] = grad[ind_grad+2];        }    }}python文件中加载库后实现函数接口      使用tf.load_op_library载入库        C++中函数名为FarthestPointSample，在python中需要改为farthest_point_sample        梯度函数需要有修饰符@tf.RegisterGradient('GatherPoint')  #! /usr/bin/env python# coding=utf-8import sysimport osimport tensorflow as tffrom tensorflow.python.framework import opsBASE_DIR = os.path.dirname(os.path.abspath(__file__))sys.path.append(BASE_DIR)sampling_module = tf.load_op_library(os.path.join(BASE_DIR, 'tf_sampling_so.so'))def farthest_point_sample(inp, nsampled):    \"\"\"    最远点采样\\n    inp:      [B, N, 3]     float32\\n    nsampled: int32\\n    returns:  [B, nsampled] float32    \"\"\"    return sampling_module.farthest_point_sample(inp, nsampled)ops.NoGradient('FarthestPointSample')def gather_point(inp, idx):    \"\"\"    根据farthest_point_sample得到的索引提取采样的点云\\n    inp:     [B, N, 3]        float32\\n    idx:     [B, nsampled]    int32\\n    returns: [B, nsampled, 3] float 32    \"\"\"    return sampling_module.gather_point(inp, idx)@tf.RegisterGradient('GatherPoint')def _gather_point_grad(op, grad):    \"\"\"    gather_point的梯度\\n    op:      gather_point操作\\n    grad:    传播到gather_point的梯度\\n    returns: 经过gather_point的梯度    \"\"\"    inp = op.inputs[0]    idx = op.inputs[1]    # 因为gather point有2个输入，因此有2个梯度输出    # 但第2个输入是idx没有梯度意义,因此为None    return [sampling_module.gather_point_grad(inp, idx, grad), None]效果实际测下来跟PointNet精度差不多，但训练难度更大一些猜测是：最远点采样中初始点的选择导致数据不稳定，或者是因为sampling机制导致的不均衡采样【1】Qi C R ,  Yi L ,  Su H , et al. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space[J].  2017."
  },
  
  {
    "title": "图像分类、目标检测精度指标",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%B2%BE%E5%BA%A6%E6%8C%87%E6%A0%87/",
    "categories": "深度学习",
    "tags": "",
    "date": "2022-02-15 00:00:00 +0000",
    





    
    "snippet": "精度precision &amp; 召回率recall          actual positive    actual negative        predicted positive    TP    FP        predicted negative    FN    TN   \\[\\begin{aligned}Presicion=&amp;\\frac{TP}{TP+FP...",
    "content": "精度precision &amp; 召回率recall          actual positive    actual negative        predicted positive    TP    FP        predicted negative    FN    TN   \\[\\begin{aligned}Presicion=&amp;\\frac{TP}{TP+FP}\\\\Recall=&amp;\\frac{TP}{TP+FN}\\\\True\\ Positive\\ Rate(TPR)=&amp;\\frac{TP}{TP+FN}\\\\False\\ Positive\\ Rate(FPR)=&amp;\\frac{FP}{FP+TN}\\end{aligned} \\tag{1}\\]其中  精度(precision): 又称查准率，表示在被预测为正样本中的真实正样本的比例  召回率(recall): 又称查全率，表示在所有真实正样本中被正确预测的样本的比例可以看到，precision关注的是虚警率，recall关注的是漏警率F1 scoreF1 score 同时考虑precision和recall，使用一个值就可以反映上述两个指标F1 score 使用调和平均计算\\[\\begin{aligned}\\frac{1}{F1}=&amp;\\frac{1}{2}(\\frac{1}{precision}+\\frac{1}{recall})\\\\\\Rightarrow F1=&amp;2*\\frac{precision*recall}{precision+recall}\\end{aligned}\\tag{2}\\]  与普通平均不一样，F1总是更接近precision和recall中最小的值  对于多分类问题，首先计算每个类别的F1，然后对其做平均ROC曲线和PR曲线概率的处理对目标分类和定位的结果是以概率的形式体现的，最终决定是否采信该结果一般有2种方法：1）设定一个阈值K，当某类别的输出概率大于该阈值则预测为阳性2）对所有样本输出的结果，依次按类别做分组，每个组按概率从大到小排序，取前N个为阳性预测第一种方法对应的指标名称为$AP_{K}$，如$AP_{0.5}$表示置信度为0.5第二种方法对应的指标名称为Top-N，如Top-5表示取前5个为阳性预测ROC、PR的定义从上一节可以看到，对于同一组样本、同一个网络，不同的概率阈值或者预测数目N都会最终导致不同的预测结果，那么该如何评估模型的表现呢ROC和PR思路都是通过取不同的阈值（K或N），观察结果的趋势：  ROC以FPR为x轴，TPR为y轴  PR以recall为x轴，precision为y轴对于同一批网络输出，选取不同的K或N，都会得到一组recall、precision、FPR和TPR，将这些点记录在ROC或PR图上，最后连成曲线图1. 左：ROC，右：PR不难理解，ROC曲线越凸向左上方效果越好，PR曲线越往右上方凸越好AUCAUC（area under curve）是ROC曲线下的面积图2. AUC  显然，AUC的值小于1，又因为ROC一般都处于y=x直线的上方，所以AUC一般为[0.5, 1]  AUC的含义为，随机挑选一个正样本（TPR）和一个负样本（FPR），根据当前阈值（图2种垂直的虚线）将这个正样本排在负样本前面的概率ROC、PR的应用场景从式(1)可以看到，ROC同时考虑了被正确分类的正、负样本，而PR仅考虑了被正确分类的正样本，因此：ROC对不平衡样本的鲁棒性更高，当测试集种正负样本的分布有显著变化的时候，ROC曲线能保持基本一致；而PR受测试正负样本比例变化影响较大图3. (a)ROC(P:N=1:1)；(b)PR(P:N=1:1)；ROC(P:N=1:10)；PR(P:N=1:10)1）如果有多份数据且存在不同类别分布，适合用ROC评估，因为其能剔除类别分布的影响，反映分类器整体性能2）如果要评估相同类别分布下正样本的预测情况，或者需要测试不同类别分布下对分类器的性能影响，适合用PR多类别分类任务上述precision、recall、f1-score、ROC曲线、PR曲线都是针对二分类任务定义的对于多类别分类任务，需要对每个类型分别计算precision、recall和f1-score，ROC曲线则有两种处理方式(参考《多分类ROC曲线及AUC计算》)macro roc该方法对应sklearn.metrics.roc_auc_score函数中参数average值为’macro’的情况方法分别在不同阈值下对每个类型计算roc，然后对所有的roc曲线取平均micro roc该方法对应sklearn.metrics.roc_auc_score函数中参数average值为’micro’的情况  首先将groundTruth展开成onehot形式如3分类的gt有5个样本：[1,2,0,2,0]，展开成onehot为[0,1,0,0,0,1,1,0,0,0,0,1,1,0,0]  然后将predict也按行展开为onehot与上述gt对应，如predict为：[[0.2,0.8,0.3], [0.7,0.2,0.6], [0.9,0.5,0.2], [0.1,0.5,0.8], [0.6,0.7,0.1]]展开后为[0.2,0.8,0.3,0.7,0.2,0.6,0.9,0.5,0.2,0.1,0.5,0.8,0.6,0.7,0.1]假设当前测试的阈值为0.3，那么展开后的onehot就是[0,1,1,1,0,1,1,1,0,0,1,1,1,1,0]  两者展开后多分类问题就变成二分类问题，在不同阈值下计算roc即可代码实现def make_metrics(np_outputs, np_labels, num_classes, step=0.01):    ''' 评估模型精度    '''    micro_roc, roc_cls = [], []    accuracy_thres = []    precision_thres, recall_thres, f1_score_thres = [], [], []    max_index = np.argmax(np_outputs, axis=1)    max_score = np_outputs[range(max_index.shape[0]), list(max_index)]    end_ = int(1./step)+1    for thresh in range(0, end_):        thresh = step*thresh        max_index_ = np.where(max_score &gt;= thresh, max_index, -1)        # make accuracy for all the classes        num_correct = (max_index_ == np_labels).astype(np.int64).sum()        accuracy = float(num_correct)/max_index_.shape[0]        accuracy_thres.append(accuracy)        # make precision for each class        precision, recall, f1_score, roc_cls_ = [], [], [], []        for cls_ in range(num_classes):            cls_pred = np.where(np_outputs[:,cls_] &gt;= thresh, 1, 0)            cls_mask = (np_labels == cls_).astype(np.int64)            cls_mask_neg = 1-cls_mask            cls_pred_neg = 1-cls_pred            tp = (cls_mask*cls_pred).sum()            tn = (cls_mask_neg*cls_pred_neg).sum()            fp = (cls_mask_neg*cls_pred).sum()            fn = (cls_mask*cls_pred_neg).sum()            # 为每个类别记录roc值            tpr = float(tp)/(tp+fn)            fpr = float(fp)/(fp+tn)            roc_cls_.append((fpr, tpr))            precision_ = float(tp)/(tp+fp)            recall_    = float(tp)/(tp+fn)            if abs(precision_) &lt; 1e-5 or abs(recall_) &lt; 1e-5:                f1_score = 0.            else: f1_score_  = 2/(1/precision_ + 1/recall_)                            precision.append(precision_)            recall.append(recall_)            f1_score.append(f1_score_)        precision_thres.append(precision)        recall_thres.append(recall)        f1_score_thres.append(f1_score)        roc_cls.append((roc_cls_, thresh))        # keep micro roc for each thresh        onehot_score = np.reshape(np_outputs, (-1,))        onehot_pd = np.where(onehot_score &gt;= thresh, 1, 0)        onehot_gt = np.zeros((num_classes*max_index_.shape[0],), dtype=np.int64)        for indx in range(max_index_.shape[0]):            onehot_gt[np_labels[indx]+indx*num_classes] = 1        tp = (onehot_gt*onehot_pd).sum()        tn = ((1-onehot_gt)*(1-onehot_pd)).sum()        fp = ((1-onehot_gt)*onehot_pd).sum()        fn = (onehot_gt*(1-onehot_pd)).sum()        tpr = float(tp)/(tp+fn)        fpr = float(fp)/(fp+tn)        micro_roc.append((fpr, tpr, thresh))            # make micro roc    path_roc = 'micro_roc.jpg'    value_auc_micro, best_thre_micro = make_roc_curve(micro_roc, path_roc)             # make macro roc    roc_cls_all = []    for cls_ in range(num_classes):        roc_cls_ = []        for ind in range(len(roc_cls)):           roc_cls_.append((roc_cls[ind][0][cls_][0], roc_cls[ind][0][cls_][1], roc_cls[ind][1]))        path_roc = 'roc_class#%03d.jpg'%cls_        value_auc, best_thre = make_roc_curve(roc_cls_, path_roc)         roc_cls_all.append(roc_cls_)    path_roc = 'macro_roc.jpg'    value_auc_macro, best_thre_macro = make_roc_curve_multiclass(roc_cls_all, path_roc)             best_step = int(best_thre_micro/step)    return (accuracy_thres[best_step],             precision_thres[best_step],             recall_thres[best_step],             f1_score_thres[best_step],            value_auc_micro,            best_thre_micro)import matplotlib.pyplot as pltfrom sklearn.metrics import aucdef make_roc_curve(roc, path_roc):    ''' 画二分类的roc曲线，计算auc，找到最佳的threshold        roc: ((x,y,t),(x,y,t),)    '''    np_roc = np.asarray(sorted(roc, key=lambda pos:pos[0]))    auc_value = auc(np_roc[:,0], np_roc[:,1])    # find the best threshold    min_diff, min_ind = 3., 0    for ind in range(len(roc)):        # distance from (0,1)        diff = math.pow(roc[ind][0],2) + math.pow(roc[ind][1]-1,2)        if diff &lt; min_diff:            min_diff = diff            min_ind  = ind    best_threshold = roc[min_ind][2]    plt.figure()    plt.plot(np_roc[:,0], np_roc[:,1],              label='micro ROC (area=%.2f)'%auc_value,              color='darkorange',             lw=2)    plt.plot([0,1], [0,1], 'k--', lw=2)    plt.xlim([0.0, 1.0])    plt.ylim([0.0, 1.05])    plt.xlabel('False Positove Rate')    plt.ylabel('True Positive Rate')    plt.title('ROC')    plt.legend(loc='lower right')    plt.savefig(path_roc)    return auc_value, best_thresholddef make_roc_curve_multiclass(roc, path_roc):    ''' 画多分类的roc曲线，计算auc，找到最佳的threshold       roc: (((x,y,t),(x,y,t),), ) ncls*npts*3       对不同种类的roc坐标取移动平均    '''    roc_sorted = []    for cls_ in range(len(roc)):        roc_sorted.append(sorted(roc[cls_], key=lambda pos:pos[0]))    # 按fpr从小到大合并所有类别的roc坐标点    roc_merged = []    ind_roc_cls = [0 for _ in range(len(roc_sorted))]    while True:        min_x   = 2.        min_cls = -1        for cls_ in range(len(roc_sorted)):            if ind_roc_cls[cls_] == -1: continue            _x = roc_sorted[cls_][ind_roc_cls[cls_]][0]            if _x &lt; min_x:                min_x   = _x                min_cls = cls_        if min_cls == -1: break        # 滑动平均        pos = roc_sorted[min_cls][ind_roc_cls[min_cls]]        if len(roc_merged) == 0:            roc_merged.append(pos)        else:            roc_merged.append((pos[0], (pos[1]+roc_merged[-1][1])/2., pos[2]))        # 更新类别指针        ind_roc_cls[min_cls] += 1        if ind_roc_cls[min_cls] &gt;= len(roc_sorted[min_cls]):            ind_roc_cls[min_cls] = -1    return make_roc_curve(roc_merged, path_roc)目标检测任务COCO数据集评测指标图4. COCO数据集评价指标这是针对目标检测任务的评测指标  AP是以0.05为步长依次考察IoU=0.5到0.95时的精度，对其取平均值（与mAP等价），$AP^{.50}$则以IoU=0.5为阈值得到的精度（也是VOC的默认指标）  $AP^s$、$AP^m$、$AP^l$分别为小面积、中等面积、大面积目标的分割精度（面积以像素数计算），反映模型在多尺度方面的性能  $AR^{max=k}$为每张图像上包含k个目标时，模型检测的最大召回率AP、mAP计算      参考《轻松计算目标检测评价指标mAP》、《目标检测重要评价指标——mAP的含义及计算》        目标检测任务输出目标的类别、框位置、置信度，AP实际上并没有考虑类别精度  AP是在某个IoU阈值下计算不同置信度下的Precision和Recall，计算PR曲线下的近似面积图5. AP计算举例1）借用文章的例子，假设对图5有7个预测结果，每个结果的置信度和IoU情况如图5右表所示，预测框与GT的IoU如果小于0.5则设置为False2）依次取不同的置信度阈值计算Precision和Recall对$threshold=0.89$，可以看到前2个满足条件即$TP=2$，因为阈值大于0.89的预测IoU均大于0.5，所以$FP=0$(IoU为True表示这个预测实际是P)，AP计算中$FN=所有结果-TP=5$(这里我暂时无法理解，按FN的定义，应该是置信度小于0.89但IoU为True的结果数3)因此有$Precision=\\frac{TP}{TP+FP}=1$，$Recall=\\frac{TP}{TP+FN}=0.28$按照上述计算方法，得到取不同置信度阈值的所有结果：# (recall, precision)((0.14, 1), (0.28, 1), (0.42, 1), (0.57, 1), (0.57, 0.8), (0.57, 0.66), (0.71, 0.71))3）画PR曲线，近似计算曲线下方面积即为AP图6. AP近似计算auc图6一共包含3组PR点:# (recall, precision)((0.33, 1), (0.33, 0.5), (0.67, 0.67))为方便计算，实际是以蓝色区域面积为估计值（对每个点取其与前一个点中最大的precision为矩形高）: AP=1*0.33+0.67*0.33=0.55mAP则是对基于不同IoU阈值得到的所有AP取均值代码实现def make_metrics(gt_bbox, outputs_bbox, outputs_score, step=0.01):    ''' 评估模型精度 AP、mAP        https://zhuanlan.zhihu.com/p/139073511    '''    matched_pred = []    for idx_img in range(len(gt_bbox)):        np_gtbbox = gt_bbox[idx_img]        np_pdbbox = outputs_bbox[idx_img]        # 对每个pred框, 找到与之iou最大的那个gt        for idx_pd in range(np_pdbbox.shape[0]):            max_iou, max_gt = 0, -1            for idx_gt in range(np_gtbbox.shape[0]):                if np_gtbbox[idx_gt][0] == -1: break                _iou = iou(np_gtbbox[idx_gt], np_pdbbox[idx_pd])                if _iou &gt; max_iou:                    max_iou = _iou                    max_gt  = idx_gt                                pred_score = outputs_score[idx_img][idx_pd]            matched_pred.append((idx_img, max_gt, max_iou, pred_score))    # 对不同的iou阈值，计算PR曲线, 计算auc即为ap    mAP, AP, bthresh = 0, [], []    pr_files = []    for iou_thre in range(50, 100, 5):        iou_thre_ = 0.01*iou_thre        end_ = int(1./step)+1        pr_curves = []        for thresh_ in range(0, end_):            thresh = step*thresh_            tp, fp = 0, 0                        for pred_ in matched_pred:                if pred_[3] &gt;= thresh:                    if pred_[2] &gt;= iou_thre_: tp += 1                    else:                     fp += 1                                tfp = tp + fp            if tfp == 0: precision = 0            else:        precision = float(tp)/(tp+fp)            recall = float(tp)/len(matched_pred)            pr_curves.append((recall, precision, thresh))                    if iou_thre_ == 0.5:            # 保存iou阈值为0.5的PR曲线图            path_curve = os.path.join(self.pred_dir,'pr_%.2f.jpg'%iou_thre_)            pr_files.append(path_curve)            label_curve = 'iou-%.2f'%iou_thre_            auc_, best_thresh   = calc_pr_area(pr_curves, path_curve, label_curve, True)        else: auc_, _ = calc_pr_area(pr_curves)        mAP += auc_        AP.append((auc_, iou_thre_))    mAP /= float(len(AP))    return mAP, AP, best_threshdef calc_pr_area(pr_pts:list,                  path_curve=None, label_curve=None, make_curve=False):    ''' 估计pr曲线下的面积    '''    pr_pts_ = sorted(pr_pts, key=lambda item:item[0])    auc = pr_pts_[0][0]*pr_pts_[0][1]    for ind in range(1, len(pr_pts)):        auc += (pr_pts_[ind][0]-pr_pts_[ind-1][0]) * \\               max(pr_pts_[ind][1], pr_pts_[ind-1][1])        # find the best threshold    min_diff, min_ind = 3., 0    for ind in range(len(pr_pts_)):        # distance from (1,1)        diff = math.pow(pr_pts_[ind][0]-1, 2) + math.pow(pr_pts_[ind][1]-1, 2)        if diff &lt; min_diff:            min_diff = diff            min_ind  = ind    best_threshold = pr_pts_[min_ind][2]    if make_curve:        draw_pr_curve(pr_pts_, auc, label_curve, path_curve)    return auc, best_thresholddef draw_pr_curve(pr_pts:list, auc:float, label_curve, path_curve):    ''' 画某个iou阈值下的PR曲线    '''    np_pr = np.asarray(pr_pts)    plt.figure()    plt.plot(np_pr[:,0], np_pr[:,1],              label='%s (area=%.2f)'%(label_curve, auc),              color='darkorange',             lw=2)    plt.plot([0,1], [0,1], 'k--', lw=2)    plt.xlim([0.0, 1.0])    plt.ylim([0.0, 1.05])    plt.xlabel('Recall')    plt.ylabel('Precision')    plt.title('PR')    plt.legend(loc='lower right')    plt.savefig(path_curve)"
  },
  
  {
    "title": "【数据结构】（6）动态规划",
    "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-6-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/",
    "categories": "数据结构",
    "tags": "",
    "date": "2022-01-28 00:00:00 +0000",
    





    
    "snippet": "动态规划与分治方法类似，都是通过组合子问题的解来求解原问题，不同点在于：  分治法将问题划分为互不相交的子问题，递归地求解子问题，再将它们的解组合起来（参考《【数据结构】（2）排序算法分析》中的归并排序）  动态规划应用于子问题重叠的情况，即不同的子问题有公共的子子问题动态规划之所以高效，是因为其在计算过程中将公共子问题的最优解存下来了，这样每种可能性只需要计算一次（属于空间换时间的思想）动...",
    "content": "动态规划与分治方法类似，都是通过组合子问题的解来求解原问题，不同点在于：  分治法将问题划分为互不相交的子问题，递归地求解子问题，再将它们的解组合起来（参考《【数据结构】（2）排序算法分析》中的归并排序）  动态规划应用于子问题重叠的情况，即不同的子问题有公共的子子问题动态规划之所以高效，是因为其在计算过程中将公共子问题的最优解存下来了，这样每种可能性只需要计算一次（属于空间换时间的思想）动态规划的两个前提条件适用于应用动态规划求解的最优化问题应具备两个要素：最优子结构、子问题重叠1）最优子结构如果一个问题的最优解包含其子问题的最优解，就称此问题具有最优子结构性质比如下面的钢条切割例子，考虑10英寸的最佳切割方案时，如果已经知道了3英寸的最佳方案，则只需要考察剩下的7英寸的方案2）子问题重叠如果递归算法反复求解相同的子问题，就称最优化问题具有重叠子问题适用于分治法求解的问题通常再递归的每一步都生成全新的问题（因此无法用之前计算的结果）动态规划案例钢条切割给定一段长度为n的钢条，一个不同长度的价格表，求钢条的最佳切割方案，使得销售收益最大#! /usr/bin/env python# coding=utf-8price_len = (1, 5, 8, 9, 10, 17, 17, 20, 24, 30)def optimal_segment_recur(len):    if len == 0: return 0, []    value = 0    for i in range(1,len+1):        best_value, best_list = optimal_segment_recur(len-i)        best_value += price_len[i-1]        if best_value &gt; value:            value = best_value            best_list_ = [i]            best_list_.extend(best_list)    return value, best_list_def optimal_segment_dp(len):    # best_value 表示长度为i时最佳切割的收益    best_value = [0 for i in range(len+1)]    best_list  = [[] for i in range(len+1)]    for i in range(1,len+1):        value = 0        for j in range(1,i+1):            best_value_ = price_len[j-1]+best_value[i-j]            if value &lt; best_value_:                value = best_value_                best_list_ = [j]                best_list_.extend(best_list[i-j])        best_value[i] = value        best_list[i]  = best_list_    return best_value[len], best_list[len]if __name__ == '__main__':    opti_value, opti_list = optimal_segment_recur(8)    print(opti_value, \": \", opti_list)    opti_value, opti_list = optimal_segment_dp(8)    print(opti_value, \": \", opti_list)矩阵链乘法矩阵序列的乘积是可以通过括号改变计算次序的\\[A_1A_2A_3=A_1(A_2A_3) \\tag{1}\\]需要注意的是，不同的计算次序虽然最终结果一样，但计算代价有差别，如\\[\\begin{aligned}P((A_{10\\times 100}A_{100\\times 5})A_{5\\times 50})=&amp;10*100*5+10*5*50=7500\\\\P(A_{10\\times 100}(A_{100\\times 5}A_{5\\times 50}))=&amp;10*100*50+100*5*50=75000\\end{aligned} \\tag{2}\\]可以看到，式2第二种计算次序需要做的乘法次数是第一种的10倍矩阵链乘法问题就是对已知的矩阵序列要找到一种最优的计算次序，使得其乘法次数是最小的用$m[i,j]$表示序列中第i到第j个矩阵连乘的最小次数，有\\[m[i,j]=\\begin{cases}0&amp;i=j\\\\\\min_{i\\leq k&lt;j}{\\{m[i,k]+m[k+1,j]+p_{i-1}p_kp_j\\}}&amp;i&lt;j\\end{cases} \\tag{3}\\]基于动态规划的实现首先计算$m[i,i+1]$、$m[i+1,i+2]$…，然后加大步长计算$m[i,i+2]$，这样就可以利用之前计算的结果#! /usr/bin/env python# coding=utf-8import numpy as npFLT_MAX = np.finfo(np.float32).max# Alist = ((10,100),(100,5),(5,50))Alist = ((30,35),(35,15),(15,5),(5,10),(10,20),(20,25))def make_split_index(splits, start, end):    if start == end:        text = \"A%d\"%start    else:        text  = \"(\"        text1 = make_split_index(splits, start, splits[start, end])        text2 = make_split_index(splits, splits[start, end]+1, end)        text = \"%s%s%s)\"%(text, text1, text2)    return text    def optimal_times_dp(Alists):    num_matrix = len(Alists)    best_value = np.zeros((num_matrix,num_matrix))    best_split = np.zeros((num_matrix,num_matrix), dtype=np.int32)    for step in range(1, num_matrix):        for i in range(num_matrix-1):            j = i+step            if j &gt;= num_matrix: break            best_value_ = FLT_MAX            for k in range(i, j):                tmp_value = best_value[i,k] + best_value[k+1,j] +\\                              Alists[i][0]*Alists[k][1]*Alists[j][1]                if tmp_value &lt; best_value_:                    best_value_     = tmp_value                    best_split[i,j] = k            best_value[i,j] = best_value_    # show split    best_split_ = make_split_index(best_split, 0, num_matrix-1)        return best_value[0,num_matrix-1], best_split_if __name__ == '__main__':    min_ops, splits = optimal_times_dp(Alist)    print(min_ops, \" =&gt; \", splits)    # 15125.0  =&gt;  ((A0(A1A2))((A3A4)A5))最长公共子序列子序列：对序列$A[a_1,a_2,…,a_m]$和序列$B[b_1,b_2,…,b_k]$，如果存在一个严格递增的A的下标序列$[i_1,i_2,…i_k]$，对所有$j=1,2,…,k$，满足$a_{i_k}=b_j$，则称$B$是$A$的子序列也就是说，子序列只考虑顺序，不考虑是否连续如$[B,C,D,B]$是$[A,B,C,B,D,A,B]$的子序列公共子序列：如果序列$C$既是$A$的子序列，也是$B$的子序列，则称$C$是$A$和$B$的公共子序列用$c[i,j]$表示序列A取前i个，序列B取前j个的最长公共子序列的长度\\[c[i,j]=\\begin{cases}0&amp;i=0\\ or\\ j=0\\\\c[i-1,j-1]+1&amp;i,j&gt;0\\ and\\ A_i=B_j\\\\max(c[i,j-1],c[i-1,j])&amp;i,j&gt;0\\ and\\ A_i\\neq B_j\\end{cases} \\tag{4}\\]令$A=[A,B,C,B,D,A,B]$，$B=[B,D,C,A,B,A]$，按式4得到的c矩阵如图1所示图1. LCS中c矩阵#! /usr/bin/env python# coding=utf-8import numpy as npA=('A','B','C','B','D','A','B')B=('B','D','C','A','B','A')def show_lcs(A_, c_, pos_a, pos_b, result):    if pos_a == 0 or pos_b == 0:        return    if c_[pos_a][pos_b] &gt; c_[pos_a-1][pos_b-1] and \\        c_[pos_a][pos_b] &gt; c_[pos_a-1][pos_b] and \\        c_[pos_a][pos_b] &gt; c_[pos_a][pos_b-1]:        result.append(A_[pos_a-1])        show_lcs(A_, c_, pos_a-1, pos_b-1, result)    elif c_[pos_a][pos_b] == c_[pos_a-1][pos_b]:        show_lcs(A_, c_, pos_a-1, pos_b, result)    else:        show_lcs(A_, c_, pos_a, pos_b-1, result)def optimal_lcs_dp(A_, B_):    c = np.zeros((len(A_)+1, len(B_)+1), dtype=np.int32)    for i in range(len(A_)):        for j in range(len(B_)):            if A_[i] == B_[j]:                c[i+1,j+1] = c[i,j]+1            elif c[i+1,j] &gt;= c[i,j+1]:                c[i+1,j+1] = c[i+1,j]            else:                c[i+1,j+1] = c[i,j+1]    # show lcs    invd_lcs = []    show_lcs(A_, c, len(A_), len(B_), invd_lcs)    lcs = invd_lcs[::-1]        return c[len(A_),len(B_)], lcsif __name__ == '__main__':    len, cls = optimal_lcs_dp(A, B)    print(len, \" =&gt; \", cls)    # 4  =&gt;  ['B', 'C', 'B', 'A']总结从上述3个案例可以看到，动态规划的核心是  要找到重叠子问题  并且找到解决该子问题的递推公式（式3，4）  在设计程序时先从最小的子问题开始求解，后面就可以使用之前的子问题求解结果  在每个子问题的求解过程中记录每个最优解的方案，一般需要额外设计递归函数还原整体方案"
  },
  
  {
    "title": "【点云深度学习】(1) PointNet(2017)",
    "url": "/posts/%E7%82%B9%E4%BA%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-PointNet/",
    "categories": "点云处理",
    "tags": "点云深度学习",
    "date": "2022-01-25 00:00:00 +0000",
    





    
    "snippet": "相比于图像，点云的深度学习主要有3个难点：  无序性：原始点云是无序的，不像图像一样可以直接得到其邻域信息  旋转性：即使对于同一个目标，观察的坐标系不一样，点云的呈现也不一样  不均匀性：与图像不同，点云在不同位置的分布是不均匀的点云深度学习的发展参考综述《Deep learning for 3d point clouds: A survey》【1】点云的深度学习方法可以分为以下3类：  ...",
    "content": "相比于图像，点云的深度学习主要有3个难点：  无序性：原始点云是无序的，不像图像一样可以直接得到其邻域信息  旋转性：即使对于同一个目标，观察的坐标系不一样，点云的呈现也不一样  不均匀性：与图像不同，点云在不同位置的分布是不均匀的点云深度学习的发展参考综述《Deep learning for 3d point clouds: A survey》【1】点云的深度学习方法可以分为以下3类：      方法    基本思路    特点        多视图（Multi-view based）    首先将点云按不同的视角（前视、俯视...）分别投影到二维，然后依次提取特征，最后将各个视角的特征做聚合得到分类/检测结果    得益于图像CNN的高精度，多视图方法分类效果好，也能做目标检测，但无法做点云分割        体素化（Volumetric based）    首先将点云体素化到网格，然后通过3DCNN网络做分类/分割    计算量大，而且由于体素化会造成细节丢失，影响特征提取，现在已经不是主流方法了        直接点云（Point based）    直接将原始点云输入网络做分类和分割    计算量少，没有损失数据  PointNetPointNet【2】是首个Point-based路线的方法，网络简单，效果不错官方网址、官方代码  使用T-net实现旋转的学习  使用对称函数解决点云无序的问题对称函数就是指对输入顺序不敏感的函数，比如加法、乘法、MaxPool等网络结构分析图1. PointNet结构1) 网络一共应用了两次T-net，一次是对输入点云做旋转，一次是对中间的特征做转换2) 对$n\\times 1024$的特征做MaxPool得到全局特征，解决点云无序问题3) 对局部特征和全局特征做concat，结合mlp实现点云分割# 点云分类网络def PointNet(input_layer):    # input_layer=[B, N, 3, 1]    N = input_layer.shape[1]    pose_trans = transform.transform_net(input_layer, 3)    transed1   = transform.apply_transform(input_layer, pose_trans)    conv = common.convolutional(transed1, (1, 3,  N, 64))    conv = common.convolutional(conv,     (1, 1, 64, 64))    feat_trans = transform.transform_net(conv, 64)    transed2   = transform.apply_transform(conv, feat_trans)    conv = common.convolutional(transed2, (1, 1,  64,   64))    conv = common.convolutional(conv,     (1, 1,  64,  128))    conv = common.convolutional(conv,     (1, 1, 128, 1024))    pool = common.max_pool(conv, (N, 1))    pool = tf.keras.layers.Reshape((1024,))(pool)    full = common.fully_connect(pool, 512, drop_rate=0.3)    full = common.fully_connect(full, 256, drop_rate=0.3)    full = common.fully_connect(full, cfg.CLASSES)    return full# 点云分割网络def PointNetSeg(input_layer):    \"\"\"    input_layer=[B, N, 3, 1]    output     =[B, N, cfg.CLASSES]    \"\"\"    N = input_layer.shape[1]    pose_trans = transform.transform_net(input_layer, 3)    transed1   = transform.apply_transform(input_layer, pose_trans)    conv = common.convolutional(transed1, (1, 3,  N, 64))    conv = common.convolutional(conv,     (1, 1, 64, 64))    feat_trans = transform.transform_net(conv, 64)    transed2   = transform.apply_transform(conv, feat_trans)    conv = common.convolutional(transed2, (1, 1,  64,   64))    conv = common.convolutional(conv,     (1, 1,  64,  128))    conv = common.convolutional(conv,     (1, 1, 128, 1024))    pool         = common.max_pool(conv, (N, 1))    global_feats = common.replicate(pool, (1, N, 1, 1))    concat_feats = tf.concat([transed2, global_feats], 3)    conv = common.convolutional(concat_feats, (1, 1, 1088, 512))    conv = common.convolutional(conv,         (1, 1,  512, 256))    conv = common.convolutional(conv,         (1, 1,  256, 128))    conv = common.convolutional(conv,         (1, 1,  128, 128))    conv = common.convolutional(conv, (1, 1, 128, cfg.CLASSES+1), activate=False, bn=False)    conv = tf.keras.layers.Reshape((N, cfg.CLASSES+1))(conv)    return convmlp对于输入的点云$n\\times 3$，mlp的卷积核为$1\\times 3$，也就是每次卷积只针对一个点云$XYZ$对于中间的特征，mlp的卷积核为$1\\times 1$T-netT-net的核心就是一个$K\\times K$的矩阵，对于输入点云的转换，$K=3$，对于特征的转换，$K=64$class TransformLayer(tf.keras.layers.Layer):    def __init__(self, K):        # super(TransformLayer, self).__init__() # py2        super().__init__()        self.k = K    def build(self, input_shape):        self.w = self.add_weight(name='trans_w',                                 shape=(input_shape[-1], self.k*self.k),                                 initializer=tf.constant_initializer(0.),                                 trainable=True)        self.b = self.add_weight(name='trans_b',                                 shape=(self.k*self.k,),                                 initializer=tf.constant_initializer(0.),                                 trainable=True)        self.b.assign_add(tf.constant(np.eye(self.k).flatten(), dtype=tf.float32))    def call(self, inputs):        trans_mat = tf.matmul(inputs, self.w) + self.b        trans_mat = tf.keras.layers.Reshape((self.k, self.k))(trans_mat)        return trans_matdef transform_net(input_layer, K):    H, W = input_layer.shape[1], input_layer.shape[2]    conv = common.convolutional(input_layer, (1, W,   H,   64))    conv = common.convolutional(conv,        (1, 1,  64,  128))    conv = common.convolutional(conv,        (1, 1, 128, 1024))    pool = common.max_pool(conv, (H, 1))    pool = tf.keras.layers.Reshape((1024,))(pool)    # pool = tf.squeeze(pool, [1, 2])    full = common.fully_connect(pool, 512)    full = common.fully_connect(full, 256)    trans_mat = TransformLayer(K)(full)    return trans_matdef apply_transform(input_layer, trans_mat):    dim2squeeze = 3    if input_layer.shape[2] == 1: dim2squeeze = 2    transd = tf.squeeze(input_layer, axis=dim2squeeze)    transd = tf.matmul(transd, trans_mat)    transd = tf.expand_dims(transd, axis=dim2squeeze)    return transd分类实验使用ModelNet40的数据需要注意的是，数据需要做归一化，并且使用重采样保证每个样本的点云数量一致（这里统一采样为2048）下表所示分类结果没有做数据增强，没有调参，迭代30次            类别      airplane      bathtub      bed      bench      bookshelf      bottle      bowl      car      chair      cone                  精度      0.9394      0.6111      0.9362      0.8000      0.8381      0.8776      0.9444      0.9916      0.9608      0.8500              类别      cup      curtain      desk      door      dresser      flower_pot      glass_box      guitar      keyboard      lamp              精度      0.4615      0.8750      0.8256      1.0000      0.8429      0.0000      0.9167      0.9570      0.9375      0.8000              类别      laptop      mantel      monitor      night_stand      person      piano      plant      radio      range_hood      sink              精度      1.0000      0.9823      0.9518      0.7442      1.0000      0.8673      0.6869      0.2857      0.9528      0.7500              类别      sofa      stairs      stool      table      tent      toilet      tv_stand      vase      wardrobe      xbox              精度      0.9490      0.6667      0.7273      1.0000      0.9518      0.9725      0.7900      0.7474      0.7857      0.6667      分割实验使用tele在铁路上的数据数据做归一化、重采样到4000、分为2类（地面、人）注意：需要特别关注有效类别点云数量和其他点云数量的比例，这里因为一帧数据只有1个行人，大部分点是地面点，所以简单的通过直通滤波降低比例图2. 分割效果文献【1】Guo Y, Wang H, Hu Q, et al. Deep learning for 3d point clouds: A survey[J]. IEEE transactions on pattern analysis and machine intelligence, 2020.【2】Qi C R, Su H, Mo K, et al. Pointnet: Deep learning on point sets for 3d classification and segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 652-660."
  },
  
  {
    "title": "单目测距",
    "url": "/posts/%E5%8D%95%E7%9B%AE%E6%B5%8B%E8%B7%9D/",
    "categories": "自动驾驶",
    "tags": "",
    "date": "2022-01-22 00:00:00 +0000",
    





    
    "snippet": "对于近距离、精度要求不高、测距目标在地面上的情况，可以考虑单目测距本文参考《单目视觉定位测距的两种方式》，并做了相应改进，实测20m以内误差3%，80m以内误差10%模型推导图1. 投影模型相机主光轴与地面的夹角（俯仰角）已知量： 相机内参、相机与地面高度$H$、图像坐标系下某个目标与地面接触的点$pt$、世界坐标系下该点与相机的距离$D$如图1所示，$H$为相机与地面高度，$O3_P$所在...",
    "content": "对于近距离、精度要求不高、测距目标在地面上的情况，可以考虑单目测距本文参考《单目视觉定位测距的两种方式》，并做了相应改进，实测20m以内误差3%，80m以内误差10%模型推导图1. 投影模型相机主光轴与地面的夹角（俯仰角）已知量： 相机内参、相机与地面高度$H$、图像坐标系下某个目标与地面接触的点$pt$、世界坐标系下该点与相机的距离$D$如图1所示，$H$为相机与地面高度，$O3_P$所在的直线是相机光轴在地面上的投影，$M$为相机光轴与其地面投影的交点，$P$是目标$Q$与光心连线$O2_Q$在投影线上的投影，$O2_Q$为世界坐标系下已知的点与相机的距离可知，$\\angle{O3_O2_M}$即为所求俯仰角  需要注意的是，图1表示的是相机光轴俯视的情况，当相机光轴仰视时$M$点是不存在的STEP1: 计算夹角$\\angle{Q_O2_P}$\\[\\begin{aligned}P1\\_O1 =&amp; (pt.y-c_y)*\\mu\\\\P1\\_O2 =&amp;\\sqrt{(f_y*\\mu)^2+(P1\\_O1)^2}\\\\P1\\_Q1=&amp;(pt.x-c_x)*\\mu\\\\O2\\_Q1=&amp;\\sqrt{(f_x*\\mu)^2+((p1\\_O1)^2+(P1\\_Q1)^2)}\\\\\\angle{Q\\_O2\\_P}=&amp;acos\\left(\\frac{(P1\\_O2)^2+(O2\\_Q1)^2-(P1\\_Q1)^2}{2*P1\\_O2*O2\\_Q1}\\right)\\end{aligned} \\tag{1}\\]其中$\\mu$为相机的像元尺寸，$f$和$c$分别为相机焦距和主点STEP2: 计算$O2_P$\\[O2\\_P=\\frac{O2\\_Q}{1+tan(\\angle{Q\\_O2\\_P})} \\tag{2}\\]公式不难推，利用直角三角形$[O2_O3_P]$、$[O2_O3_Q]$和$[O2_Q_P]$之间的关系，这里省略STEP3: 计算俯仰角$\\angle{O3_O2_M}$\\[\\begin{aligned}O3\\_P=&amp;\\sqrt{(O2\\_P)^2-H^2}\\\\\\angle{P\\_O2\\_M}=&amp;atan\\left(\\frac{P1\\_O1}{f_y*\\mu}\\right)\\\\\\angle{P\\_O2\\_O3}=&amp;atan\\left(\\frac{O3\\_P}{H}\\right)\\\\\\angle{O3\\_O2\\_M}=&amp;\\angle{P\\_O2\\_O3}-\\angle{P\\_O2\\_M}\\end{aligned} \\tag{3}\\]计算图像点的空间距离已知量： 相机内参、相机与地面高度$H$、相机主光轴与地面的夹角、图像坐标系下某个目标与地面接触的点$pt$还是以图1为例STEP1: 计算夹角$\\angle{Q_O2_P}$与式(1)一样STEP2: 计算$O2_P$\\[\\begin{aligned}\\angle{P\\_O2\\_M}=&amp;atan\\left(\\frac{P1\\_O1}{f_y*\\mu}\\right)\\\\\\angle{O2\\_P\\_O3}=&amp;\\frac{\\pi}{2}-\\angle{Q\\_O2\\_P}-\\angle{P\\_O2\\_M}\\\\O3\\_P=&amp;\\frac{H}{tan(\\angle{O2\\_P\\_O3})}\\\\O2\\_P=&amp;\\sqrt{H^2+(O3\\_P)^2}\\end{aligned} \\tag{4}\\]STEP3: 计算距离$O2_Q$\\[\\begin{aligned}O2\\_Q=&amp;\\sqrt{(O2\\_P)^2+(tan(\\angle{Q\\_O2\\_P})*O2\\_P)^2}\\end{aligned} \\tag{5}\\]  容易证明，距离计算对于是相机光轴仰视的情况也是有效的使用激光点云辅助计算单目测距的精度受俯仰角的影响非常大，而使用图像上的点计算俯仰角不够稳定，并且无法处理光轴仰视的情况可以临时在相机旁边固定一个激光雷达，根据地面的激光点云获得雷达与地面的高度、俯仰角更精确的，可以进一步标定激光雷达与相机的外参，获得相机与地面的高度、俯仰角图2. 实测精度图2的DIFF1是直接假设相机俯仰角为90度（主光轴与地面平行），20m的误差为3.67m，80m的误差为30mDIFF2是利用点云获得俯仰角和高度，20m的误差为0.6m，80m的误差为9.21m代码https://github.com/yayo13/lidar2camera/tree/master/tool/mono_distance"
  },
  
  {
    "title": "相机成像模型",
    "url": "/posts/%E7%9B%B8%E6%9C%BA%E6%88%90%E5%83%8F%E6%A8%A1%E5%9E%8B/",
    "categories": "图像处理",
    "tags": "",
    "date": "2022-01-14 00:00:00 +0000",
    





    
    "snippet": "相机成像是以小孔成像为基础的，普通的镜头就是使用小孔成像模型但很显然，小孔模型无法做到大于180度的成像（投影到无限远），因此在实际中基于小孔模型有多种映射方式投影模型根据物点和像点的映射关系，可以分为如下表所列的6种模型      投影模型    模型函数    投影特性        透射映射(Perspective)    r=f*tan(θ)    普通镜头的小孔成像模型，可成像的角...",
    "content": "相机成像是以小孔成像为基础的，普通的镜头就是使用小孔成像模型但很显然，小孔模型无法做到大于180度的成像（投影到无限远），因此在实际中基于小孔模型有多种映射方式投影模型根据物点和像点的映射关系，可以分为如下表所列的6种模型      投影模型    模型函数    投影特性        透射映射(Perspective)    r=f*tan(θ)    普通镜头的小孔成像模型，可成像的角度θ不能大于90度        正交投影(Orthographic)    r=f*sin(θ)    一种广角相机模型，成像角度θ可达180度，不会产生近大远小的透视效果        等距映射(Equidistant)    r=f*θ    最简单的一种鱼眼相机模型，成像角度θ可达360度        等立体角投影(Equisolid angle)    r=2f*sin(θ/2)    比较常用的一种鱼眼相机模型，成像角度θ可达360度         球极投影(Stereographic)    r=2f*tan(θ/2)    一种鱼眼相机模型，成像角度θ可达360度，是一种将圆球面投影至平面的映射        等距圆柱投影(Equirectangular)    -    360度VR常用的模型，竖直方向用等距投影，水平方向用圆柱投影  物点经过上述投影模型首先投影到相机成像平面，然后经过内参转到像素坐标系图1. 投影模型素材取自《Models for the various classical lens projections》图2是模拟的站在隧道内中间位置使用不同相机模型看对面隧道壁的效果，素材取自WiKi-Fisheye lens图2. 投影模型的模拟效果等圆柱投影（正方形投影）是360度VR的主流方法，一般用两个鱼眼图像拼接为一张图图3. 等圆柱投影素材取自《为什么不同焦距的鱼眼镜头可以达到同样的视角？》      圆柱投影就是把物点投影到圆柱上再展开为平面，可以参考世界地图的效果        根据等距投影的公式，可知这种投影得到的图像宽高均为$\\pi*f$，因此也叫正方形投影，一般为了得到360度的效果，使用两张图拼接（如图3），因此这种VR视频的高宽比一般为1:2  鱼眼相机投影细节OpenCV的fisheye是基于Kannala【1】的方法实现的方法将正交、等距、等立体和球极的投影模型按泰勒级数展开，发现都可以用奇次多项式表示，由此得到一个通用模型\\[r_d=k_0\\theta+k_1\\theta^3+k_2\\theta^5+k_3\\theta^7+... \\tag{1}\\]在OpenCV中令$k_0=1$，取到9次方，因此标定的畸变参数有4个由此，空间中一点$[X_w, Y_w, Z_w]^T$到鱼眼图像的点$[u, v]^T$的变换过程为《鱼眼相机成像模型》：1）世界坐标系到相机坐标系\\[\\begin{aligned}[X_c, Y_c, Z_c]^T=&amp;RX+t\\\\x_c=&amp;\\frac{X_c}{Z_c}\\\\y_c=&amp;\\frac{Y_c}{Z_c}\\end{aligned} \\tag{2}\\]2）相机坐标系的多项式投影\\[\\begin{aligned}r^2=&amp;x_c^2+y_c^2\\\\\\theta=&amp;arctan(r)\\\\r_d=&amp;k_0\\theta+k_1\\theta^3+k_2\\theta^5+k_3\\theta^7+k_4\\theta^9\\\\x_d=&amp;\\frac{r_d}{r}x_c\\\\y_d=&amp;\\frac{r_d}{r}y_c\\end{aligned} \\tag{3}\\]3）内参投影到图像坐标系\\[\\begin{aligned}u=&amp;f_xx_d+c_x\\\\v=&amp;f_yy_d+c_y\\end{aligned} \\tag{4}\\]文献【1】Kannala J, Brandt S S. A generic camera model and calibration method for conventional, wide-angle, and fish-eye lenses[J]. IEEE transactions on pattern analysis and machine intelligence, 2006, 28(8): 1335-1340."
  },
  
  {
    "title": "【数据结构】（5）红黑树",
    "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-5-%E7%BA%A2%E9%BB%91%E6%A0%91/",
    "categories": "数据结构",
    "tags": "",
    "date": "2022-01-12 00:00:00 +0000",
    





    
    "snippet": "二叉树操作的平均时间复杂度是$\\Theta(log_2^n)$，但当其左右子树不平衡时会恶化（图1）图1. 左斜树  红黑树是一种可以自平衡的二叉树，可以保证在最坏情况下时间复杂度为$O(log_2^n)$  红黑树主要用来存储有序数据，效率非常高。Java集合中的TreeSet和TreeMap、C++中的set、map，以及Linux虚拟内存的管理，都是通过红黑树实现的本文大部分内容引自《...",
    "content": "二叉树操作的平均时间复杂度是$\\Theta(log_2^n)$，但当其左右子树不平衡时会恶化（图1）图1. 左斜树  红黑树是一种可以自平衡的二叉树，可以保证在最坏情况下时间复杂度为$O(log_2^n)$  红黑树主要用来存储有序数据，效率非常高。Java集合中的TreeSet和TreeMap、C++中的set、map，以及Linux虚拟内存的管理，都是通过红黑树实现的本文大部分内容引自《30张图带你彻底理解红黑树》红黑树的性质红黑树在普通二叉树的基础上，给每个结点增加了颜色属性（红/黑），并且规定了5个性质，在插入、删除过程中通过满足这些性质实现自平衡  性质1：每个结点要么是黑色，要么是红色  性质2：根结点是黑色  性质3：每个叶子结点是黑色（这里的叶子结点是指为空的叶子结点，由于二叉树中叶子结点的定义与此不同，因此下文不再考虑该性质）  性质4：每个红色结点的两个子结点一定都是黑色  性质5：任意一结点到每个叶子结点的路径都包含相同数量的黑色结点图2. 红黑树可以看到，其实红黑树并不能保证左右子树的层数一致，但可以确保左右子树的黑色结点层数相等，因此称红黑树的平衡为黑色完美平衡红黑树的三种操作红黑树在插入和删除过程中通过定义的三种操作实现以上5条性质的满足，达到自平衡变色结点的颜色由黑变红或由红变黑左旋以某个结点作为支点（旋转结点），其右子结点变为旋转结点的父结点，右子结点的左子结点变为旋转结点的右子结点，右子结点的右子结点保持不变，旋转结点的左子结点保持不变左旋的效果是将右子树分一部分到左子树图3. 左旋右旋以某个结点作为支点（旋转结点），其左子结点变为旋转结点的父结点，左子结点的右子结点变为旋转结点的左子结点，左子结点的左子结点保持不变，旋转结点的右子结点保持不变右旋的效果是将左子树分一部分到右子树图4. 右旋红黑树的插入和删除  查找操作不会改变红黑树，因此不做讨论  在讨论插入和删除之前，首先明确一下红黑树遵循的顺序：$lchild \\le father \\le rchild$  再考虑左旋和右旋，可以看到经过旋转后红黑树依然遵循上述顺序插入插入分为两个部分：首先是找到插入的位置，然后是插入后的自平衡。这里讨论插入后的自平衡操作不难理解，除了插入位置已有结点的情况外，其他插入位置都是叶子结点      情况    处理        红黑树为空树    把插入结点作为根结点，并设置为黑色        插入结点已存在    更新该结点的内容，保持结点颜色不变        插入结点的父结点为黑色    插入结点，并设置为红色        插入结点的父结点为红色    叔叔结点存在并且为红色    将PP改为红色，P、S改为黑色，I设置为红色，将PP设置为插入点         叔叔结点不存在或为黑色，并且插入结点的父结点是祖父结点的左子结点     插入结点是其父结点的左子结点     以PP为支点做右旋，将P设为黑色，PP和插入点设为红色         插入结点是其父结点的右子结点     以P为支点做左旋，将P设为插入点，回到2.1的情形         叔叔结点不存在或为黑色，并且插入结点的父结点是祖父结点的右子结点     插入结点是其父结点的右子结点     以PP为支点做左旋，然后将P设为黑色，将PP和插入点设为红色         插入结点是其父结点的左子结点     以P为支点做右旋，将P设为插入点，回到3.1的情形  表中前3种情况很简单，下面重点解释“插入结点的父结点为红色”的情况1）叔叔结点存在并且为红色图5. 叔叔结点存在并且为红色如图5，插入点是I，其可能插入P，也可能插入S，此时其祖父结点PP必然为黑色（性质4，红色的左右子结点为黑色）将PP改为红色，P、S改为黑色，I设置为红色即可；但此时因为PP变成红色，如果PP的父结点是红色就违反了性质4，因此需要将插入点设为PP迭代处理2）叔叔结点不存在或为黑色，并且插入结点的父结点是祖父结点的左子结点假设叔叔结点不存在，由于父结点为红，插入结点不能为红（性质4），而如果将插入结点设为黑，则该分支的黑色结点数与叔叔分支的结点数不一致（性质5）假设叔叔结点存在且为黑，由于父结点为红，需要将插入点设为黑色，但这样一来插入点的兄弟结点所在分支的黑色结点数少1个2.1）插入点是父结点的左子结点图6. 插入点是父结点的左子结点如图6所示，首先以PP为支点做右旋，然后将P设为黑色，PP设为红色，插入点设为红色（也可以将P设为红色，I和PP设为黑色，但如此一来需要考察P的父结点，如果其父结点为红色，则需要进一步处理）2.2）插入点是父结点的右子结点图7. 插入点是父结点的右子结点如图7所示，首先以P为支点做左旋，就变成2.1)的情形了，把P设为插入结点，按2.1）处理即可3）叔叔结点不存在或为黑色，并且插入结点的父结点是祖父结点的右子结点情况分析与2）类似3.1）插入点是父结点的右子结点图8. 插入点是父结点的右子结点如图8所示，首先以PP为支点做左旋，然后将P设为黑色，将PP和插入点设为红色3.2）插入点是父结点的左子结点图9. 插入点是父结点的左子结点如图9所示，首先以P为支点做右旋，就变成3.2)的情形了，把P设为插入点，按3.1)处理即可删除删除一个结点后需要用其子结点替换到该位置，否则就跟父结点断开了红黑树删除结点找替代结点有3种情况：  情况1：若删除结点无子结点，直接删除  情况2：若删除结点只有1个子结点，用该子结点替换删除结点  情况3：若删除结点有2个子结点，用后继结点（大于删除结点的最小结点）替换删除结点按照红黑树的大小顺序，后继结点就是删除结点右子树的最左结点（整个右子树中值最小的结点）而实际上，情况2和情况3是可以转化为情况1的：  情况2： 唯一的子结点替换为删除结点后，从红黑树整体的形状上可以认为删除的是子结点。若子结点没有子结点，则转为情况1；若子结点只有1个子结点，则转为情况2；若子结点有2个子结点，相当于转为情况3。  情况3： 替换删除结点的后继结点肯定没有左子结点，如果后继结点有右子结点，相当于转为情况2，否则转为情况1。图10. 删除结点的转化接下来分析每次转为情况1，也就是删除结点为叶子结点的情况。需要理解的是，红黑树的一次删除可能需要转换几次，意味着下面的过程也要递归走几次      情况    处理        替换结点是红色    把替换结点换成删除结点的颜色        替换结点是黑色    替换结点是其父结点的左子结点    替换结点的兄弟结点是红色    将S设为黑色，P设为红色，以P为支点做左旋，得到情景2.1.2.3        替换结点的兄弟结点是黑色    替换结点的兄弟结点的右子结点是红色，左子结点任意颜色    互换S和P的颜色，将SR设为黑色，以P为支点做左旋        替换结点的兄弟结点的右子结点是黑色，左子结点红色    将S设为红色，SL设为黑色，以S为支点做右旋，得到情景2.1.2.1        替换结点的兄弟结点的子结点都是黑色    将S设为红色，把P作为新的替换结点重新进行删除结点处理        替换结点是其父结点的右子结点    替换结点的兄弟结点是红色    将S设为黑色，P设为红色，以P为支点做右旋，得到情景2.2.2.3        替换结点的兄弟结点是黑色    替换结点的兄弟结点的左子结点是红色，右子结点任意颜色    将S和P互换颜色，将SL设为黑色，以P为支点做右旋        替换结点的兄弟结点的左子结点是黑色，右子结点红色    将S设为黑色，将SR设为黑色，以S为支点做左旋，得到情景2.2.2.1        替换结点的兄弟结点的子结点都是黑色    将S设为红色，把P做为新的替换结点重新进行删除结点处理  接下来所有例子都将R作为待替换的结点1）替换结点是红色由于替换结点是红色，删除也了不会影响红黑树的平衡，只要把替换结点的颜色设为删除的结点的颜色即可重新平衡2）替换结点是黑色2.1）替换结点是其父结点的左子结点2.1.1）替换结点的兄弟结点是红色根据性质4，可知兄弟结点的父结点肯定为黑色图11.替换结点的兄弟结点是红色 将S设为黑色，P设为红色，以P为支点做左旋，得到情景2.1.2.32.1.2）替换结点的兄弟结点是黑色此时兄弟结点的父结点颜色不确定，需要考虑多种情况2.1.2.1）替换结点的兄弟结点的右子结点是红色，左子结点任意颜色即将删除的左子树的一个黑色结点，显然左子树的黑色结点少1了图12.右子结点是红色，左子结点任意颜色 互换S和P的颜色，将SR设为黑色，以P为支点做左旋2.1.2.2）替换结点的兄弟结点的右子结点是黑色，左子结点红色图13.右子结点是黑色，左子结点红色 将S设为红色，SL设为黑色，以S为支点做右旋，得到情景2.1.2.12.1.2.3）替换结点的兄弟结点的所有子结点是黑色图14.所有子结点是黑色 将S设为红色，把P作为新的替换结点重新进行删除结点处理2.2）替换结点是其父结点的右子结点这类情况的处理与2.1是对称的2.2.1）替换结点的兄弟结点是红结点图15.替换结点的兄弟结点是红结点 将S设为黑色，P设为红色，以P为支点做右旋，得到情景2.2.2.32.2.2）替换结点的兄弟结点是黑结点2.2.2.1）替换结点的兄弟结点的左子结点是红结点，右子结点任意颜色图16.左子结点是红结点，右子结点任意颜色 将S和P互换颜色，将SL设为黑色，以P为支点做右旋2.2.2.2）替换结点的兄弟结点的左子结点为黑结点，右子结点为红结点图17.左子结点为黑结点，右子结点为红结点 将S设为黑色，将SR设为黑色，以S为支点做左旋，得到情景2.2.2.12.2.2.3）替换结点的兄弟结点的子结点都为黑结点图18.替换结点的兄弟结点的子结点都为黑结点 将S设为红色，把P做为新的替换结点重新进行删除结点处理"
  },
  
  {
    "title": "【数据结构】（4）树&二叉树",
    "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-4-%E6%A0%91&%E4%BA%8C%E5%8F%89%E6%A0%91/",
    "categories": "数据结构",
    "tags": "",
    "date": "2022-01-05 00:00:00 +0000",
    





    
    "snippet": "树树是一种非线性的存储结构，存储具有“一对多”关系的数据元素图1. 树如图1所示，数据$A$与$B$、$C$、$D$有关系，$B$与$E$、$F$有关系      名词    解释        结点    结点    树结构中的每一个元素都是一个结点        父结点    图1中A是B、C、D的父结点，B是E、F的父结点        根结点    如果某个结点没有父结点，那么这个结...",
    "content": "树树是一种非线性的存储结构，存储具有“一对多”关系的数据元素图1. 树如图1所示，数据$A$与$B$、$C$、$D$有关系，$B$与$E$、$F$有关系      名词    解释        结点    结点    树结构中的每一个元素都是一个结点        父结点    图1中A是B、C、D的父结点，B是E、F的父结点        根结点    如果某个结点没有父结点，那么这个结点就是整棵树的根结点，图1中A是根结点        叶子结点    如果结点没有任何子结点，那么这个结点就是叶子结点，图1中K、L、F、G、M、I、J是叶子结点        子树    从某个结点出发，到其下面的所有叶子结点，也组成了一棵树，称做该结点父结点的子树。如从结点B出发得到的BEFKL是A的一棵子树，A一共有3棵子树        度    一个结点的度是该结点的子树数量（分支数），如A的度是3；一棵树的度是各结点度的最大值        层次    根结点是第1层，其子结点是第2层...        深度/高度    一棵树的高度是其中所有结点的最大层数，如图1的高度是4  树一般用广义表来表示，如图1的树可表示为(A , ( B ( E ( K , L ) , F ) , C ( G ) , D ( H ( M ) , I , J ) ) )二叉树二叉树满足：  度不超过2  是有序树（左子树和右子树的顺序不能变，即使只有一个孩子结点，也区分左子树右子树）二叉树有以下性质：  第$i$层最多有$2^{i-1}$个结点  如果二叉树的深度为$K$，那么其最多有$2^K-1$个结点  令叶子结点数为$n_0$，度为2的结点数为$n_2$，则$n_0=n_2+1$（证明见《什么是二叉树，二叉树及其性质详解》）对于$n$个结点组成的二叉树，搜索、删除、插入的平均时间复杂度为$\\Theta(log_2^n)$，最坏时间复杂度为$O(n)$满二叉树如果二叉树中除了叶子结点，其他所有结点的度都为2，并且所有叶子结点都在同一层，则称为满二叉树图2. 满二叉树满二叉树满足：  第$i$层的结点数为$2^{i-1}$个  深度为$K$的满二叉树有$2^K-1$个结点  结点数为$n$的满二叉树的深度为$log_2^{(n+1)}$完全二叉树如果二叉树中去掉最高一层结点为满二叉树，并且最后一层的结点依次从左到右分布，则为完全二叉树图3. 完全二叉树完全二叉树满足：  结点数为$n$的完全二叉树的深度为$\\lfloor log_2^n+1\\rfloor$如果将完全二叉树的结点从上到下从左到右编号，那么对于编号为$i$的结点1）当$i&gt;1$时，其父结点为$\\lfloor n/2\\rfloor$2）其左孩子为$2\\times i$，右孩子为$2\\times i+1$二叉树存储结构二叉树有顺序和链式两种存储结构顺序存储结构用数组存储二叉树，顺序为由上往下、由左往右，空结点用特殊符号标记图4. 顺序存储链式存储结构完全二叉树的情况比较少，此时顺序存储比较浪费空间，因此一般用链式存储图5. 链式存储struct Node{    DataType data;    Node* lchild, rchild;    Node* parent;};二叉树的遍历二叉树有：先序遍历、中序遍历、后序遍历、层次遍历其中先序、中序、后序指的是根结点与左右孩子的顺序关系，属于深度优先遍历，一般用递归实现层次遍历又称广度优先遍历，一般用队列实现1）先序遍历先序遍历是先访问根结点，然后访问左子树，最后访问右子树图2的遍历结果为1、2、4、5、3、6、72）中序遍历先序遍历是先访问左子树，然后访问根结点，最后访问右子树图2的遍历结果为4、2、5、1、6、3、73）后序遍历先序遍历是先访问左子树，然后访问右子树，最后访问根结点图2的遍历结果为4、5、2、6、7、3、14）层次遍历层次遍历是按二叉树的层次从上到下从左到右访问图2的遍历结果为1、2、3、4、5、6、7代码实现import ctypesimport queueclass node:    def __init__(self, id_data, id_left, id_right):        self._data   = id_data        self._lchild = id_left        self._rchild = id_right    def add_lchild(self, id_left):        self._lchild = node(id_left, -1, -1)        def add_rchild(self, id_right):        self._rchild = node(id_right, -1, -1)    def has_lchild(self):        if self._lchild != -1: return True        else: return False    def has_rchild(self):        if self._rchild != -1: return True        else: return False    def get_value(self):        return ctypes.cast(self._data, ctypes.py_object).value    def get_lchild(self):        return self._lchild    def get_rchild(self):        return  self._rchildclass binary_tree:    def __init__(self, datas):        self._tree = -1        if len(datas) &gt; 0 and datas[0] != '^':            self._tree = node(id(datas[0]), -1, -1)            self.build_binary_tree(datas, 0, self._tree)    def build_binary_tree(self, datas, ind_node, node_tree):        id_lchild, ind_lchild = -1, 2*ind_node+1        id_rchild, ind_rchild = -1, 2*ind_node+2        if len(datas) &gt; ind_lchild and datas[ind_lchild] != '^':            id_lchild = id(datas[ind_lchild])        if len(datas) &gt; ind_rchild and datas[ind_rchild] != '^':            id_rchild = id(datas[ind_rchild])        if id_lchild != -1:            node_tree.add_lchild(id_lchild)            self.build_binary_tree(datas, ind_lchild, node_tree.get_lchild())        if id_rchild != -1:            node_tree.add_rchild(id_rchild)            self.build_binary_tree(datas, ind_rchild, node_tree.get_rchild())    def preorder_traverse(self, node_tree=-1):        \"\"\"        先序遍历        \"\"\"        if node_tree == -1: node_tree = self._tree        print(node_tree.get_value())        if node_tree.has_lchild():            self.preorder_traverse(node_tree.get_lchild())        if node_tree.has_rchild():            self.preorder_traverse(node_tree.get_rchild())    def midorder_traverse(self, node_tree=-1):        \"\"\"        中序遍历        \"\"\"        if node_tree == -1: node_tree = self._tree        if node_tree.has_lchild():            self.midorder_traverse(node_tree.get_lchild())        print(node_tree.get_value())        if node_tree.has_rchild():            self.midorder_traverse(node_tree.get_rchild())    def postorder_traverse(self, node_tree=-1):        \"\"\"        后序遍历        \"\"\"        if node_tree == -1: node_tree = self._tree        if node_tree.has_lchild():            self.postorder_traverse(node_tree.get_lchild())        if node_tree.has_rchild():            self.postorder_traverse(node_tree.get_rchild())        print(node_tree.get_value())    def levelorder_traverse(self):        \"\"\"        层次遍历        \"\"\"        q = queue.Queue()        q.put(self._tree)        while not q.empty():            node_tree = q.get()            print(node_tree.get_value())            if node_tree.has_lchild():                q.put(node_tree.get_lchild())            if node_tree.has_rchild():                q.put(node_tree.get_rchild())def main():    datas_ = [1, 2, 3, 4, '^', '^', 7]    tree_ = binary_tree(datas_)    tree_.preorder_traverse()    tree_.midorder_traverse()    tree_.postorder_traverse()    tree_.levelorder_traverse()if __name__ == '__main__':    main()"
  },
  
  {
    "title": "卷积、反卷积、膨胀卷积",
    "url": "/posts/%E5%8D%B7%E7%A7%AF-%E5%8F%8D%E5%8D%B7%E7%A7%AF-%E8%86%A8%E8%83%80%E5%8D%B7%E7%A7%AF/",
    "categories": "深度学习",
    "tags": "",
    "date": "2021-12-30 00:00:00 +0000",
    





    
    "snippet": "卷积api doctf.nn.conv2d(    input, filters, strides, padding, data_format='NHWC', dilations=None,    name=None)  input: 输入tensor，根据data_format格式排列，维数不小于4  filters: 卷积核，根据data_format格式排列（[filter_heigh...",
    "content": "卷积api doctf.nn.conv2d(    input, filters, strides, padding, data_format='NHWC', dilations=None,    name=None)  input: 输入tensor，根据data_format格式排列，维数不小于4  filters: 卷积核，根据data_format格式排列（[filter_height, filter_width, in_channels, out_channels]）  strides: 卷积滑动步长  padding: 边界处理方式，有’SAME’和’VALID’两种，也可以直接指定与data_format匹配的pad值，如[[0, 0], [pad_top,pad_bottom], [pad_left, pad_right], [0, 0]]VALID模式当padding=’VALID’时，输入边界不做扩展\\[O=\\lceil\\frac{I-k}{s}\\rceil +1 \\tag{1}\\]其中$I$为输入$H$或$W$，$O$为输出对应的$H$或$W$，$k$为卷积核尺寸，$s$为卷积步长，$\\lceil \\rceil$为向上取整SAME模式当padding=’SAME’时，输入边界根据kernel和stride做补0扩展\\[O=\\lceil\\frac{I}{s} \\rceil \\tag{2}\\]‘SAME’的意思只是要保证输入的每个元素都能覆盖，并不保证输出与输入尺寸一致扩充的尺寸计算如下if (in_height % strides[1] == 0):  pad_along_height = max(filter_height - strides[1], 0)else:  pad_along_height = max(filter_height - (in_height % strides[1]), 0)if (in_width % strides[2] == 0):  pad_along_width = max(filter_width - strides[2], 0)else:  pad_along_width = max(filter_width - (in_width % strides[2]), 0)pad_top = pad_along_height // 2pad_bottom = pad_along_height - pad_toppad_left = pad_along_width // 2pad_right = pad_along_width - pad_left反卷积在语义分割、自动编码器等模型中需要执行上采样，此时除了用插值的方法，还可以用反卷积需要说明的是，反卷积只能扩大输出尺寸，并不能还原出卷积前的数值，与卷积并不是可逆关系《深入理解深度学习中的反 (转置) 卷积》介绍的比较详细，推荐阅读《Deconvolution and Checkerboard Artifacts》分析了反卷积容易导致棋盘格效果的原因，并认为改善的方法有1）确保反卷积的kerner_size能被strides整除2）或者放弃反卷积，用插值+卷积的方法实现上采样反卷积首先对输入做补0扩展，然后做stride=1的卷积api doctf.nn.conv2d_transpose(    input, filters, output_shape, strides, padding='SAME',    data_format='NHWC', dilations=None, name=None)  filters: 卷积核，需要注意这里kernel的顺序为[height,width, output_channels, in_channels]  strides: 这里不再是步长的意思，而是表示在输入各元素之间补$strides-1$个0（内部扩充）  padding: 输入与conv2d一致，两种模式代表了不同的边界扩充方式（外部扩充）在根据strides元素做内部扩充后，再根据’SAME’和’VALID’做外部扩充VALID模式在内部扩充完的基础上，于周围填充宽度为$k-1$的0元素（$k$为核尺寸）input  = tf.constant(1.0, shape=[1,2,2,1])kernel = tf.constant(1.0, shape=[3,3,1,1])outpout = tf.nn.conv2d_transpose(input, kernel, output_shape=[1,5,5,1], strides=[1,2,2,1], padding='VALID')由于strides=2，因此各元素间填充2-1=1个0，变成$3\\times 3$然后在周围填充3-1=2个0，变成$7\\times 7$最后用$3\\times 3$的kernel卷积（步长恒为1），输出尺寸为$5\\times 5$图1. VALIDSAME模式在内部扩充完的基础上，根据output_shape的尺寸在周围做填充input  = tf.constant(1.0, shape=[1,2,2,1])kernel = tf.constant(1.0, shape=[3,3,1,1])outpout = tf.nn.conv2d_transpose(input, kernel, output_shape=[1,3,3,1], strides=[1,2,2,1], padding='SAME')由于strides=2，因此各元素间填充2-1=1个0，变成$3\\times 3$由于输出尺寸为$3\\times 3$，kernel尺寸为$3\\times 3$, 因此外部扩充1个0，变成$5\\times 5$最后用$3\\times 3$的kernel卷积（步长恒为1），输出尺寸为$3\\times 3$反卷积的计算方式反卷积的基本思路是，首先将卷积转化为矩阵乘的形式，然后将卷积核转置，最后由输出计算输入图2. 卷积转化为矩阵乘图3. 转置求输入无论是卷积还是反卷积，SAME只跟strides有关，VALID跟kernel和strides都有关膨胀卷积参考《膨胀卷积(空洞卷积)学习篇》  反卷积是对输入图像做尺寸扩大，膨胀卷积则通过“扩大卷积核”提高感受野感受野\\[RF_{l+1}=RF_l+(k-1)S_l\\tag{3}\\]  $RF_l$: 第$l$层感受野，$RF_{l+1}$: 第$l+1$层感受野  $k$: 第$l+1$层卷积核尺寸  $S_l$: 前$l$层的步长之积图4. 感受野如图4所示，第一层每个点的感受野是$3\\times 3$，第三层每个点的感受野是$7\\times 7$膨胀卷积原理膨胀卷积就是对原始卷积核做“膨胀”，增加卷积核的尺寸，这样根据式(3)就可以有效提高感受野图5. 卷积核膨胀\\[K_c=S\\times (K_o-1)+1\\tag{4}\\]  $S$: 膨胀因子  $K_o$: 原始卷积核尺寸  $K_c$: 膨胀后卷积核尺寸图5是对原始卷积核做$S=2$的膨胀# dilation就是膨胀因子torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)# dilation_rate就是膨胀因子tf.keras.layers.Conv2D(    filters,    kernel_size,    strides=(1, 1),    padding='valid',    data_format=None,    dilation_rate=(1, 1),    groups=1,    activation=None,    use_bias=True,    kernel_initializer='glorot_uniform',    bias_initializer='zeros',    kernel_regularizer=None,    bias_regularizer=None,    activity_regularizer=None,    kernel_constraint=None,    bias_constraint=None,    **kwargs)膨胀卷积的问题及改进从图5可以看到，虽然卷积核膨胀后变大了，但实际参与计算的还是$3\\times 3=9$个位置，中间会出现大量“空洞”（这也是膨胀卷积又称为空洞卷积的原因）空洞的存在会丢失数据之间的连续性核完整性，尤其对于小目标一个改进的方法是确保：  叠加卷积的膨胀因子不能有大于1的公约数，如$[2,4,6]$不理想，会有空洞  叠加卷积的膨胀因子设计成锯齿状结构，如$[1,2,5,1,2,5]$，尽量补上其他层的空洞"
  },
  
  {
    "title": "【数据结构】（3）散列表",
    "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-3-%E6%95%A3%E5%88%97%E8%A1%A8/",
    "categories": "数据结构",
    "tags": "",
    "date": "2021-12-22 00:00:00 +0000",
    





    
    "snippet": "散列表（哈希表）是根据关键码key直接进行访问的数据结构，是为了解决如何高效插入、删除、查询记录的问题  数组：寻址容易（直接根据下标寻址，$O(1)$），但插入和删除复杂度高  链表：寻址困难（需要依次遍历，$O(n)$），但插入和删除复杂度低散列表是数组的扩展，不同的是其考虑用有限容量记录任意长度的数据，属于多对一的映射建立上述多对一映射的方法就是散列函数（hash函数），原始数据经过散...",
    "content": "散列表（哈希表）是根据关键码key直接进行访问的数据结构，是为了解决如何高效插入、删除、查询记录的问题  数组：寻址容易（直接根据下标寻址，$O(1)$），但插入和删除复杂度高  链表：寻址困难（需要依次遍历，$O(n)$），但插入和删除复杂度低散列表是数组的扩展，不同的是其考虑用有限容量记录任意长度的数据，属于多对一的映射建立上述多对一映射的方法就是散列函数（hash函数），原始数据经过散列函数映射后得到的值为散列值（hash值），根据散列值在散列表内做插入、删除、查询操作散列函数散列函数（hash函数）实现原始数据$key$到散列值$index$的映射举例：假设学生的学号组成为“年级+班级+入学编号”，如030211，那么一个简单的hash函数就是取其后两位数。这样$hash(030211)=11$位置的散列表内存放的就是030211的信息由此可以得到散列函数的几个特点：  输出是一个非负整数  对于相同的输入，输出也相同  对于不同的输入，输出尽可能不同  应尽量满足均匀散列假设：每个输入都被等可能散列到散列表内的任意位置比较有名的MD5、SHA1、CRC就是hash函数以MD5为例，其可以将任意长度信息输出为128位，一般用于  防止文件被篡改：通过比较MD5序列确定拷贝/下载的文件是否与原始文件一致（对应散列函数的第2个特点）  防止直接看到明文：UNIX中将密码做MD5保存，攻击者无法根据保存的值推断原始密码，但系统可以将用户输入的密码做MD5与保存的MD5比较确定是否登陆成功散列冲突由于本质上是一个多对少映射，因此总是存在不同的输入被映射到同一输出的情况，我们称之为散列冲突通常有开放寻址法和链表法两种方式处理开放寻址法这种方法散列表的一个槽位保存一个数据，其处理冲突的办法是，如果映射到的槽位已经保存了数据，则按照规则保存到另一个空槽位根据规则的不同，开放寻址法可以分为3种1）线性探测插入：如果映射到的槽位为空，则直接插入，否则依次找到下一个空槽位插入查询：如果映射到的槽位为空，则查询失败，如果有数据，则比较待查询数据的$key$和当前槽位的$key$，匹配上则查询成功，否则依次往后查询删除：如果查询成功，则将该槽位的数据删除，并且标记为deleted（如果不标记，那么下一次查询其他数据时会因为该位置为空停止查询）2）二次探测与线性探测过程一样，前者搜索步长为1，后者为原来的二次方即：$hash(key)+0$、$hash(key)\\pm 1^2$、$hash(key)\\pm 2^2$、$hash(key)\\pm 3^2$3）双重散列如果映射到的槽位有冲突，使用另一个散列函数重新映射链表法显然，开放寻址法随着空闲槽位越来越少，散列冲突的概率会越来越高，寻址操作复杂度越来越高，最终插入、查询、删除失败链表法是一种更为常用的解决散列冲突的方法图1. 链表法散列表中每个下标对应一个链表，所有经过散列函数得到的相同散列值的元素，都放到对应下标的链表中基于链表法的散列表相当于数组+链表的组合体"
  },
  
  {
    "title": "地面点云分割算法(DZ)",
    "url": "/posts/%E5%9C%B0%E9%9D%A2%E7%82%B9%E4%BA%91%E5%88%86%E5%89%B2%E7%AE%97%E6%B3%95(DZ)/",
    "categories": "点云处理",
    "tags": "",
    "date": "2021-12-07 00:00:00 +0000",
    





    
    "snippet": "  本文是《Fast Segmentation of 3D Point Clouds for Ground Vehicles》【1】的解析及实现  同时做了部分改进提高分割的精度  部分内容参考了文章《地面点云快速分割算法—linefit_ground_segmentation》基本步骤本方法的思路比较简单：  通过点云的前向x和纵向y计算俯视下点的距离d，与该点的高度z组成二维数据  可以...",
    "content": "  本文是《Fast Segmentation of 3D Point Clouds for Ground Vehicles》【1】的解析及实现  同时做了部分改进提高分割的精度  部分内容参考了文章《地面点云快速分割算法—linefit_ground_segmentation》基本步骤本方法的思路比较简单：  通过点云的前向x和纵向y计算俯视下点的距离d，与该点的高度z组成二维数据  可以想象，对于路面的点提取的DZ数据可以拟合成斜率较小的线段，而路面上的DZ数据则会拟合为接近垂直的线段  该方法与双目中的UV视差有点像，都是对原始数据做降维处理拟合直线，通过斜率区分地面DZ数据生成1) 按固定的角度间隔将360度等分为不同区间，每个区间称为一个segment2) 对每个segment，按固定的距离间隔等分为不同的区间，称为一个bin3) 遍历每个三维点，根据$x-y$计算其距离及角度，其中角度决定在哪个segment，距离决定在哪个bin4) 对每个bin，记录所有落在其中的点中高度值最小的点（因此每个bin其实只有1个点）图1. livox全部点云投影在xy平面图1中每个格子为一个bin，每个小扇形为一个segment直线拟合对每个segment，遍历落在所有bin的点，对其做直线拟合图2. 上下分别为不同segment的拟合情况，红色为落在bin中的点，绿色为拟合的线图2中横坐标是DZ点的距离d，纵坐标是其高度z直线拟合有如下几个约束：  当前点与上一个点的距离d之差不能大于指定阈值LONG_LINE_THRE  当前点与上一个点的高度z之差不能大于指定阈值HEIGHT_THRESH  当前点与已拟合的直线垂直距离不能大于指定阈值MAX_LINE_ERROR  当前点加入后拟合的直线斜率不能大于指定阈值MAX_LINE_SLOPE具体实现中分为2种情况：1）如果只有2个待拟合的点，只要其距离之差或者高度之差不满足要求，则丢弃2）如果有多于2个待拟合的点，对新加入的点p2.1）拟合直线2.2）如果p与上一个点的距离d之差大于指定阈值2.2.1）如果拟合的点足够多(此时地面的可能性高一些)，则计算点p与直线的距离error，如果error大于指定阈值，则认为点p不满足要求，跳2.6)2.2.2）如果拟合的点比较少，认为点p不满足要求2.3）查找所有参与拟合直线的点与直线的最大距离max_error，如果大于指定阈值，则认为拟合失败，点p不满足要求，跳2.6)2.4）如果拟合的直线斜率大于指定阈值，则认为点p不满足要求，跳2.6)2.5）认为点p满足要求，继续考察后面的点2.6）记录点p之前的点拟合的直线，并开始下一轮新直线的拟合地面点云分割  遍历每个点云，根据索引找到其对应的DZ点$p$  计算$p$所在segment和相邻segment中所有线段的垂直距离，如果小于指定阈值SEARCH_DIST_THRE，则认为该点属于地面实验发现阈值SEARCH_DIST_THRE很不好设定，如果比较小则地面过滤不干净，如果稍大则地面以上点会被过滤经过分析，是因为拟合的直线引入了误差，参与拟合的点越多，上下的误差越不可控图3. 直线拟合引入的误差如图3，上图绿色是根据红色的DZ点拟合的直线，下图黄色点是落在当前segment下所有点的DZ投影，可以看到，由于直线拟合带来的误差，黄色点中真实属于地面的点与直线有较大偏差因此设定两个阈值：1）第一个阈值SEARCH_DIST_THRE1比较宽松，点$p$与拟合的直线求距离，根据该阈值做分割，结果会有部分地面以上的点2）第二个阈值SEARCH_DIST_THRE2比较严格，首先在参与拟合的所有点中查找，找到横坐标d与点$p$最近的两个点$p_1$、$p_2$，计算$p$到$p_1$、$p_2$组成的直线的距离，根据该阈值做分割，得到最终的地面点图4. 改进后地面分割的效果图4上部是原始的点云，包含地面和车头；下部右边是改进前单阈值的效果，可以看到为了尽可能过滤干净地面，部分车轮也被切掉了下部左边是改进后双阈值的效果，车轮被保留并且地面也被过滤干净了代码实现ground_extract.hpp、ground_extract.cpphttps://github.com/yayo13/ros_livox_camera/tree/master/lidar_camera_merge/src参考文献【1】Himmelsbach M ,  Hundelshausen F V ,  Wuensche H J . Fast segmentation of 3D point clouds for ground vehicles[J]. IEEE, 2010."
  },
  
  {
    "title": "【数据结构】（2）排序算法分析",
    "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-2-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90/",
    "categories": "数据结构",
    "tags": "",
    "date": "2021-11-29 00:00:00 +0000",
    





    
    "snippet": "普通排序            排序算法      时间复杂度      空间复杂度      是否稳定                  插入排序      $\\Theta(n^2)$      $O(n)$      稳定              冒泡排序      $\\Theta(n^2)$      $O(n)$      稳定              选择排序      $\\T...",
    "content": "普通排序            排序算法      时间复杂度      空间复杂度      是否稳定                  插入排序      $\\Theta(n^2)$      $O(n)$      稳定              冒泡排序      $\\Theta(n^2)$      $O(n)$      稳定              选择排序      $\\Theta(n^2)$      $O(n)$      不稳定              归并排序      $\\Theta(nlog^n)$      $O(n)$      稳定              快速排序      $\\Theta(nlog^n)$      $O(n)$      不稳定              堆排序      $\\Theta(nlog^n)$      $O(n)$      不稳定      一个稳定的排序算法是指：对于初始序列中相同的元素，排序后相对位置没有发生变化插入排序插入排序假定当前子序列已经排好序，对于下一个元素，与子序列逐个比较，将其插入合适的位置# 插入排序def insertion_sort(data):    for i in range(len(data)):        key = data[i]        j = i-1        while j &gt;= 0 and data[j] &gt; key:            data[j+1] = data[j]            j -= 1        data[j+1] = key冒泡排序冒泡排序为嵌套双层循环结构      外部循环$n-1$次，每次外循环将当前子序列中最大/最小元素“推到”序列顶端        内部循环则比较相邻元素，将更大/更小元素swap到前面，看上去就像鱼吐泡泡往上冒  虽然冒泡排序与插入排序的最坏时间复杂度一样，但更建议用插入排序，因为其交换次数少于冒泡（所以STL中的qsort都会以插入排序作为快速排序的补充来处理少量元素）# 冒泡排序def bubble_sort(data):    for i in range(len(data)-1):        swaped = False        for k in range(len(data)-i-1):            if data[k] &gt; data[k+1]:                tmp       = data[k+1]                data[k+1] = data[k]                data[k]   = tmp                swaped    = True        if not swaped:            break选择排序选择排序将序列分为已排序区间和未排序区间，每次从未排序区间找最小值与该区间第一个元素交换，交换后的元素并入已排序区间选择排序与插入排序和冒泡排序一样都是$O(n^2)$的时间复杂度，但并不是稳定排序，因此一般不用# 选择排序def choose_sort(data, start, end):    if start == end: return    min_value = data[start]    min_index = start    for i in range(start+1, end+1):        if data[i] &lt; min_value:            min_value = data[i]            min_index = i    tmp = data[start]    data[start] = data[min_index]    data[min_index] = tmp    choose_sort(data, start+1, end)归并排序归并排序是一个递归结构的排序方法  每个递归将序列分为两半，分别对两部分序列做排序  排序后的两部分序列做合并操作归并排序过程可以展开为二叉树，时间复杂度是$O(nlog^n)$# 归并排序def merge_sort(data, start, end):    if start &gt;= end: return    mid = int((end-start)/2)+start    merge_sort(data, start, mid)    merge_sort(data, mid+1, end)    merge(data, start, mid, end)def merge(data, start, mid, end):    left  = data[start:mid+1]    right = data[mid+1:end+1]    ind_left  = 0    ind_right = 0    for i in range(start, end+1):        if ind_left &lt; len(left) and ind_right &lt; len(right):            if left[ind_left] &lt; right[ind_right]:                data[i]   = left[ind_left]                ind_left += 1            else:                data[i]    = right[ind_right]                ind_right += 1        elif ind_left &gt;= len(left):            data[i] = right[ind_right]            ind_right += 1        else:            data[i] = left[ind_left]            ind_left += 1快速排序快速排序也是一个递归结构的排序方法  首先选择一个base（一般取序列第一个），将序列partial为左右两部分，左部分比base小，右部分比base大  递归的处理左右两部分快速排序的重点在partial：  两个指针分别指向序列首尾两端  首先后指针从后往前遍历，如果比base大则继续遍历，否则停止；将该指针指向的值赋给前指针的位置  前指针指向的值被修改后，开始往后遍历，如果比base小则继续，否则停止；将该指针指向的值赋给后指针的位置快速排序的时间复杂度是$O(nlog^n)$，并且是原地排序# 快速排序def quick_sort(data, start, end):    if start &gt;= end: return     mid = partial(data, start, end)    quick_sort(data, start, mid-1)    quick_sort(data, mid+1, end)def partial(data, start, end):    base = data[start]    ind1, ind2 = start, end    direction = -1    while ind1 &lt; ind2:        if direction == -1:            if data[ind2] &gt;= base:                ind2 -= 1            else:                data[ind1] = data[ind2]                ind1 += 1                direction = 0        else:            if data[ind1] &lt;= base:                ind1 += 1            else:                data[ind2] = data[ind1]                ind2 -= 1                direction = -1    data[ind1] = base    return ind1堆排序堆排序每次将序列整理为一个最大/最小堆，这样根节点的值就是当前序列最大/最小值最大/小堆是一个二叉树结构，节点值比其左右孩子都大/小堆排序的时间复杂度是$O(nlog^n)$，但频繁的建堆过程会影响性能，因此一般更倾向用快速排序# 堆排序def heap_sort(data):    build_max_heap(data, 0, len(data)-1)    for i in range(len(data)-1):        end = len(data)-i-1        tmp       = data[end]        data[end] = data[0]        data[0]   = tmp        build_max_heap(data, 0, end-1)def build_max_heap(data, start, end):    length = end-start+1    node_  = start+int(length/2)-1    for i in range(node_, -1, -1):        tmp_value = data[i]        max_ind   = i        left_child = 2*i+1        if left_child &lt;= end and data[left_child] &gt; data[max_ind]:            max_ind   = left_child        right_child = left_child+1        if right_child &lt;= end and data[right_child] &gt; data[max_ind]:            max_ind   = right_child                if max_ind != i:            data[i] = data[max_ind]            data[max_ind] = tmp_value            build_max_heap(data, max_ind, end)线性排序上述普通排序对任意数据都适用，但时间复杂度的极限是$O(nlog^n)$线性排序的时间复杂度可以达到$O(n)$，但对数据有要求（比如需要已知数据范围，或者数据分布比较均匀等）            排序算法      时间复杂度      空间复杂度      是否稳定                  桶排序      $\\Theta(n)$      $O(n*k)$      稳定              计数排序      $\\Theta(n)$      $O(n+k)$      稳定              基数排序      $\\Theta(kn)$      $O(n+k)$      稳定      桶排序桶排序首先将数据按大小分到对应范围的“桶”内，然后对每个桶内的数据做快速排序  桶排序适于处理海量数据（外部排序，即无法一次性在内存里载入所有数据）  桶排序要求数据分布尽可能均匀（可以想象极端情况会退化到快速排序）假设数据量为$n$，“桶”数量为$m$，那么总的排序复杂度为$\\Theta(nlog^{n/m})$当$m$接近$n$，时间复杂度就趋向于$\\Theta(n)$计数排序计数排序首先统计序列中每个数出现的次数然后对统计结果从前往后做累加得到$counting$，这样counting位置$i$的值表示序列中不大于$i$的数量最后对序列从后往前遍历，如果值为$k$，则其在排序后的序列位置为$counting[k]-1$，接着做$counting[k]-=1$表示该位置元素减一  计数排序要求已知数据范围，并且为正整数（可以通过数据转换规避正整数的要求）  计数排序适用于数据重复较多的情况def count_sort(data, min, max):    counting = np.zeros((max-min+1,), np.int32)    for v in data:        counting[v-min] += 1    for ind in range(max-min):        counting[ind+1] += counting[ind]    data_ = data[:]    ind_sorted = list(range(len(data)))    # for ind in range(len(data_)):    # 注意要用逆序，否则基数排序会出错    for ind in range(len(data_)-1,-1,-1):        ind_sort = counting[data_[ind]-min]-1        data[ind_sort] = data_[ind]        ind_sorted[ind_sort] = ind        counting[data_[ind]-min] -= 1    return ind_sorted基数排序基数排序对序列中所有数据按位排序假设所有数据有$k$位（小于$k$位的数据前面补0），那么首先根据个位对数据排序，然后根据十位排序…排序方法可以用桶排序和计数排序（已知范围为[0,9]）  基数排序要求已知数据最大位数，并且为正整数（可以通过数据转换规避正整数的要求）def radix_sort(data, bits):    radix = 1    ind_sorted = list(range(len(data)))    for b in range(bits):        bit_data = []        for ind in ind_sorted:            v_in_b = int(data[ind]/radix)%10            bit_data.append(v_in_b)        ind_sorted_ = count_sort(bit_data, 0, 9)        ind_sorted_updated = []        for ind_ in ind_sorted_:            ind_sorted_updated.append(ind_sorted[ind_])        ind_sorted = ind_sorted_updated[:]        radix *= 10    data_ = data[:]    for ind_ in range(len(data)):        data[ind_] = data_[ind_sorted[ind_]]"
  },
  
  {
    "title": "【数据结构】（1）计算复杂度",
    "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-1-%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%88%86%E6%9E%90/",
    "categories": "数据结构",
    "tags": "",
    "date": "2021-11-29 00:00:00 +0000",
    





    
    "snippet": "为了刻画算法效率，我们分析当输入规模$n$足够大时，算法执行的基础步数$T(n)$图1. 三种渐近记号如图所示，$O(n)$表示最坏情况（上界）、$\\Theta(n)$表示平均情况（往往和最坏情况大致一样）、$\\Omega(n)$表示最好情况（下界）渐近分析计算方法顺序执行情况以插入排序为例def INSERTION_SORT(A)    for j in range(len(A)):   ...",
    "content": "为了刻画算法效率，我们分析当输入规模$n$足够大时，算法执行的基础步数$T(n)$图1. 三种渐近记号如图所示，$O(n)$表示最坏情况（上界）、$\\Theta(n)$表示平均情况（往往和最坏情况大致一样）、$\\Omega(n)$表示最好情况（下界）渐近分析计算方法顺序执行情况以插入排序为例def INSERTION_SORT(A)    for j in range(len(A)):         # c1* n      key = A[j]                    # c2* (n-1)      i = j-1                       # c3* (n-1)      while i &gt;= 0 and A[i]&gt;key:    # c4* (1+2+...+n-1)        A[i+1] = A[i]               # c5* (1+2+...+n-2)        i = i-1                     # c6* (1+2+...+n-3)      A[i+1] = key                  # c7* (n-1)上述$c_i$为基础操作（赋值、比较）的耗时，与平台相关且是常数时间将其加起来有：\\[\\begin{aligned}T(n)&amp;=an^2+bn+c\\\\&amp;=O(n^2)\\end{aligned} \\tag{1}\\]可知插入排序最坏情况是$n^2$的量级考虑到$n$足够大的前提，在渐近分析里忽略常数和低阶，因此实际中并不需要逐行计算，重点关注循环就可以了递归执行情况以归并排序为例def MERGE_SORT(A, p, r):    if p &lt; r:      q = int((p+r)/2)      MERGE_SORT(A, p, q)      MERGE_SORT(A, q+1, r)      MERGE(A, p, q, r)假设分解为子问题的时间加上合并子问题的时间为$\\Theta(n)$，那么有\\[T(n)=\\begin{cases}\\Theta(1)&amp;n=1\\\\2T(n/2)+\\Theta(n)&amp;n&gt;1\\end{cases} \\tag{2}\\]可以证明（图2），对于(2)，有\\[T(n)=\\Theta(nlog_2^n) \\tag{3}\\]图2. 递归树可以看到，归并排序比插入排序性能更好常见的渐近形式及结论            渐近形式      记号      说明                  常数      $O(1)$      不包含n              对数      $O(log^n)$      接近常数              多项式      $O(n^c)$      可接受              指数      $O(c^n)$      不可接受      几种级数的结论            级数名称      公式      结论                  算术级数      $T_1(n)=1+2+…+n=\\frac{n(n+1)}{2}$      $O(n^2)$              幂方级数      $T_2(n)=1^2+2^2+…+n^2=\\frac{n(n+1)(2n+1)}{6}$      $O(n^3)$              几何级数      $T_3(n)=a^0+a^1+…+a^n=\\frac{a^{n+1}-1}{a-1}$      $O(a^n)              收敛级数      $T_4(n)=1+\\frac{1}{2^2}+…+\\frac{1}{n^2}$      $O(1)$              调和级数      $T_5(n)=1+\\frac{1}{2}+…+\\frac{1}{n}$      $O(log^n)$              对数级数      $T_6(n)=log^1+log^2…log^n$      $O(nlog^n)$      "
  },
  
  {
    "title": "【CNN系列目标检测】（9）YOLO4算法",
    "url": "/posts/CNN%E7%B3%BB%E5%88%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-9-YOLO4%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, CNN系列目标检测",
    "tags": "CNN系列目标检测",
    "date": "2021-11-17 00:00:00 +0000",
    





    
    "snippet": "YOLOv4【1】对目标检测各个阶段的最新方法做了总结和比较，可以当成一篇综述来读与YOLOv3相比，YOLOv4  使用了多种的训练阶段技巧（Bag of Freebies）提升精度  使用改进的CSPDarnet53作为backbone  在backbone和head间引入neck模块  输出head模块修改了损失函数YOLOv4使用V100在COCO上达到了65FPS的速度，精度比YO...",
    "content": "YOLOv4【1】对目标检测各个阶段的最新方法做了总结和比较，可以当成一篇综述来读与YOLOv3相比，YOLOv4  使用了多种的训练阶段技巧（Bag of Freebies）提升精度  使用改进的CSPDarnet53作为backbone  在backbone和head间引入neck模块  输出head模块修改了损失函数YOLOv4使用V100在COCO上达到了65FPS的速度，精度比YOLOv3也有了大幅提升图1. YOLOv4性能对比网络结构作者认为当前的one-stage和two-stage目标检测方法都可以用下图的结构来表示图2. 目标检测算法框架      模块    功能    典型方法    说明        input    数据输入    图片、图片金字塔    训练阶段各种数据增强方法        backbone    特征提取    VGG16、ResNet50、ResNeXt101、Darknet53、CSPResNeXt50、CSPDarknet53    接在input后        neck    对特征做融合    SPP、ASPP、RFB、SAM、FPN、PAN    接在backbone后，由几条自底向上和自顶向下的路径组成        head    目标预测    RPN、YOLO、SSD、RetinaNet、CornerNet、CenterNet    one-stage（Dense Prediction）        Faster-RCNN、R-FCN、Mask-RCNN、RepPoints    two-stage（Sparse Prediction）  图3是YOLOv4的网络结构图，参考《深入浅出Yolo系列之Yolov3&amp;Yolov4&amp;Yolov5&amp;Yolox核心基础知识完整讲解》图3. YOLOv4网络结构backboneYOLOv4对darknet53做了改进，提出CSPDarknet53：  借鉴了CSPNet【2】（跨阶段局部网络），降低计算量提高精度  使用Mish【3】激活函数图4. CSPDarknet53网络结构def CSPDarknet53(input_data):    input_data = common.convolutional(input_data, (3, 3,  3, 32), activate_type=\"mish\")    input_data = common.convolutional(input_data, (3, 3, 32, 64), downsample=True, activate=\"mish\")    input_data = common.cross_stage_partial(input_data, 64, 64, 32, 64, 1, \"mish\")    input_data = common.convolutional(input_data, (3, 3, 64, 64, 128), downsample=True, activate_type=\"mish\")    input_data = common.cross_stage_partial(input_data, 128, 64, 64, 64, 2, \"mish\")    input_data = common.convolutional(input_data, (3, 3, 128, 256), downsample=True, activate_type=\"mish\")    input_data = common.cross_stage_partial(input_data, 256, 128, 128, 128, 8, \"mish\")        route_1    = input_data    input_data = common.convolutional(input_data, (3, 3, 256, 512), downsample=True, activate_type=\"mish\")    input_data = common.cross_stage_partial(input_data, 512, 256, 256, 256, 8, \"mish\")    route_2    = input_data    input_data = common.convolutional(input_data, (3, 3, 512, 1024), downsample=True, activate_type=\"mish\")    input_data = common.cross_stage_partial(input_data, 1024, 512, 512, 512, 4, \"mish\")    # neck    input_data = common.convolutional(input_data, (1, 1, 1024, 512))    input_data = common.convolutional(input_data, (3, 3, 512, 1024))    input_data = common.convolutional(input_data, (1, 1, 1024, 512))    input_data = common.spp_block(input_data, 512)    input_data = common.convolutional(input_data, (1, 1, 2048, 512))    input_data = common.convolutional(input_data, (3, 3, 512, 1024))    input_data = common.convolutional(input_data, (1, 1, 1024, 512))    return route_1, route_2, input_dataCSPNet      CSPNet旨在通过对现有网络做少量改造加强网络学习能力、去除计算瓶颈、降低内存消耗        CSPNet的实验表明用了该结构后可以降低20%的计算量，同时保持甚至超过原始网络的精度  主流的CNN结构为逐层传递，CSPNet认为这样虽然紧凑且高效，但会导致第k层的梯度经过k-1、k-2…，从而重复学习冗余信息\\[\\begin{aligned}y&amp;=F(x_0)\\\\&amp;=H_k(x_{k-1},H_{k-1}(x_{k-2}),H_{k-2}(x_{k-3}),...,H_1(x_0),x_0)\\end{aligned} \\tag{1}\\]式1中$F$为一个CNN网络，$H_k$为第$k$层的操作（通常包含卷积和激活函数），是一个逐层嵌套的结构\\[\\begin{aligned}w_1'&amp;=f_1(w_1,\\{g_0\\})\\\\w_2'&amp;=f_2(w_2,\\{g_0,g_1\\})\\\\w_k'&amp;=f_k(w_k,\\{g_0,g_1,...,g_{k-1}\\})\\end{aligned} \\tag{2}\\]式2中$f_k$为第$k$层权重更新函数，$g_k$为传播到$k$层的梯度，可以看到大量的$g_{k-1}$被重复学习基于此CSPNet提出图5所示的改造方案图5. CSPNet改造方案上图(a)是原始结构，(b)是改造后的结构，可以看到CSPNet的改造方式是：  将上层的输出按channels分割成2部分  一部分依然通过block，然后做一次transition，另一部分直接与第一部分的结果做concat，然后做transition乍一看该结构与ResNet很像，但其实有2点不一样：1）ResNet是将上层的输出copy成2部分2）ResNet最后不是做concat，而是做加法关于transition layertransition layer使用二次卷积来代替一次卷积，降低计算量如对于[56, 56, 256] -&gt; [28, 28, 512]一次卷积为[3, 3, 256, 512]，参数量为1179648二次卷积为[1, 1, 256, 128]、[3, 3, 128, 512]，参数量为622592CSPDarknet53的借鉴在CSPDarknet53中，作者对CSPNet做了改动（参考《YOLOv4特征提取网络——CSPDarkNet结构解析及PyTorch实现》）：图6. 左：CSPNet的改造方式；右：CSPDarknet53的方式可以看到1）后者并没有对上层输出做split，而是直接做copy2）做Resdual之前，分支先做transition，然后做channel减半的操作；另一个分支也做channel减半的操作上述第1点是为了提高特征的重用性，第2点是为了达到split的效果，减少计算量def cross_stage_partial(input_data, input_channel, \\                        filter_num1, filter_num2, filter_num3, \\                        res_num, activate_type_):    route    = input_data    route    = convolutional(route,      filters_shape=(1, 1, input_channel, filter_num1), activate_type=activate_type_)    csp_data = convolutional(input_data, filters_shape=(1, 1, input_channel, filter_num1), activate_type=activate_type_)    for i in range(res_num):        csp_data = residual_block(csp_data, filter_num1, filter_num2, filter_num3, activate_type=activate_type_)    csp_data = convolutional(csp_data, filters_shape=(1, 1, filter_num3, filter_num3), activate_type=activate_type_)        csp_data = tf.concat([csp_data, route], axis=-1)    csp_data = convolutional(csp_data, filters_shape=(1, 1, filter_num1+filter_num3, input_channel),                                activate_type=activate_type_)    return csp_dataMish激活函数激活函数是为了提高网络的学习能力，提升梯度的传递效率目前的普遍看法是，平滑的激活函数允许更好的信息深入神经网络，从而得到更好的准确性和泛化早期网络常用的有ReLU，LeakyReLU，softplus等，后来又有了Swish，Mish等\\[\\begin{aligned}softplus(x)&amp;=log(1+e^x)\\\\mish(x)&amp;=x\\times\\tanh(softplus(x))\\end{aligned} \\tag{3}\\]其中\\[\\tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}} \\tag{4}\\]可以看到Mish比ReLU的计算复杂度要高不少图7. 左：ReLU与softplus对比；右：Mish  Mish和ReLU一样，都是无正向边界的，可以避免梯度饱和  Mish函数是处处光滑的，并且在绝对值较小的负值区域允许一些负值neck  neck网络通常位于backbone和head的中间位置，利用它对特征做聚合，可以进一步提升特征的多样性及鲁棒性  YOLOv4在neck中应用的修改的SPP模块【4】和PANet模块【5】  SPP模块融合不同尺度大小的特征图增大感受野  PANet模块有助于保留空间信息SPPSPP提出来主要是用来解决不同尺寸的特征图如何进入全连接层的图8. SPP如图，SPP将输出分为不同大小的网格，每个网格做最大池化，最后将池化结果拼接，实现输出尺寸的统一在CSPDarknet53中（需要注意的是，在实现上SPP模块放在了CSPDarknet53中），作者同时也将上层的输出加进去做池化，输出尺寸无法保持统一了，可以看到其主要目的是增加感受野def spp_block(input_data, input_channel):    spp_data = tf.concat([tf.nn.max_pool(input_data, ksize=13, padding='SAME', strides=1),                           tf.nn.max_pool(input_data, ksize=9,  padding='SAME', strides=1),                          tf.nn.max_pool(input_data, ksize=5,  padding='SAME', strides=1),                          input_data], axis=-1)    return spp_dataPANetPANet（Path Aggregation Network for Instance Segmentation）是在FPNet（Feature Pyramid Network）【6】的基础上做的修改：  FPNet针对目标检测中多尺度问题，提出用特征金字塔，融合高层特征（大尺度）与低层特征（小尺度）  PANet面向的是语义分割领域，其中一部分结构是FPNet的修改图9. 左：FPN；右：PANet一部分可以看到，PANet在FPN的基础上增加了Bottom-up Path Augemtation（图右(b)），其主要目的是为了增强浅层网络的特征（浅层网络中的边缘形状特征对分割很重要）在YOLOv4中，PANet的BPA部分的融合由相加换成了concat，可见其主要要是为了改善目标多尺度问题图10. 左：原始BPA的聚合；右：YOLOv4中BPA的聚合head与YOLOv3一致文献【1】Bochkovskiy A ,  Wang C Y ,  Liao H . YOLOv4: Optimal Speed and Accuracy of Object Detection[J].  2020.【2】Wang C Y ,  Liao H ,  Wu Y H , et al. CSPNet: A New Backbone that can Enhance Learning Capability of CNN[C]// 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). IEEE, 2020.【3】Misra D . Mish: A Self Regularized Non-Monotonic Neural Activation Function[J].  2019.【4】He K ,  Zhang X ,  Ren S , et al. Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition[C]// IEEE Transactions on Pattern Analysis &amp; Machine Intelligence. 2014:1904-16.【5】Liu S ,  Qi L ,  Qin H , et al. Path Aggregation Network for Instance Segmentation[C]// 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2018.【6】Lin T Y ,  Dollar P ,  Girshick R , et al. Feature Pyramid Networks for Object Detection[C]// 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2017."
  },
  
  {
    "title": "ubuntu系统维护",
    "url": "/posts/ubuntu%E7%B3%BB%E7%BB%9F%E7%BB%B4%E6%8A%A4/",
    "categories": "linux",
    "tags": "",
    "date": "2021-10-27 00:00:00 +0000",
    





    
    "snippet": "系统备份与恢复备份备份的时候可以直接在当前系统内：sudo tar zcvpf /media/disk/ubuntu_`date +%Y-%m-%d`.tar.gz --exclude=/proc --exclude=/tmp --exclude=/lost+found --exclude=/media --exclude=/mnt --exclude=run /该指令将根目录下的内容打包压...",
    "content": "系统备份与恢复备份备份的时候可以直接在当前系统内：sudo tar zcvpf /media/disk/ubuntu_`date +%Y-%m-%d`.tar.gz --exclude=/proc --exclude=/tmp --exclude=/lost+found --exclude=/media --exclude=/mnt --exclude=run /该指令将根目录下的内容打包压缩到tar.gz，exclude的目录被排除1）tar 参数介绍-c: 创建一个文件-v: 显示详细信息-p: 保存权限并应用到所有文件-z: 用gzip压缩文件-f: 指定文件的名称，只能做最后一个参数2）exclude目录介绍/proc:       一个虚拟文件系统，由系统自动创建，系统运行的每一个进程都会自动在这个目录下面创建一个进程目录/tmp:        一个临时文件夹，系统的一些临时文件会放在这里/lost+found: 系统发生错误时（比如非法关机），可以在这里找回一些丢失文件/media:      多媒体挂载点，像u盘、移动硬盘、windons分区等都会自动挂载到这个目录下/mnt:        临时挂载点，你可以自己挂载一些文件系统到这里/run:        系统从启动以来产生的一些信息文件恢复  使用U盘启动进LiveCD系统  挂载保存有备份文件的硬盘  挂载待恢复系统所在的磁盘分区到/mntsudo tar zxvpf /media/disk/ubuntu_2021-10-27.tag.gz -C /mnt  在/mnt下创建被exclude的其他目录  拔掉U盘重启机器，可以看到系统恢复了显卡驱动失效的解决对于独立显卡机器，可能会经常发现系统开机无法识别显卡这其实是因为系统内核自动升级了，新的内核里显卡驱动失效了如果要在新内核里继续使用，只能重新安装显卡驱动也可以通过禁用内核自动更新保证系统的“稳定性”禁用内核自动更新  首先查看下当前系统使用的内核版本uname -r  然后查看当前系统所有内核的情况dpkg --get-selections | grep linux结果示例binutils-x86-64-linux-gnu\t\t\tinstallconsole-setup-linux\t\t\t\tinstalllibselinux1:amd64\t\t\t\tinstalllibselinux1:i386\t\t\t\tinstalllinux-base\t\t\t\t\tinstalllinux-firmware\t\t\t\t\tinstalllinux-generic-hwe-18.04\t\t\t\tinstalllinux-headers-5.4.0-81-generic\t\t\tinstalllinux-headers-5.4.0-87-generic\t\t\tinstalllinux-headers-generic-hwe-18.04\t\t\tinstalllinux-hwe-5.4-headers-5.4.0-42\t\t\tinstalllinux-hwe-5.4-headers-5.4.0-81\t\t\tinstalllinux-hwe-5.4-headers-5.4.0-87\t\t\tinstalllinux-image-5.4.0-42-generic\t\t\tdeinstalllinux-image-5.4.0-81-generic\t\t\tinstalllinux-image-5.4.0-87-generic\t\t\tinstalllinux-image-generic-hwe-18.04\t\t\tinstalllinux-libc-dev:amd64\t\t\t\tinstalllinux-modules-5.4.0-42-generic\t\t\tdeinstalllinux-modules-5.4.0-81-generic\t\t\tinstalllinux-modules-5.4.0-87-generic\t\t\tinstalllinux-modules-extra-5.4.0-42-generic\t\tdeinstalllinux-modules-extra-5.4.0-81-generic\t\tinstalllinux-modules-extra-5.4.0-87-generic\t\tinstalllinux-signed-generic-hwe-18.04\t\t\tinstalllinux-sound-base\t\t\t\tinstallpptp-linux\t\t\t\t\tinstallsyslinux\t\t\t\t\tinstallsyslinux-common\t\t\t\t\tinstallsyslinux-legacy\t\t\t\t\tinstallutil-linux\t\t\t\t\tinstall  禁用特定版本的内核升级sudo apt-mark hold 5.4.0-87-generic经过测试禁用特定版本内核仍会下载新的内核  禁用内核升级sudo apt-mark hold linux-image-generic linux-headers-generic有效性还待测试  关闭软件和更新选项图1. 关闭软件更新上述软件更新选项对应的配置文件是/etc/apt/apt.conf/d/10periodic# /etc/apt/apt.conf/d/10periodicAPT::Periodic::Update-Package-Lists \"0\";APT::Periodic::Download-Upgradeable-Packages \"0\";APT::Periodic::AutocleanInterval \"0\";APT::Periodic::Unattended-Upgrade \"0\";将其全部设为0也是一个效果内核删除如果内核已经更新了，导致显卡无法识别，可以通过删除新的内核使用老内核恢复系统  首先查看当前系统所有内核的情况dpkg --get-selections | grep linux  删除特定版本内核sudo apt-get purge linux-headers-x.x.x-x-genericsudo apt-get purge linux-image-x.x.x-x-generic...dpkg 命令下列出来的所有特定版本的项目都删除  最后更新grubsudo update-grub"
  },
  
  {
    "title": "TensorRT数据类型",
    "url": "/posts/TensorRT%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/",
    "categories": "深度学习",
    "tags": "",
    "date": "2021-10-26 00:00:00 +0000",
    





    
    "snippet": "TensorRT支持模型参数的数据类型包括FP32、FP16、TF32、INT8，数据精度越低模型推理速度越快在具体分析各种数据类型之前，首先回顾下浮点数的二进制表示浮点数的二进制表示在计算机中一般以IEEE754标准存储浮点数，其在内存中的存储形式为            符号位      指数位      有效数字                  sign      exponent ...",
    "content": "TensorRT支持模型参数的数据类型包括FP32、FP16、TF32、INT8，数据精度越低模型推理速度越快在具体分析各种数据类型之前，首先回顾下浮点数的二进制表示浮点数的二进制表示在计算机中一般以IEEE754标准存储浮点数，其在内存中的存储形式为            符号位      指数位      有效数字                  sign      exponent      fraction      其中  sign占1bit，表示数字的正负  exponent和fraction按数据类型不同有不同占位  exponent决定数据类型的表达范围  fraction决定数据类型的精度范围以FT32数据类型为例图1. FT32二进制表示浮点数的二进制到十进制的计算方式是\\[D=F_1(sign)\\times F_2(exponent)\\times F_3(fraction) \\tag{1}\\]其中$F_1$确定数据正负\\[F_1(sign) = (-1)^{sign} \\tag{2}\\]$F_2$和$F_3$为指数部分和有效数部分，总体分三种情况1）$exponent$不全为0，或不全为1\\[\\begin{aligned}F_2(exponent)&amp;=2^{exponent-m}\\\\F_3(fraction)&amp;=1+\\frac{fraction}{M}\\end{aligned} \\tag{3}\\]2）$exponent$全为0\\[\\begin{aligned}F_2(exponent)&amp;=2^{1-m}\\\\F_3(fraction)&amp;=\\frac{fraction}{M}\\end{aligned} \\tag{4}\\]3）$exponent$全为1\\[D=\\begin{cases}\\pm inf(无穷大) &amp; fraction全为0 \\\\NAN(非数字) &amp; fraction不全为0\\end{cases} \\tag{5}\\]式(3)(4)中\\[m=(2^8-1)/2=127\\]其中$8$是FT32的$exponent$位数\\[M=2^{23}=8388608\\]其中$23$是FT32的$fraction$位数根据上述的计算方法，可以得到FT32的最大值为2**(int('11111110',2)-127)*(1+(int('11111111111111111111111',2)/8388608.0)) \\= 3.4028+38各种数据类型的对比图2. 各数据类型二进制表示对比            标志      数据类型      特性                  FT32      单精度浮点型      深度学习中最常见的数据格式，训练推理都会用到              FP16      半精度浮点型      相比FP32占用内存减少一半，有相应的指令值，速度比FP32要快很多              TF32      截短的FT32数据格式      第三代Tensor Core支持的一种数据类型，保持了与FP16同样的精度(尾数位都是 10 位），同时还保持了FP32的动态范围(指数位都是8位)              INT8      整型      相比FP16占用内存减小一半，有相应的指令集，模型量化后可以利用INT8进行加速      FT32与FT16混合精度加速  FT16与FT32相比，内存占用减半，计算更快  FT16与FT32相比，有数据溢出和舍入误差的问题数据溢出FP16的有效动态范围比FP32的要窄很多对深度学习而言，训练后期激活函数的梯度会非常小，乘以学习率后会更小，FP16相比FP32就更容易出现下溢出，导致梯度为0无法更新图3. 激活函数梯度分布上图是SSD训练过程中激活函数梯度分布情况，有67%的梯度小于$2^{-24}$，如果用FT16表示，这些梯度都会变成0舍入误差FP16与FP32相比，能表达的数据粒度大，也就是说上一个数到下一个数的step更大，精细度差图4. 舍入误差从上图可看到，由于FP16的step为$2^{-13}$，因此在做加法的时候$2^{-14}$被截断，在深度学习中，意味着这一次的权重更新无效Mixed precision training百度和NVIDIA在ICLR2018的论文《Mixed precision training》【1】提出了一种解决方法FP32权重备份该方法旨在解决舍入误差的问题可以概括为：weights、activations、gradients在训练中用FP16存储，同时拷贝一份FP32的weights用于更新注意到权重更新的公式\\[weights=weights+lr\\times gradients \\tag{6}\\]其中$lr\\times gradients$往往是非常小的，与$weights$相加很容易出现舍入误差的问题而在权重更新时用FP32的$weights$就可以避免这个问题当然FP32的引入会增加训练时的内存占用，但一般训练中占用内存大部分的基本都是activations，因此该方案与全部用FP32相比内存占用也基本能减半Loss Scale该方法旨在解决FP16下溢出的问题论文对计算出来的loss进行scale（一般取8-32k）由于链式法则的存在，loss上的scale会作用在所有梯度上，这样scaled-gradient就可以一直用FP16存储，只有在更新的时候才将其转化为FP32，同时将scale抹去计算精度对于某些模型，在FP16矩阵乘法中，需要使用FP32做矩阵乘法中间的加法，然后再转换为FP16存储，以减少舍入误差，确保推理过程中的精度但目前只有拥有TensorCore的显卡才支持这种混合精度的加法图5. 支持混合精度加法的显卡参考《【PyTorch】唯快不破：基于Apex的混合精度加速》和《浅谈混合精度训练》文献【1】Micikevicius P, Narang S, Alben J, et al. Mixed precision training[J]. arXiv preprint arXiv:1710.03740, 2017."
  },
  
  {
    "title": "线性代数（6）",
    "url": "/posts/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-6/",
    "categories": "数据结构",
    "tags": "",
    "date": "2021-10-21 00:00:00 +0000",
    





    
    "snippet": "对称矩阵  对称矩阵满足$A=\\bar{A}^T=A^H$，其中$\\bar{A}$表示取其共轭，$H$表示取共轭转置  对实对称矩阵，有$A=A^T$对称矩阵拥有多个很好的性质：1）可对角化\\[A=S\\Lambda S^{-1} \\tag{1}\\]对称矩阵可能有重复的特征值，但一定有足够多的特征向量来进行对角化2）可选出一组标准正交的特征向量\\[\\begin{aligned}A&amp;=S...",
    "content": "对称矩阵  对称矩阵满足$A=\\bar{A}^T=A^H$，其中$\\bar{A}$表示取其共轭，$H$表示取共轭转置  对实对称矩阵，有$A=A^T$对称矩阵拥有多个很好的性质：1）可对角化\\[A=S\\Lambda S^{-1} \\tag{1}\\]对称矩阵可能有重复的特征值，但一定有足够多的特征向量来进行对角化2）可选出一组标准正交的特征向量\\[\\begin{aligned}A&amp;=S\\Lambda S^{-1}\\\\A^T&amp;={(S^{-1})}^{T}\\Lambda S^T\\end{aligned} \\tag{2}\\]因为$A=A^T$，因此有$S^{-1}=S^T$，即$S$为正交矩阵：\\[S^TS=I \\tag{3}\\]一般用$Q$表示标准正交矩阵，$A$可表示为\\[\\begin{aligned}A&amp;=Q\\Lambda Q^{-1}\\\\&amp;=Q\\Lambda Q^T\\end{aligned} \\tag{4}\\]3）主元符号与特征值符号一致（正主元数量=正特征值数量）4）特征值为实数以$A$为实数矩阵为例，有\\[\\begin{aligned}Ax&amp;=\\lambda x\\\\\\Rightarrow A\\bar{x}&amp;=\\bar{\\lambda}\\bar{x}\\\\\\Rightarrow \\bar{x}^TA^T&amp;=\\bar{x}^T\\bar{\\lambda}\\\\\\Rightarrow \\bar{x}^TA&amp;=\\bar{x}^T\\bar{\\lambda}\\\\\\Rightarrow \\bar{x}^TAx&amp;=\\bar{x}^T\\bar{\\lambda}x\\end{aligned} \\tag{5}\\]又因为\\[\\begin{aligned}Ax&amp;=\\lambda x\\\\\\Rightarrow \\bar{x}^TAx&amp;=\\bar{x}^T\\lambda x\\end{aligned} \\tag{6}\\]比较(5)和(6)，因为$\\bar{x}^Tx&gt;0$，有\\[\\lambda=\\bar{\\lambda} \\tag{7}\\]即$\\lambda$为实数正定矩阵  给定一个大小为$n\\times n$的实对称矩阵$A$，如果对于任意长度为$n$的非零向量$x$，恒有\\[x^TAx&gt;0\\tag{8}\\]  则称$A$为正定矩阵  如果恒有\\[x^TAx\\geq0\\tag{9}\\]  则称$A$为半正定矩阵  (半)正定矩阵是一种特殊的对称矩阵1）正定矩阵所有特征值$\\lambda_i&gt;0$又因为对称矩阵的特征值和主元符号一致，因此正定矩阵所有主元大于02）正定矩阵所有子行列式都大于03）协方差矩阵为半正定矩阵令协方差矩阵\\[C=E[(t-\\bar{t})(t-\\bar{t})^T] \\tag{10}\\]对任意一个向量$x$，有\\[\\begin{aligned}x^TCx&amp;=x^TE[(t-\\bar{t})(t-\\bar{t})^T]x\\\\&amp;=E[x^T(t-\\bar{t})(t-\\bar{t})^Tx]\\\\&amp;=E(s^2)\\\\&amp;=\\sigma_s^2\\end{aligned} \\tag{11}\\]其中\\[\\sigma_s=x^T(t-\\bar{t})=(t-\\bar{t})^Tx \\tag{12}\\]由于$\\sigma_s^2\\geq 0$，因此$x^TCx\\geq 0$，协方差矩阵$C$为半正定矩阵4）对任意$A_{m\\times n}$，$A^TA$半正定\\[\\begin{aligned}x^T(A^TA)x&amp;=(Ax)^T(Ax)\\\\&amp;=||Ax||^2\\\\&amp;\\geq 0 \\end{aligned}\\tag{13}\\]如果$A$的$n$列线性无关，即$Rank(A)=n$，则$A^TA$正定相似矩阵  设$A$、$B$都是$n$阶矩阵，若有可逆矩阵$P$，使\\[P^{-1}AP=B \\tag{14}\\]  则称$A$与$B$相似1）如果$A$与$B$相似，则两者特征值相同，拥有相同数量的特征向量\\[\\begin{aligned}&amp;Ax=\\lambda x\\\\\\Rightarrow &amp;APP^{-1}x=\\lambda x\\\\\\Rightarrow &amp;P^{-1}APP^{-1}x=\\lambda P^{-1}x\\\\\\Rightarrow &amp;BP^{-1}x=\\lambda P^{-1}x\\end{aligned} \\tag{15}\\]可以看到，$B$的特征向量为$P^{-1}x$2）$A$与$B$相似的意义是：它们是同一个线性变换在不同坐标系下的表示（此外矩阵的本质就是线性变换）借用知乎《理解相似矩阵》一文的图片图1. 相似矩阵对应的线性变换空间中点$m$，在坐标系$i_1j_1$下的表示为$\\vec{v_1}$，在坐标系$i_2j_2$下的表示为$\\vec{v_2}$现假设空间中另一点$n$，在坐标系$i_1j_1$下与$m$的关系为$A\\vec{v_1}$，在坐标系$i_2j_2$下与$m$的关系为$B\\vec{v_1}$又已知坐标系$i_1j_1$和$i_2j_2$的变换关系为$P$，即$\\vec{v_1}=P\\vec{v_2}$于是有\\[\\begin{aligned}&amp;A\\vec{v_1}=PB\\vec{v_2}&amp;=PBP^{-1}\\vec{v_1}\\\\\\Rightarrow &amp;P^{-1}AP=B\\end{aligned} \\tag{16}\\]可以看到，$A$和$B$本质是同一个线性变换（都是描述的点$m$与点$n$的线性变换），通过不同坐标系的转换关系$P$建立联系，因此说$A$与$B$相似奇异值分解SVD矩阵基于特征值的对角化\\[A=S\\Lambda S^{-1} \\tag{17}\\]需要保证1）A为$n$阶方阵2）A有$n$个线性无关的特征向量而对于任意矩阵$A_{m\\times n}$，均可通过奇异值分解将其对角化\\[A=U\\Sigma V^T \\tag{18}\\]      上式中$\\Sigma$是大小为$n\\times n$对角矩阵，对角元素称为奇异值        $U$的大小为$m\\times m$，其列向量正交，称为左奇异向量        $V$的大小为$n\\times n$，其列向量正交，称为右奇异向量  \\[\\begin{aligned}U^TU&amp;=I\\\\V^TV&amp;=I\\end{aligned} \\tag{19}\\]奇异值及奇异向量求解根据式(18)，有\\[\\begin{aligned}A^TA&amp;=V\\Sigma ^TU^TU\\Sigma V^T\\\\&amp;=V\\Sigma^T\\Sigma V^T\\\\&amp;=V\\left[\\begin{matrix}\\sigma_1^2&amp;0&amp;0\\\\0&amp;\\ddots&amp;0\\\\0&amp;0&amp;\\sigma_n^2\\end{matrix}\\right]V^T\\end{aligned} \\tag{20}\\]可知$V$是$A^TA$的特征向量矩阵，$\\sigma^2$是$A^TA$的特征值类似的，根据$AA^T$的推导可求得$U$SVD的应用1）矩阵的近似可以通过舍弃小的奇异值得到矩阵$A$的近似\\[\\begin{aligned}A_{m\\times n}&amp;=U_{m\\times n}\\Sigma_{n\\times n}V_{n\\times n}^T\\\\&amp;\\approx U_{m\\times r}\\Sigma_{r\\times r}V_{n\\times r}^T\\end{aligned} \\tag{21}\\]其中  $\\Sigma_{r\\times r}$是将奇异值从大到小排列，取前$r$个组成的奇异值矩阵  $U_{m\\times r}$是$U$中前$r$个奇异值对应的$r$列左奇异向量  $V_{n\\times r}$是$V$中前$r$个奇异值对应的$r$列右奇异向量2）PCA通过舍弃小的奇异值实现PCA降维\\[\\begin{aligned}A_{r\\times n} &amp;= U_{n\\times r}^TA_{m\\times n}\\\\A_{m\\times r}&amp;=A_{m\\times n}V_{n\\times r}\\end{aligned} \\tag{22}\\]上式分别实现了原始矩阵$A$行向量和列向量的降维左逆、右逆、伪逆非方阵$A_{m\\times n}(m\\neq n)$肯定不可逆，那么是否存在$A_L^{-1}$和$A_R^{-1}$使得\\[\\begin{aligned}A_L^{-1}A&amp;=I\\\\AA_R^{-1}&amp;=I\\end{aligned}\\tag{23}\\]  当$A$各列向量线性无关，即$rank(A)=n$，则存在$A_L^{-1}A=I$，其中$A_L^{-1}$为$A$的左逆  当$A$各行向量线性无关，即$rank(A)=m$，则存在$AA_R^{-1}=I$，其中$A_R^{-1}$为$A$的右逆证明：当$A$各列向量线性无关，所以$A^TA$可逆（矩阵左乘相当于初等行变换，不改变列秩）因此必然有\\[(A^TA)^{-1}A^TA=I\\]令\\[A_L^{-1}=(A^TA)^{-1}A \\tag{24}\\]当$A$各行向量线性无关，$AA^T$可逆，类似可得\\[A_R^{-1}=A^T(AA^T)^{-1} \\tag{25}\\]  对称矩阵不一定可逆，比如全0矩阵  对非方阵，左逆和右逆不可能同时存在伪逆数理统计中一般利用大量数据拟合模型，这些数据组成的矩阵$A$无法保证可逆，因此只能求其伪逆$A^+$左逆$A_L^{-1}$和右逆$A_R^{-1}$分别需要在列满秩或行满秩的情况下才存在，但伪逆$A^+$对任意$A$都存在  当$A_{m\\times n}$列满秩时$A^+=A_L^{-1}$  当$A_{m\\times n}$行满秩时$A^+=A_R^{-1}$  当$A_{n\\times n}$可逆时$A^+=A^{-1}$伪逆$A^+$的定义对矩阵$A_{m\\times n}$，如果矩阵$A_{n\\times m}^+$满足\\[\\begin{aligned}AA^+A&amp;=A\\\\A^+AA^+&amp;=A^+\\\\AA^+&amp;=(AA^+)^T\\\\A^+A&amp;=(A^+A)^T\\end{aligned} \\tag{26}\\]则$A^+$是$A$的伪逆《Moore-Penrose伪逆和最小二乘问题》里有伪逆存在性和唯一性的证明伪逆$A^+$的求解如果$A$可逆，则伪逆就是其矩阵逆如果$A$满足列满秩或行满秩，可以分别按左逆、右逆求得其他情况可以按如下通用方法求解：1）对$A$，求其SVD分解，$A=U\\Sigma V^T$2）对$\\Sigma$对角线非0元素取其倒数，0元素保持不变\\[(\\Sigma^+)^T=\\left[\\begin{matrix}\\frac{1}{\\sigma_1}&amp;0&amp;0\\\\0&amp;\\ddots&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right] \\tag{27}\\]3）整体转置\\[\\begin{aligned}A^+&amp;=(U\\Sigma^+V^T)^T\\\\&amp;=V\\Sigma^+U^T\\end{aligned} \\tag{28}\\]伪逆$A^+$在最小二乘里的应用对问题\\[min\\ f(x)=\\frac{1}{2}||Ax-B||_2^2\\]由(26)可知，当$A$不可逆时$x=A^+B$是其近似最优解"
  },
  
  {
    "title": "ELAS算法解析",
    "url": "/posts/ELAS%E7%AE%97%E6%B3%95%E8%A7%A3%E6%9E%90/",
    "categories": "自动驾驶",
    "tags": "",
    "date": "2021-09-26 00:00:00 +0000",
    





    
    "snippet": "ELAS【1】是2010年提出的一种立体匹配算法，与SGM精度差不多但效率高，在KITTI上精度排名288，运行时间0.3sELAS最大的特点是没有遵循典型的代价计算、代价聚合、视差优化、视差细化的过程，而是首先建立稀疏视差（支撑点），然后建立概率生成模型来做稠密立体匹配，最后用常规方法做视差优化部分内容参考《双目立体匹配算法：ELAS》计算稀疏视差在ELAS里计算的稀疏视差被称作支撑点，其...",
    "content": "ELAS【1】是2010年提出的一种立体匹配算法，与SGM精度差不多但效率高，在KITTI上精度排名288，运行时间0.3sELAS最大的特点是没有遵循典型的代价计算、代价聚合、视差优化、视差细化的过程，而是首先建立稀疏视差（支撑点），然后建立概率生成模型来做稠密立体匹配，最后用常规方法做视差优化部分内容参考《双目立体匹配算法：ELAS》计算稀疏视差在ELAS里计算的稀疏视差被称作支撑点，其结构为$(u,v,d)$计算的方式比较简单：  对左右图的每个像素计算sobel特征向量（尺寸为16*8bit）  计算左右图sobel特征向量的sad损失，取最小的为其视差d，得到支撑点$(u,v,d)$  过滤不可信支撑点：邻域其他视差接近的支撑点数量小于阈值则不可信  过滤多余支撑点：横向纵向搜索，删除视差值接近的点（认为在一条直线上，可以减少后续计算量）官方代码里对sad计算使用了SSE2加速，值得学习基于概率生成模型的稠密匹配ELAS将视差和左右图的像素看作随机变量，将其联合分布分解为\\[p(d_n,o_n^{(l)},o_n^{(r)},S)\\propto p(d_n|S,o_n^{(l)})p(o_n^{(r)}|o_n^{(l)},d_n)\\tag{1}\\]其中  $d_n$为第$n$个像素的视差  $o_n=(u_n,v_n,f_n)$为第$n$个像素的观测值，(u_n,v_n)为坐标，$f_n$为sobel特征向量，$o_n^{(l)}$、$o_n^{(r)}$分别为左图、右图的观测值  $S$为上一节计算的所有支撑点集合式（1）中\\[p(d_n|S,o_n^{(l)})\\]称为先验概率\\[p(o_n^{(r)}|o_n^{(l)},d_n)\\]称为似然概率先验概率建模先验概率表示给定支撑点集合、左图某个观测值，视差为$d_n$的分布ELAS认为其符合高斯分布，建模为\\[p(d_n|S,o_n^{(l)})\\propto \\begin{cases}\\gamma+e^{-\\frac{d_n-\\mu(S,o_n^{(l)})}{2\\sigma^2}}&amp; |d_n-\\mu|&lt;3\\sigma \\\\ 0&amp; other\\end{cases}\\tag{2}\\]其中$\\mu(S,o_n^{(l)})$表示观测点基于邻域支撑点的视差均值那么怎么计算这个邻域的视差均值呢？因为支撑点的分布是不均匀的，不方便直接网格化处理，于是ELAS基于支撑点使用三角剖分确定邻域对每个剖分，可以以$(u,v,d)$为坐标拟合一个视差平面，那么处于该平面的每个$(u,v)$就可以计算一个理论$d$作为其视差均值了\\[\\mu=&gt;au_n+bv_n+c=d\\tag{3}\\]似然概率建模似然概率表示给定左图观测点、视差值，右图观测点的分布ELAS认为其符合拉普拉斯分布，建模为\\[p(o_n^{(r)}|o_n^{(l)},d_n)\\propto \\begin{cases}e^{-\\beta||f_n^{(l)}-f_n^{(r)}||_1}&amp; (u_n^{(l)}=u_n^{(r)}+d_n) \\&amp;(v_n^{(l)}=v_n^{(r)}) \\\\ 0&amp; other\\end{cases}\\tag{4}\\]可以看到，其实就是计算左右sobel特征向量的距离视差能量模型根据式子(1)、(2)、(4)，可以得到\\[\\hat{E}(d)=e^{-\\beta||f_n^{(l)}-f_n^{(r)}||_1}(\\gamma+e^{-\\frac{d_n-\\mu(S,o_n^{(l)})}{2\\sigma^2}}) \\tag{5}\\]将其取负对数（单调递减）\\[E(d)=\\beta||f_n^{(l)}-f_n^{(r)}||_1-log[\\gamma+e^{-\\frac{d_n-\\mu(S,o_n^{(l)})}{2\\sigma^2}}]\\tag{6}\\]在代码中，式(6)被修改为\\[E(d)=||f_n^{(l)}-f_n^{(r)}||_1+\\frac{log\\gamma - log(\\gamma+e^{-\\frac{d_n-\\mu(S,o_n^{(l)})}{2\\sigma^2}})}{\\beta}\\tag{7}\\]视差后处理左右一致性检查 -&gt; 连通域噪声去除 -&gt; 孔洞插值 -&gt; 近似双边滤波 -&gt; 中值滤波代码结构图1. ELAS官方代码解析文献【1】Geiger A, Roser M, Urtasun R. Efficient large-scale stereo matching[C]//Asian conference on computer vision. Springer, Berlin, Heidelberg, 2010: 25-38."
  },
  
  {
    "title": "匈牙利、KM算法及其实现",
    "url": "/posts/%E5%8C%88%E7%89%99%E5%88%A9-KM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E5%AE%9E%E7%8E%B0/",
    "categories": "数据结构",
    "tags": "",
    "date": "2021-08-24 00:00:00 +0000",
    





    
    "snippet": "背景  在目标检测跟踪算法中，我们需要对当前帧跟踪预测到的目标${A_i}$与检测到的目标${B_i}$做匹配，确定其是否属于同一个目标  其中匹配的依据可以是${A_i}$和${B_i}$的iou、宽高比、面积等  算法SORT【1】及其改进算法 Deep SORT【2】针对上述问题提出卡尔曼滤波+匈牙利算法的方案。其中卡尔曼滤波实现目标跟踪预测，匈牙利算法实现最大$\\{A_i\\}$和$\\...",
    "content": "背景  在目标检测跟踪算法中，我们需要对当前帧跟踪预测到的目标${A_i}$与检测到的目标${B_i}$做匹配，确定其是否属于同一个目标  其中匹配的依据可以是${A_i}$和${B_i}$的iou、宽高比、面积等  算法SORT【1】及其改进算法 Deep SORT【2】针对上述问题提出卡尔曼滤波+匈牙利算法的方案。其中卡尔曼滤波实现目标跟踪预测，匈牙利算法实现最大$\\{A_i\\}$和$\\{B_i\\}$的最大匹配  本文内容及图片参考《km算法入门》匈牙利算法匈牙利算法由匈牙利数学家Edmonds提出，用于解决二分图的最大匹配问题图1. 员工与工作的最大匹配如上图所示，一个公司有员工A、B、C，有三种工作a、b、c，员工与工作之间有连线表示该员工可以完成该工作（在目标检测中则是跟踪到的目标与检测的目标根据iou等指标可以视为同一目标）  二分图是指，顶点可以分为两个集合，两个集合之间的元素可以相连，但同一个集合内的元素不能相连  最大匹配是指，基于两个集合的连接情况，确定一个方案，实现尽可能多的匹配算法过程图示图2. 匈牙利算法图示  首先考虑A，将其与a匹配（图2左）；  然后考虑B，按图1，B可以与a匹配，但此时a已经与A匹配，于是考虑A与c匹配，B与a匹配解决冲突（图2中）；  最后考虑C，C只能与c匹配，但此时c已经与A匹配，考虑A与a匹配，但此时a以及与B匹配，考虑B与b匹配，最终得到最大匹配A-a、B-b、C-c（图2右）算法理论从图示2可以看到，匈牙利算法的核心就是递归的去解决匹配冲突，算法不停地寻找增广路径，通过对路径的取反增加匹配个数，当不能再找到增广路径时，算法就结束，得到最大匹配  增广路径是指：由一个未匹配的顶点开始，经过若干个匹配顶点，最终到达对面集合的一个未匹配顶点的路径。这条路径将两个不同集合的两个未匹配顶点通过一系列匹配顶点相连。  如图2中 B-&gt;a-&gt;A-&gt;c就是一条增广路径  增广路径的取反则是：把增广路径上奇数编号的边加入到已知匹配中，偶数编号的边从已知匹配中删除。每做一次取反操作，得到的匹配就比原匹配多一个（B-&gt;a-&gt;A-&gt;c =&gt; B-&gt;a , A-&gt;c）算法实现import numpy as npclass hungary_match:    '''    input:           relation_matrix: 关系矩阵, 二分图中a与b有连接则relation_matrix[a][b]=1    output:          匹配列表, N*2    '''    def match(self, relation_matrix):        self._relation = relation_matrix        self._matched  = np.zeros((relation_matrix.shape[1],), np.uint8)        for node_a in range(relation_matrix.shape[0]):            self._visited = np.zeros((relation_matrix.shape[1],), np.uint8)            self.dfs(node_a)        return self.make_matched()    def dfs(self, node_ind):        for node_b in range(self._relation.shape[1]):            # node_b 还没有被匹配，并且 node_ind 与 node_b 有连接            if not self._visited[node_b] and self._relation[node_ind][node_b]:                self._visited[node_b] = 1                if not self._matched[node_b] or self.dfs(self._matched[node_b]-1):                    self._matched[node_b] = node_ind+1                    return True        return False    def make_matched(self):        matched = []        for node_b in range(self._matched.shape[0]):            if self._matched[node_b] &gt; 0:                matched.append((self._matched[node_b]-1, node_b))        return matchedKM算法匈牙利算法将所有的连接一视同仁，但实际中每个连接往往带有权重：两个目标的iou大于某个阈值则确定连接，但iou更大的连接应该要被优先考虑KM算法是匈牙利算法的改进，解决上述带权二分图的最优匹配问题算法过程图示图3. 员工与工作的带权匹配  首先对每个顶点赋值，将左边顶点赋值为其连接的最大权重，右边的顶点赋值为0（图3(1)）  接下来考虑顶点A，KM的匹配规则是：只能选择权重相同的边，即满足左顶点值4+右顶点值0=连接权重4。基于该规则，建立匹配A-&gt;c  然后考虑顶点B，满足KM匹配规则的连接是B-&gt;c，但此时c已经与A匹配了，而对于A，此时也没有其他匹配能满足规则。对冲突路径B-&gt;c-&gt;A，将路径中左顶点-1，右顶点+1得到图3(2)。得到满足规则的匹配A-&gt;c, B-&gt;a  最后考虑顶点C，5+1 != 5显然此时无法匹配，将C-1满足规则。但此时c已经与A匹配了，A可以与a匹配，但a已经与B匹配了，但此时B已经无法与其他点匹配了。此时的增广路径为C-&gt;c-&gt;A-&gt;a-&gt;B，将路径中左顶点-1，右顶点+1，得到图3(3)。此时B与b可以匹配，将增广路径C-&gt;c-&gt;A-&gt;a-&gt;B-&gt;b取反，得到最优匹配C-&gt;c, A-&gt;a, B-&gt;b 图3(4)每次遇到冲突对顶点减1的操作其实就是为了获得最大匹配而降低效率算法实现import numpy as npclass hungary_match:    '''    input:           relation_matrix: 关系矩阵, 二分图中a与b有连接则relation_matrix[a][b]=weight          其中 weight 为该连接的权重    output:          匹配列表, N*2    '''    def match_km(self, relation_matrix):        self._minz          = 9999        self._relation      = relation_matrix        self._matched       = np.zeros((relation_matrix.shape[1],), np.uint8)        self._weight_node_a = np.max(relation_matrix, axis=1)        self._weight_node_b = np.zeros((relation_matrix.shape[1],), np.float32)        for node_a in range(relation_matrix.shape[0]):            while True:                self._visited_node_a = np.zeros((relation_matrix.shape[0],), np.uint8)                self._visited_node_b = np.zeros((relation_matrix.shape[1],), np.uint8)                if self.dfs_km(node_a): break                # 产生冲突，weight_node_a减去minz，weight_node_b加上minz                to_break = False                for node in range(relation_matrix.shape[0]):                    if self._visited_node_a[node]:                        weight_ = self._weight_node_a[node] - self._minz                        if weight_ &lt;= 0:                            to_break = True                            break                        self._weight_node_a[node] = weight_                if to_break: break                for node in range(relation_matrix.shape[1]):                    if self._visited_node_b[node]:                        self._weight_node_b[node] += self._minz                return self.make_matched()    def dfs_km(self, node_ind):        self._visited_node_a[node_ind] = 1        for node_b in range(self._relation.shape[1]):            if not self._visited_node_b[node_b] and self._relation[node_ind][node_b] &gt; 0:                diff = self._weight_node_a[node_ind] + self._weight_node_b[node_b] - self._relation[node_ind][node_b]                if diff == 0:                    self._visited_node_b[node_b] = 1                    if not self._matched[node_b] or self.dfs_km(self._matched[node_b]-1):                        self._matched[node_b] = node_ind+1                        return True                else:                    self._minz = min(self._minz, diff)                    self._minz = max(0.02, self._minz)        return False    def make_matched(self):        matched = []        for node_b in range(self._matched.shape[0]):            if self._matched[node_b] &gt; 0:                matched.append((self._matched[node_b]-1, node_b))        return matched参考【1】Bewley A ,  Ge Z ,  Ott L , et al. Simple Online and Realtime Tracking[J]. 2016 IEEE International Conference on Image Processing (ICIP), 2016.【2】Wojke N ,  Bewley A ,  Paulus D . Simple Online and Realtime Tracking with a Deep Association Metric[J]. IEEE, 2017:3645-3649."
  },
  
  {
    "title": "ubuntu上使用clash",
    "url": "/posts/ubuntu%E4%B8%8A%E4%BD%BF%E7%94%A8clash/",
    "categories": "linux",
    "tags": "",
    "date": "2021-08-18 00:00:00 +0000",
    





    
    "snippet": "update找到一个带GUI的clash版本，支持windows、linux，不需要再向下面一样手动配置了但依然需要配置系统网络，将clash中的端口号修改为系统网络的端口（1090）下载clashDreamacro项目下载最新clash linux 版本，将其解压缩到clash/目录下配置下载config_tmp.yaml 和 Country.mmdbcd clash/# [url]为机场...",
    "content": "update找到一个带GUI的clash版本，支持windows、linux，不需要再向下面一样手动配置了但依然需要配置系统网络，将clash中的端口号修改为系统网络的端口（1090）下载clashDreamacro项目下载最新clash linux 版本，将其解压缩到clash/目录下配置下载config_tmp.yaml 和 Country.mmdbcd clash/# [url]为机场给的链接wget -O config_tmp.yaml [url]wget -O Country.mmdb &lt;https://www.sub-speeder.com/client-download/Country.mmdb&gt;生成config.yaml上一步得到的config_tmp.yaml与clash不一定兼容，需要以clash官方提供的为模板，将订阅信息更新进去## config.yaml 模板# HTTP(S) and SOCKS5 server on the same portmixed-port: 1090# Set to true to allow connections to the local-end server from# other LAN IP addressesallow-lan: false# This is only applicable when `allow-lan` is `true`# '*': bind all IP addresses# 192.168.122.11: bind a single IPv4 address# \"[aaaa::a8aa:ff:fe09:57d8]\": bind a single IPv6 addressbind-address: '*'# Clash router working mode# rule: rule-based packet routing# global: all packets will be forwarded to a single endpoint# direct: directly forward the packets to the Internetmode: rule# Clash by default prints logs to STDOUT# info / warning / error / debug / silentlog-level: debug# When set to false, resolver won't translate hostnames to IPv6 addressesipv6: false# RESTful web API listening addressexternal-controller: 127.0.0.1:9090# Static hosts for DNS server and connection establishment (like /etc/hosts)## Wildcard hostnames are supported (e.g. *.clash.dev, *.foo.*.example.com)# Non-wildcard domain names have a higher priority than wildcard domain names# e.g. foo.example.com &gt; *.example.com &gt; .example.com# P.S. +.foo.com equals to .foo.com and foo.comhosts:  # '*.clash.dev': 127.0.0.1  # '.dev': 127.0.0.1  # 'alpha.clash.dev': '::1'proxies:  # vmess  # cipher support auto/aes-128-gcm/chacha20-poly1305/none  - name: \"vmess1\"    type: vmess    server: cdn-cn.123.cn    port: 19083    uuid: a381baf6-1122-3676-a364-8a90219e8b22    alterId: 0    cipher: auto    network: ws    ws-path: /aaff  - name: \"vmess2\"    type: vmess    server: cdn-cn.123.cn    port: 19049    uuid: a381baf6-1122-3676-a364-8a90219e8b22    alterId: 0    cipher: auto    network: ws    ws-path: /ddefg  - name: \"vmess3\"    type: vmess    server: cdn-cn.123.cn    port: 19050    uuid: a381baf6-1122-3676-a364-8a90219e8b22    alterId: 0    cipher: auto    network: ws    ws-path: /cat  - name: \"vmess4\"    type: vmess    server: cdn-cn.123.cn    port: 19042    uuid: a381baf6-1122-3676-a364-8a90219e8b22    alterId: 0    cipher: auto    network: ws    ws-path: /dogproxy-groups:  # select is used for selecting proxy or proxy group  # you can use RESTful API to switch proxy is recommended for use in GUI.  - name: \"group1\"    type: select    # disable-udp: true    proxies:      - vmess1      - vmess2  - name: \"group2\"    type: url-test    proxies:      - vmess1      - vmess2      - vmess3      - vmess4    # tolerance: 150    lazy: true    url: 'http://www.gstatic.com/generate_204'    interval: 300proxy-providers:rules:  - DOMAIN-SUFFIX,google.com,group1  - DOMAIN-SUFFIX,qq.com,REJECT  - DOMAIN-SUFFIX,cnblogs.com,DIRECT  - IP-CIDR,127.0.0.0/8,DIRECT  - IP-CIDR,192.168.0.0/16,DIRECT  - IP-CIDR,10.0.0.0/8,DIRECT  - MATCH,group2从wget生成的config_tmp.yaml中，将proxies、proxy-groups、rules信息更新进模板，得到config.yaml配置系统网络图1. 系统网络设置如图，将网络代理设为手动设置HTTP代理、HTTPS代理、Socks主机的IP为127.0.0.1，端口分别设置为1090、1090、9090设置忽略主机为localhost, 127.0.0.0/8, ::1配置board终端执行./clash -f ./config.yaml 开启clash浏览器打开 https://clash.razord.top/ 进入board，一切没问题就可以在board上选择节点，选择全局代理图2. clash board设置配置终端为了让终端也能走proxy，在~/.bashrc内加入export http_proxy=http://127.0.0.1:1090export https_proxy=http://127.0.0.1:1090"
  },
  
  {
    "title": "【CNN系列目标检测】（8）YOLO3算法",
    "url": "/posts/CNN%E7%B3%BB%E5%88%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B-8-YOLO3%E7%AE%97%E6%B3%95/",
    "categories": "深度学习, CNN系列目标检测",
    "tags": "CNN系列目标检测",
    "date": "2021-08-05 00:00:00 +0000",
    





    
    "snippet": "  YOLOv3【1】使用了BN+Resnet，与v1和v2不再是同一代  YOLOv3在保证与SSD和RetinaNet精度相当的前提下速度达到其3倍左右，是工程界首选检测算法  重点参考《YOLOv3 算法的一点理解》、《yolo系列之yolo v3【深度解析】》图1. YOLOv3性能对比网络结构yolo 每一代的提升很大一部分取决于backbone，yolov3使用darknet-5...",
    "content": "  YOLOv3【1】使用了BN+Resnet，与v1和v2不再是同一代  YOLOv3在保证与SSD和RetinaNet精度相当的前提下速度达到其3倍左右，是工程界首选检测算法  重点参考《YOLOv3 算法的一点理解》、《yolo系列之yolo v3【深度解析】》图1. YOLOv3性能对比网络结构yolo 每一代的提升很大一部分取决于backbone，yolov3使用darknet-53替代了v2的darknet-19，此外提供tiny-darknet获得更高速但精度略低的检测与前代相比，yolov3网络有如下几个不同：1）去池化和全连接：网络中不再使用池化层和全连接层，而是通过改变卷积的步长实现尺寸变化，这样可以一定程度上保留特征的空间信息2）DBL：每一个卷积层都附带BN和Leaky relu，组成最小组件DBL3）Resnet：DBL和残差网络Resnet组合形成大组件（BN改善梯度消失的问题，Resnet改善网络退化的问题）4）多尺度检测：网络输出3种尺度的检测结果，分别将图像缩小8倍、16倍、32倍5）类别与框绑定：对每种尺度的每个格子输出维度是$(80+5)\\times 3=255$，即每个格子预测3种框，每个框预测其位置信息$(x,y,w,h,prob)$和80种类别概率。对比yolo，是$20+5\\times 2=30$，即每个格子预测2种框，同时预测20个类别（框和类别是分离的）图2. YOLOv3网络结构图def YOLOv3(input_layer):    route_1, route_2, conv = backbone.darknet53(input_layer)    conv = common.convolutional(conv, (1, 1, 1024,  512))    conv = common.convolutional(conv, (3, 3,  512, 1024))    conv = common.convolutional(conv, (1, 1, 1024,  512))    conv = common.convolutional(conv, (3, 3,  512, 1024))    conv = common.convolutional(conv, (1, 1, 1024,  512))    conv_lobj_branch = common.convolutional(conv, (3, 3, 512, 1024))    conv_lbbox = common.convolutional(conv_lobj_branch, (1, 1, 1024, 3*(NUM_CLASS + 5)), activate=False, bn=False)    conv = common.convolutional(conv, (1, 1,  512,  256))    conv = common.upsample(conv)    conv = tf.concat([conv, route_2], axis=-1)    conv = common.convolutional(conv, (1, 1, 768, 256))    conv = common.convolutional(conv, (3, 3, 256, 512))    conv = common.convolutional(conv, (1, 1, 512, 256))    conv = common.convolutional(conv, (3, 3, 256, 512))    conv = common.convolutional(conv, (1, 1, 512, 256))    conv_mobj_branch = common.convolutional(conv, (3, 3, 256, 512))    conv_mbbox = common.convolutional(conv_mobj_branch, (1, 1, 512, 3*(NUM_CLASS + 5)), activate=False, bn=False)    conv = common.convolutional(conv, (1, 1, 256, 128))    conv = common.upsample(conv)    conv = tf.concat([conv, route_1], axis=-1)    conv = common.convolutional(conv, (1, 1, 384, 128))    conv = common.convolutional(conv, (3, 3, 128, 256))    conv = common.convolutional(conv, (1, 1, 256, 128))    conv = common.convolutional(conv, (3, 3, 128, 256))    conv = common.convolutional(conv, (1, 1, 256, 128))    conv_sobj_branch = common.convolutional(conv, (3, 3, 128, 256))    conv_sbbox = common.convolutional(conv_sobj_branch, (1, 1, 256, 3*(NUM_CLASS +5)), activate=False, bn=False)    return [conv_sbbox, conv_mbbox, conv_lbbox]backbone  yolov3使用darknet-53作为其主干网络  该网络最大的特点是：无池化层和全连接层、引入了Resnet单元图3. darknet-53结构上图中最后的平均池化和全连接是用来分类的，在yolo3中被去掉了图3展示了darknet-53与其他网络的性能对比，可以看到该网络速度远超其他网络，但精度却相当图4. darknet-53性能对比def darknet53(input_data):    input_data = common.convolutional(input_data, (3, 3,  3,  32))    input_data = common.convolutional(input_data, (3, 3, 32,  64), downsample=True)        for i in range(1):        input_data = common.residual_block(input_data,  64,  32, 64)    input_data = common.convolutional(input_data, (3, 3,  64, 128), downsample=True)    for i in range(2):        input_data = common.residual_block(input_data, 128,  64, 128)    input_data = common.convolutional(input_data, (3, 3, 128, 256), downsample=True)    for i in range(8):        input_data = common.residual_block(input_data, 256, 128, 256)    route_1 = input_data    input_data = common.convolutional(input_data, (3, 3, 256, 512), downsample=True)    for i in range(8):        input_data = common.residual_block(input_data, 512, 256, 512)    route_2 = input_data    input_data = common.convolutional(input_data, (3, 3, 512, 1024), downsample=True)    for i in range(4):        input_data = common.residual_block(input_data, 1024, 512, 1024)    return route_1, route_2, input_dataDBL结构DBL=conv+bn+leaky_reludef convolutional(input_layer, filters_shape, downsample=False, activate=True, bn=True):    if downsample:        input_layer = tf.keras.layers.ZeroPadding2D(((1, 0), (1, 0)))(input_layer)        padding = 'valid'        strides = 2    else:        strides = 1        padding = 'same'    conv = tf.keras.layers.Conv2D(filters=filters_shape[-1], kernel_size = filters_shape[0], strides=strides, padding=padding,                                  use_bias=not bn, kernel_regularizer=tf.keras.regularizers.l2(0.0005),                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01),                                  bias_initializer=tf.constant_initializer(0.))(input_layer)    if bn: conv = BatchNormalization()(conv)    if activate == True: conv = tf.nn.leaky_relu(conv, alpha=0.1)    return convResnet残差模块  网络深度的增加可能造成网络退化，即更深的网络表现还不如浅网络。Resnet认为这是因为网络很难模拟恒等映射，否则更深的网络肯定等于或好于浅网络，因此干脆从结构上直接造一个恒等映射，从此更深的网络设计变得可能  还有一种说法是，Resnet其实解决的是梯度碎片化的问题图5. resnet结构def residual_block(input_layer, input_channel, filter_num1, filter_num2):    short_cut = input_layer    conv = convolutional(input_layer, filters_shape=(1, 1, input_channel, filter_num1))    conv = convolutional(conv       , filters_shape=(3, 3, filter_num1,   filter_num2))    residual_output = short_cut + conv    return residual_output网络输出decode如上一节所述，每个格子的输出为$(80+5)*3=255$，其中3是指每个格子预测3种尺寸的框（anchor）,每个框包含5个值$(x,y,w,h,prob)$，同时对80种类别做估计anchor prior  因为yolo是直接回归框，不够稳定。从yolo2开始，作者在数据集上聚类得到框的先验尺寸anchor prior  yolo3中有3种尺寸的输出，每个格子预测3种框，因此anchor prior一共有$3*3=9$组，每组包含宽、高10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326  在代码实现中有2处涉及到该先验尺寸：网络输出结果框的高宽需要乘以该先验尺寸、预测的框需要与该先验尺寸做iou筛选# ANCHORS作为高宽的系数pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)# self.anchors作为筛选依据anchors_xywh[:, 0:2] = np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5anchors_xywh[:, 2:4] = self.anchors[i]iou_scale = self.bbox_iou(bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh)iou.append(iou_scale)iou_mask = iou_scale &gt; 0.3bounding box predition  网络预测的框$(t_x,t_y,t_w,t_h)$不是绝对坐标和尺寸，而是与其对应格子的相对值  下图中宽高的$p_w$和$p_h$就是上面提到的anchor prior，$\\sigma(t_x)$是$sigmoid$函数\\[\\sigma(x)=\\frac{1}{1+e^{-x}} \\tag{1}\\]图6. bounding box换算def decode(conv_output, i=0):    \"\"\"    return tensor of shape [batch_size, output_size, output_size, anchor_per_scale, 5 + num_classes]            contains (x, y, w, h, score, probability)    \"\"\"    conv_shape       = tf.shape(conv_output)    batch_size       = conv_shape[0]    output_size      = conv_shape[1]        conv_output = tf.reshape(conv_output, (batch_size, output_size, output_size, 3, 5 + NUM_CLASS))    conv_raw_dxdy = conv_output[:, :, :, :, 0:2]    conv_raw_dwdh = conv_output[:, :, :, :, 2:4]    conv_raw_conf = conv_output[:, :, :, :, 4:5]    conv_raw_prob = conv_output[:, :, :, :, 5: ]    y = tf.tile(tf.range(output_size, dtype=tf.int32)[:, tf.newaxis], [1, output_size])    x = tf.tile(tf.range(output_size, dtype=tf.int32)[tf.newaxis, :], [output_size, 1])    xy_grid = tf.concat([x[:, :, tf.newaxis], y[:, :, tf.newaxis]], axis=-1)    xy_grid = tf.tile(xy_grid[tf.newaxis, :, :, tf.newaxis, :], [batch_size, 1, 1, 3, 1])    xy_grid = tf.cast(xy_grid, tf.float32)    pred_xy = (tf.sigmoid(conv_raw_dxdy) + xy_grid) * STRIDES[i]    pred_wh = (tf.exp(conv_raw_dwdh) * ANCHORS[i]) * STRIDES[i]    pred_xywh = tf.concat([pred_xy, pred_wh], axis=-1)    pred_conf = tf.sigmoid(conv_raw_conf)    pred_prob = tf.sigmoid(conv_raw_prob)    return tf.concat([pred_xywh, pred_conf, pred_prob], axis=-1)损失函数yolo3中一共有置信度损失、框回归损失、分类损失3种置信度损失置信度损失反映预测框内有无物体，让模型学习分辨图像的前景和背景区域respond_bgd = (1.0 - respond_bbox) * tf.cast( max_iou &lt; IOU_LOSS_THRESH, tf.float32 )conf_focal = tf.pow(respond_bbox - pred_conf, 2)conf_loss = conf_focal * (            respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)            +            respond_bgd * tf.nn.sigmoid_cross_entropy_with_logits(labels=respond_bbox, logits=conv_raw_conf)    )其中respond_bbox是gorund truth 里有目标的概率，conf_focal是网络预测该格子内有目标的概率，respond_bgd是无物体的概率框回归损失giou = tf.expand_dims(bbox_giou(pred_xywh, label_xywh), axis=-1)input_size = tf.cast(input_size, tf.float32)# small bbox should to increase the lossbbox_loss_scale = 2.0 - 1.0 * label_xywh[:, :, :, :, 2:3] * label_xywh[:, :, :, :, 3:4] / (input_size ** 2)giou_loss = respond_bbox * bbox_loss_scale * (1- giou)  yolo1中考虑到越小的框对误差的容忍度越低（需要增大惩罚），因此对宽高做开根号处理，弱化框尺寸对损失值的影响。yolo3中用bbox_loss_scale计算该惩罚系数  giou_loss中的respond_bbox表示，框回归的损失仅在该格子内有目标时才计算  注意到这里计算iou用的是giou（广义IoU）GIoUGIoU【2】是一种新的优化边界框的方法，解决了IoU的两个问题：  预测框与真实框无重叠时，IoU为0，导致优化函数梯度为0，无法优化  预测框与真实框的IoU即使一致，也无法保证其对齐方式一致图7. 以上三种重叠方式IoU均一致图8. GIoU算法步骤    left_up = tf.maximum(boxes1[..., :2], boxes2[..., :2])    right_down = tf.minimum(boxes1[..., 2:], boxes2[..., 2:])    inter_section = tf.maximum(right_down - left_up, 0.0)    inter_area = inter_section[..., 0] * inter_section[..., 1]    union_area = boxes1_area + boxes2_area - inter_area    iou = inter_area / union_area    enclose_left_up = tf.minimum(boxes1[..., :2], boxes2[..., :2])    enclose_right_down = tf.maximum(boxes1[..., 2:], boxes2[..., 2:])    enclose = tf.maximum(enclose_right_down - enclose_left_up, 0.0)    enclose_area = enclose[..., 0] * enclose[..., 1]    giou = iou - 1.0 * (enclose_area - union_area) / enclose_area分类损失采用二分类的交叉熵，把多分类看成二分类问题prob_loss = respond_bbox * tf.nn.sigmoid_cross_entropy_with_logits(labels=label_prob, logits=conv_raw_prob)其他nms在检测阶段需要对预测的框做nms，这里依次对同类别的所有框，首先计算iou，如果大于阈值，则保留概率最大的框，剔除其他框def nms(bboxes, iou_threshold, sigma=0.3, method='nms'):    \"\"\"    :param bboxes: (xmin, ymin, xmax, ymax, score, class)    Note: soft-nms, https://arxiv.org/pdf/1704.04503.pdf          https://github.com/bharatsingh430/soft-nms    \"\"\"    # use set() to delete repeat class    classes_in_img = list(set(bboxes[:, 5]))    best_bboxes = []    for cls in classes_in_img:        cls_mask = (bboxes[:, 5] == cls)        cls_bboxes = bboxes[cls_mask]        while len(cls_bboxes) &gt; 0:            max_ind = np.argmax(cls_bboxes[:, 4])            best_bbox = cls_bboxes[max_ind]            best_bboxes.append(best_bbox)            cls_bboxes = np.concatenate([cls_bboxes[: max_ind], cls_bboxes[max_ind + 1:]])            iou = bboxes_iou(best_bbox[np.newaxis, :4], cls_bboxes[:, :4])            weight = np.ones((len(iou),), dtype=np.float32)            assert method in ['nms', 'soft-nms']            if method == 'nms':                iou_mask = iou &gt; iou_threshold                weight[iou_mask] = 0.0            if method == 'soft-nms':                weight = np.exp(-(1.0 * iou ** 2 / sigma))            cls_bboxes[:, 4] = cls_bboxes[:, 4] * weight            score_mask = cls_bboxes[:, 4] &gt; 0.            cls_bboxes = cls_bboxes[score_mask]    return best_bboxes学习率设置使用【3】里的cosine设置方式：一开始学习率比较小，确保网络训练的稳定性，然后逐渐增大学习率加快训练过程，然后慢慢降低学习率避免震荡# update learning rateglobal_steps.assign_add(1)if global_steps &lt; warmup_steps:    lr = global_steps / warmup_steps *cfg.TRAIN.LR_INITelse:    lr = cfg.TRAIN.LR_END + 0.5 * (cfg.TRAIN.LR_INIT - cfg.TRAIN.LR_END) * (                (1 + tf.cos((global_steps - warmup_steps) / (total_steps - warmup_steps) * np.pi))            )optimizer.lr.assign(lr.numpy())文献【1】Redmon J ,  Farhadi A . YOLOv3: An Incremental Improvement[J]. arXiv e-prints, 2018.【2】H  Rezatofighi,  Tsoi N ,  JY  Gwak, et al. Generalized Intersection Over Union: A Metric and a Loss for Bounding Box Regression[C]// 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.【3】He T ,  Zhang Z ,  H  Zhang, et al. Bag of Tricks for Image Classification with Convolutional Neural Networks[C]// 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019."
  },
  
  {
    "title": "神经网络的改进",
    "url": "/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%94%B9%E8%BF%9B/",
    "categories": "深度学习",
    "tags": "",
    "date": "2021-05-31 00:00:00 +0000",
    





    
    "snippet": "激活函数+代价函数通过代价函数和激活函数的配合避免  激活函数进入饱和区  误差大的时候偏导$\\frac{\\partial C}{\\partial w}$、$\\frac{\\partial C}{\\partial b}$大（学习速度快），误差小的时候偏导小（学习速度慢）sigmoid+交叉熵sigmoid激活函数\\[\\sigma(z)=\\frac{1}{1+e^{-z}}\\tag{1}\\]交叉...",
    "content": "激活函数+代价函数通过代价函数和激活函数的配合避免  激活函数进入饱和区  误差大的时候偏导$\\frac{\\partial C}{\\partial w}$、$\\frac{\\partial C}{\\partial b}$大（学习速度快），误差小的时候偏导小（学习速度慢）sigmoid+交叉熵sigmoid激活函数\\[\\sigma(z)=\\frac{1}{1+e^{-z}}\\tag{1}\\]交叉熵代价函数\\[C=-\\frac{1}{n}\\sum_x[y\\ln a+(1-y)\\ln (1-a)]\\tag{2}\\]假设对于1层网络，式(2)对$w$求偏导，有\\[\\begin{aligned}\\frac{\\partial C}{\\partial w}=&amp;\\frac{1}{n}\\sum_x\\left ( \\frac{y}{a}-\\frac{1-y}{1-a}\\right )\\frac{\\partial a}{\\partial w}\\\\=&amp;\\frac{1}{n}\\sum_x\\left ( \\frac{y}{\\sigma(z)}-\\frac{1-y}{1-\\sigma(z)}\\right )\\sigma'(z)x\\\\=&amp;\\frac{1}{n}\\sum_x\\frac{\\sigma'(z)x}{\\sigma(z)(1-\\sigma(z))}(\\sigma(z)-y)\\end{aligned}\\tag{3}\\]又因为\\[\\sigma'(z)=\\sigma(z)(1-\\sigma(z))\\tag{4}\\]代入(3)，有\\[\\frac{\\partial C}{\\partial w}=\\frac{1}{n}\\sum_xx(\\sigma(z)-y)\\tag{5}\\]从式(5)可以看到，使用sigmoid+交叉熵的方式，学习速度与误差呈正相关softmax+对数似然softmax激活函数\\[\\sigma(z_k)=\\frac{e^{z_k}}{\\sum_ke^{z_k}}\\tag{6}\\]对数似然代价函数\\[C=-\\ln a_y^L\\tag{7}\\]类似的，可以求得\\[\\begin{aligned}\\frac{\\partial C}{\\partial w_{jk}^L}=&amp;a_k^{L-1}(a_j^L-y_j)\\\\\\frac{\\partial C}{\\partial b_j^L}=&amp;a_k^L-y_j\\end{aligned}\\tag{8}\\]注意到式(6)中的softmax激活函数值域为$[0,1]$，因此很方便作为某类概率的估计tensorflow中的几种交叉熵首先说明下logit的定义\\[logit=ln{\\frac{p}{1-p}} \\tag{9}\\]其作用是将取值范围为[0,1]的值映射到[-inf,inf]tensorflow中后缀带logits的损失函数表示其将输入的预测值默认为[-inf,inf]范围，在函数内做softmax或sigmoid。因此函数外不需要手动做标准化，否则会影响结果和精度tf.nn.softmax_cross_entropy_with_logits衡量独立互斥离散分类任务的误差这类任务类与类之间是独立而且互斥的，一个样本对应一个类（目标分类）\\[loss=\\sum{z*-log(softmax(x))} \\tag{10}\\]其中$z=labels$，$x=logits$，对于每一个样本，loss为一个值从softmax的计算方式(6)可以看出，(10)中首先将所有类别的概率之和归一化，因为该样本只可能对应某一类&gt;&gt;&gt; logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]&gt;&gt;&gt; labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]&gt;&gt;&gt; tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)...    &lt;tf.Tensor: shape(2,), dtype=float32, numpy=array([0.16984604, 0.82474494], dtype=float32)&gt;tf.nn.sparse_softmax_cross_entropy_with_logits是tf.nn.softmax_cross_entropy_with_logits的易用版本，功能一样，但输入的label不再是one-hot，取值为[0,num_classes]，是哪一类就标对应的label&gt;&gt;&gt; logits = tf.constant([[2., -5., .5, -.1],...                      [0., 0., 1.9, 1.4],...                      [-100., 100., -100., -100.]])&gt;&gt;&gt; labels = tf.constant([0, 3, 1])&gt;&gt;&gt; tf.nn.sparse_softmax_cross_entropy_with_logits(...    labels=labels, logits=logits).numpy()array([0.29750752, 1.1448325, 0. ], dtype=float32)tf.nn.sigmoid_cross_entropy_with_logits衡量独立不互斥离散分类任务的误差这类任务类与类之间是独立但不互斥的，一个样本可能包含多个类别（目标检测）\\[loss=z*-log(sigmoid(x))+(1-z)*-log(1-sigmoid(x)) \\tag{11}\\]其中$z=labels$，$x=logits$，对于每一个样本，loss为一个向量，向量长度为类别数可以看到，对于每个类别的概率，(11)都是单独处理的，因为不同类别可能同时出现&gt;&gt;&gt; logits = tf.constant([1., -1., 0., 1., -1., 0., 0.])&gt;&gt;&gt; labels = tf.constant([0., 0., 0., 1., 1., 1., 0.5])&gt;&gt;&gt; tf.nn.sigmoid_cross_entropy_with_logits(...    labels=labels, logits=logits).numpy()array([1.3132617, 0.3132617, 0.6931472, 0.3132617, 1.3132617, 0.6931472, 0.6931472], dtype=float32)PyTorch中的两种交叉熵PyTorch中CrossEntropy和BCELoss两种交叉熵都可用于分类任务，但适用场景有差别：  CrossEntropy期望模型对GT类的输出越大越好  BCELoss期望模型对GT类的输出越接近1越好CrossEntropy参考PyTorch官方文档，CrossEntropy接受两种$y$的形式1）$y$为类别的标号\\[\\begin{aligned}l(x,y)&amp;=\\begin{cases}\\sum_{n=1}^N\\frac{1}{\\sum_{n=1}^Nw_{y_n}*1\\{y_n\\neq ignore\\_index\\}}l_n&amp;reduction='mean'\\\\\\sum_{n=1}^Nl_n&amp;reduction='sum'\\end{cases}\\\\l_n&amp;=-w_{y_n}log\\frac{exp(x_{n,y_n})}{\\sum_{c=1}^Cexp(x_{n,c})}*1\\{y_n\\neq ignore\\_index\\}\\end{aligned} \\tag{12}\\]x1 = [[0, 0, 1, 0]]x2 = [[0, 0, 100, 0]]# 样本真实类别id为2# 此时y的dtype需要为torch.longy = [[2]]式12中，$C$为类别数量（例子中为4），$N$为batch大小（例子中为1），$y_n$为第$n$个样本真实类别id（例子中为2）对于(x1, y)，计算的$loss=-log(\\frac{e}{1+1+e+1})=0.9048$对于(x2, y)，计算的$loss=-log(\\frac{e^{100}}{1+1+e^{100}+1})=0.$可以看到，CrossEntropy需要模型对应类别的输出越大，损失才越接近02）$y$为与$x$维度一样的概率分布\\[\\begin{aligned}l(x,y)&amp;=\\begin{cases}\\frac{\\sum_{n=1}^Nl_n}{N}&amp;reduction='mean'\\\\\\sum_{n=1}^Nl_n&amp;reduction='sum'\\end{cases}\\\\l_n&amp;=-\\sum_{c=1}^Cw_clog\\frac{exp(x_{n,c})}{\\sum_{i=1}^Cexp(x_{n,i})}y_{n,c}\\end{aligned} \\tag{13}\\]x1 = [[0, 0, 1, 0]]x2 = [[0, 0, 100, 0]]# 样本真实类别的概率分布# 此时y的dtype需要与x一致，一般为torch.float32y = [[0, 0, 1, 0]]对于(x1, y)\\[\\begin{aligned}loss=-log(&amp;0*\\frac{1}{1+1+e+1}+0*\\frac{1}{1+1+e+1}+\\\\&amp;1*\\frac{1}{1+1+e+1}+0*\\frac{1}{1+1+e+1})=0.9048\\end{aligned}\\]对于(x2, y)\\[\\begin{aligned}loss=-log(&amp;0*\\frac{1}{1+1+e^{100}+1}+0*\\frac{1}{1+1+e^{100}+1}+\\\\&amp;1*\\frac{e^{100}}{1+1+e^{100}+1}+0*\\frac{1}{1+1+e^{100}+1})=0.\\end{aligned}\\]BCELoss参考PyTorch官方文档，与函数torch.nn.functional.binary_cross_entropy等价要求$y$为与$x$维度一样的概率分布\\[\\begin{aligned}l(x,y)&amp;=\\begin{cases}mean(L)&amp;reduction='mean'\\\\sum(L)&amp;reduction='sum'\\end{cases}\\\\l_n&amp;=-w_n[y_nlogx_n+(1-y_n)log(1-x_n)]\\end{aligned} \\tag{14}\\]x1 = [[0, 0, 1, 0]]# 样本真实类别的概率分布# 此时y的dtype需要与x一致，一般为torch.float32y = [[0, 0, 1, 0]]对于(x1, y)\\[loss=(0log0+1log1)+(0log0+1log1)+(1log1+0log0)+(0log0+1log1)=0\\]一点思考比较(13)和(14)中关于$l_n$的计算，为什么CrossEntropy是$ylogx$的形式，而BCELoss是$ylogx+(1-y)log(1-x)$的形式？考虑式14，如果删掉$(1-y)log(1-x)$，那么只要模型对所有类别输出均为1就可实现loss=0，而这显然是不合理的而式13之所以能不考虑$(1-y)log(1-x)$，一是因为$x$的取值不再为[0,1]，二是softmax的操作相当于已经考虑了“负样本”其他激活函数1) 双曲正切函数tanh\\[\\sigma(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\\tag{15}\\]图1. 左：sigmoid，右：tanh  sigmoid激活函数的值域为(0,1)，tanh的值域为(-1,1)  在一些时候tanh的表现会比sigmoid更好一种理论认为：由于sigmoid的输出总是大于0，导致同一个神经元的所有权重每次都是一起增加或减小；而tanh的输出有正有负，使得同一神经元的权重可能一部分增加另一部分减小，可能更符合实际2) 修正线性单元ReLU\\[\\sigma(z)=max(0,z)\\tag{16}\\]  ReLU在实践中有不错的效果（似乎总是比sigmoid好，速度是后者3-5倍），其在$z&gt;0$的区间不会饱和，在$z \\le 0$的区间停止学习  计算量小，收敛比sigmoid和tanh快  可能会出现神经元死亡，之后梯度永远为0的情况3) Leaky ReLU\\[\\sigma(z)=max(\\alpha z, z)\\tag{17}\\]图2. 红色：ReLU；黄色：Leaky ReLU  一般取$\\alpha=0.1$，某种程度上改善ReLU神经元死亡的情况4) Swish/SiLU\\[\\begin{aligned}\\sigma(z)=&amp;z*sigmoid(\\beta z)\\\\=&amp;\\frac{z\\beta}{1+e^{-z}}\\end{aligned}\\tag{18}\\]图3. Swish  当$\\beta=0$时，swish是线性函数$z/2$；当$beta=\\inf$时，swish是Relu；因此Swish是介于线性函数和Relu之间的平滑函数  Swish在深层模型上效果优于ReLU  是Yolo5和YoloX中使用的激活函数（称为SiLU）5) Mish\\[\\begin{aligned}\\sigma(z)=&amp;z*tanh(softplus(z))\\\\=&amp;z*tanh(log(1+e^z))\\end{aligned}\\tag{19}\\]图4. 左：ReLU/softplus；右：Mish  这是Yolo4中使用的激活函数，相比Swish提升0.494%，相比ReLU提升1.671%过拟合降低过拟合的方法有：  增加训练样本的数量  正则化  对既有训练数据做扩展其中增加训练样本的数量最有效，但也最难做到正则化  正则化通常包含：L1正则化、L2正则化、dropout  L1正则化和L2正则化是在代价函数中加入权重的惩罚项，借此达到最小化代价和最小化权重之间的折中  dropout不改变代价函数，而是通过随机删减连接直接改变网络本身  正则化有效的机理目前没有严格的理论支撑，只是一种实验事实L1、L2正则化\\[\\begin{aligned}C=&amp;C_0+\\frac{\\lambda}{n}\\sum_w|w|\\\\C=&amp;C_0+\\frac{\\lambda}{2n}\\sum_ww^2\\end{aligned}\\tag{20}\\]式(11)上方是L1正则化，下方是L2正则化，其中  $C_0$是原始的代价函数（交叉熵、对数似然、二次等）  $\\lambda$是正则化参数，平衡最小化代价和最小化权重  L1倾向于“稀疏化”权重，即重要的连接上权重比较大，其他连接上的权重趋向0  L2倾向于“平均化”权重，即所有连接的权重整体按比例缩小dropout  dropout在每轮训练（前向+后向）中随机删除一半的隐含层神经元（相应的权重置0）；在预测时保留所有神经元，但隐含层神经元的输出减半  dropout认为这样相当于同时训练了多个网络，最终取多个网络的平均作为输出，从而降低过拟合；类似于adaboost里通过很多弱分类器组成一个强连接器的机理  dropout在训练大规模深度网络时很有用，这种网络很容易发生过拟合  dropout一般只应用在全连接层，不会用在卷积层。因为卷积层的共享权重有先天性的抗过拟合的效果数据扩展旋转、扭曲、加噪权重初始化  激活函数和代价函数的精心选择可以尽量避免输出层因为陷入饱和区导致的学习缓慢的问题，但无法改善隐藏层的饱和问题  合理的权重初始化(式12)可以避免隐藏层过早陷入激活函数饱和区，加速学习（但不会改变网络最终性能）\\[\\begin{aligned}w=&amp;N\\left( 0,\\frac{1}{\\sqrt{M}}\\right )\\\\b=&amp;N\\left(0,1\\right)\\end{aligned}\\tag{21}\\]其中$M$是神经元输入权重的个数，$N$是正态分布按式(15)，有\\[\\begin{aligned}z=&amp;\\sum_j^Mw_jx_j+b\\\\=&amp;N\\left(0,\\sqrt{1+\\frac{1}{M}}\\right)\\end{aligned}\\tag{22}\\]此时$z$服从均值为0，标准差接近1的正态分布（取值集中于0附近），可以确保激活函数不在饱和区内如果对每个权重$w$使用$N(0,1)$初始化，那么$z=N(0,\\sqrt{M+1})$，随着M的增大，$z$的分布曲线会越来越平，取值不再集中在0附近，无法保证激活函数不在饱和区内超参数选择      学习率$\\eta$1）从高往低尝试，在训练数据上的代价开始下降而非震荡或增加时作为$\\eta$的估计2）这只是个量级的估计，如首先尝试1.0，如果增加或震荡，则尝试0.1，0.01…3）也可以在验证集上测试，因为学习率只是控制梯度下降的步长，不直接影响最终分类精度        正则化参数$\\lambda$1）代价函数中先不加入$\\lambda$，等学习率$\\eta$确定后再加入2）在验证集中从$\\lambda=1.0$开始，根据网络表现逐步调整为0.1，0.001…找到合适的量级后再细调3）$\\lambda$确定好后，再返回重新优化学习率$\\eta$        批量数据大小1）选择每次训练的数据batch大小也是一种折中：太小了影响整体计算速度（无法充分发挥矩阵并行计算的性能）；太大了不能频繁更新权重2）与学习率$\\eta$一样可以用不同量级的值在验证集中测试，根据精度提升速度确定  目前已有自动搜索（grid search）各超参数的研究：综述【1】、贝叶斯自动优化方法【2】梯度消失和梯度爆炸在多层网络训练中，$l$层的权重更新值很大概率是比$l+1$更小。也就是说从输出层BP回来的误差被逐层削减，造成梯度消失的情况考虑包含3个隐含层的网络，每层只有1个神经元，有\\[\\frac{\\partial C}{\\partial b_1}=\\sigma'(z_1)w_2\\sigma'(z_2)w_3\\sigma'(z_3)w_4\\sigma'(z_4)\\frac{\\partial C}{\\partial a_4}\\tag{23}\\]以sigmoid为例$\\sigma(z)=\\frac{1}{1+e^{-z}}$，对应的导数在z=0取最大值0.25从式(20)可以看到，由于连乘的结构，每层的梯度是非常不稳定的，可能急速下降造成梯度消失，也可能急速上升造成梯度爆炸改善梯度不稳定的问题可以应用上面章节的方法：  卷积层的权值共享大大减少参数数量，使学习更容易  使用更多正则化技术（尤其是dropout）减少过拟合  使用充分大的数据集训练避免过拟合  使用ReLU代替sigmoid函数  使用GPU并做长时间训练文献【1】Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of machine learning research, 2012, 13(2).【2】Snoek J, Larochelle H, Adams R P. Practical bayesian optimization of machine learning algorithms[J]. arXiv preprint arXiv:1206.2944, 2012."
  },
  
  {
    "title": "反向传播四个基本方程",
    "url": "/posts/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%9B%9B%E4%B8%AA%E5%9F%BA%E6%9C%AC%E6%96%B9%E7%A8%8B/",
    "categories": "深度学习",
    "tags": "",
    "date": "2021-05-19 00:00:00 +0000",
    





    
    "snippet": "定义\\[\\begin{aligned}z_j^l&amp;=\\sum_kw_{jk}^la_k^{l-1}+b_j^l&amp;(1.1)\\\\a_j^l&amp;=\\sigma(z_j^l)&amp;(1.2)\\\\C&amp;=\\frac{1}{2}\\sum_j(y_j-a_j^l)^2&amp;(1.3)\\end{aligned}\\]其中  $z_j^l$为第$l$层第$j$个神经元激活函...",
    "content": "定义\\[\\begin{aligned}z_j^l&amp;=\\sum_kw_{jk}^la_k^{l-1}+b_j^l&amp;(1.1)\\\\a_j^l&amp;=\\sigma(z_j^l)&amp;(1.2)\\\\C&amp;=\\frac{1}{2}\\sum_j(y_j-a_j^l)^2&amp;(1.3)\\end{aligned}\\]其中  $z_j^l$为第$l$层第$j$个神经元激活函数的带权输入  $a_j^l$为第$l$层第$j$个神经元的激活输出，$\\sigma$是激活函数  $C$为输出层二次代价函数定义第$l$层的第$j$个神经元的误差$\\delta_j^l$为：\\[\\delta_j^l=\\frac{\\partial C}{\\partial z_j^l}\\tag{2}\\]BP基本方程\\[\\begin{aligned}\\delta_j^L&amp;=(a_j^L-y_j)\\sigma'(z_j^L)&amp;(3.1)\\\\\\delta_j^l&amp;=\\sigma'(z_j^l)\\sum_kw_{kj}^{l+1}\\delta_k^{l+1}&amp;(3.2)\\\\\\frac{\\partial C}{\\partial b_j^l}&amp;=\\sigma_j^l&amp;(3.3)\\\\\\frac{\\partial C}{\\partial w_{jk}^l}&amp;=a_k^{l-1}\\delta_j^l&amp;(3.4)\\end{aligned}\\]其中  $\\delta_j^L$是输出层第$j$个神经元误差  $\\delta_j^l$是第$l$层第$j$个神经元误差，式(3.2)实现了通过下一层的误差计算当前层误差  $\\frac{\\partial C}{\\partial b_j^l}$是代价函数关于网络中第$l$层第$j$个偏置的改变率，式(3.3)说明了该改变率就是对应神经元的误差  $\\frac{\\partial C}{\\partial w_{jk}^l}$是代价函数关于网络中连接第$l-1$层第$k$个神经元与第$l$层第$j$个神经元权重的改变率，式(3.4)表明其仅与该神经元误差和第$l-1$层第$k$个神经元的激活输出有关方程推导方程3.1\\[\\begin{aligned}\\delta_j^L=&amp;\\frac{\\partial C}{\\partial z_j^L}\\\\=&amp;\\frac{\\partial C}{\\partial a_j^L}\\frac{\\partial a_j^L}{\\partial z_j^L}\\\\=&amp;\\frac{\\partial C}{\\partial a_j^L}\\frac{\\partial [\\sigma(z_j^L)]}{\\partial z_j^L}\\\\=&amp;\\frac{[\\frac{1}{2}\\sum_k(y_k-a_k^L)^2]}{\\partial a_j^L}\\sigma'(z_j^L)\\\\=&amp;(a_j^L-y_j)\\sigma'(z_j^L)\\end{aligned}\\]方程3.2\\[\\begin{aligned}\\delta_j^l=&amp;\\frac{\\partial C}{\\partial z_j^l}\\\\=&amp;\\sum_k\\left ( \\frac{\\partial C}{\\partial z_k^{l+1}}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\right )\\\\=&amp;\\sum_k\\left ( \\delta_k^{l+1}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\right )\\end{aligned}\\]因为\\[\\begin{aligned}z_k^{l+1}=&amp;\\sum_j(w_{kj}^{l+1}a_j^l+b_k^{l+1})\\\\=&amp;\\sum_j(w_{kj}^{l+1}\\sigma(z_j^l)+b_k^{l+1})\\end{aligned}\\]有\\[\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}=w_{kj}^{l+1}\\sigma'(z_j^l)\\]所以\\[\\delta_j^l=\\sigma'(z_j^l)\\sum_k\\left ( \\delta_k^{l+1}w_{kj}^{l+1}\\right )\\]方程3.3\\[\\begin{aligned}\\frac{\\partial C}{\\partial b_j^l}=&amp;\\frac{\\partial C}{\\partial z_j^l}\\frac{\\partial z_j^l}{\\partial b_j^l}\\\\=&amp;\\delta_j^l\\frac{\\partial z_j^l}{\\partial b_j^l}\\end{aligned}\\]因为\\[z_j^l=\\sum_k\\left (w_{jk}^la_k^{l-1}+b_j^l\\right )\\]有\\[\\frac{\\partial z_j^l}{\\partial b_j^l}=1\\]所以\\[\\frac{\\partial C}{\\partial b_j^l}=\\delta_j^l\\]方程3.4\\[\\begin{aligned}\\frac{\\partial C}{\\partial w_{jk}^l}=&amp;\\frac{\\partial C}{\\partial z_j^l}\\frac{\\partial z_j^l}{\\partial w_{jk}^l}\\\\=&amp;\\delta_j^l\\frac{\\partial z_j^l}{\\partial w_{jk}^l}\\\\=&amp;\\delta_j^l\\frac{\\partial (\\sum_i(w_{ji}^la_i^{l-1}+b_j^l))}{\\partial w_{jk}^l}\\\\=&amp;\\delta_j^la_k^{l-1}\\end{aligned}\\]"
  },
  
  {
    "title": "线性代数（5）",
    "url": "/posts/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-5/",
    "categories": "数据结构",
    "tags": "",
    "date": "2020-02-29 00:00:00 +0000",
    





    
    "snippet": "特征值和特征向量  对$n$阶方阵$A$，如果满足\\[Ax=\\lambda x \\tag{1}\\]  其中$x$为$n$维非0向量，$\\lambda$为常数（可以为复数）  则称$\\lambda$为$A$的特征值，$x$为$A$属于$\\lambda$的特征向量式(1)的几何意义是：向量$x$经过转换矩阵$A$后，依然保持原来的方向，只有$\\lambda$倍伸缩的变化特征向量与特征值的求解对式...",
    "content": "特征值和特征向量  对$n$阶方阵$A$，如果满足\\[Ax=\\lambda x \\tag{1}\\]  其中$x$为$n$维非0向量，$\\lambda$为常数（可以为复数）  则称$\\lambda$为$A$的特征值，$x$为$A$属于$\\lambda$的特征向量式(1)的几何意义是：向量$x$经过转换矩阵$A$后，依然保持原来的方向，只有$\\lambda$倍伸缩的变化特征向量与特征值的求解对式(1)稍作变形，有\\[(A-\\lambda I)x=0 \\tag{2}\\]要使得式(2)有非0解，则$A-\\lambda I$为奇异矩阵，即\\[det(A-\\lambda I)=0 \\tag{3}\\]求解式(3)，可以得到所有特征值$\\lambda_i$将特征值代入式(2)，可以求得对应的特征向量$x_i$特征向量的性质1）所有特征值的和等于矩阵对角线之和（迹）\\[tr(A)=\\sum{\\lambda_i}=\\sum{a_{kk}} \\tag{4}\\]2）所有特征值的乘积等于主元乘积等于矩阵的行列式\\[\\prod{\\lambda_i}=\\prod{u_i}=|A| \\tag{5}\\]其中$u_i$为矩阵主元，通过消元得到3）对角矩阵和三角矩阵的特征值是其对角线上的元素\\[\\lambda_i=a_{ii}  \\tag{6}\\]4）对$n$阶的方阵，如果其有$n$个不同的特征值，那么对应的$n$个特征向量线性无关（充分不必要条件）方阵$A$的$S\\Lambda S^{-1}$分解  如果$n$阶方阵$A$有$n$个线性无关的特征向量，则\\[A=S\\Lambda S^{-1} \\tag{7}\\]  其中  $S$是由$A$的$n$个特征向量（列向量）组成的特征矩阵  $\\Lambda$是由$A$的$n$个特征值组成的对角阵证明：设$S=[x_1,x_2,…x_n]$，$\\Lambda=diag(\\lambda_1,\\lambda_2,…\\lambda_n)$其中$\\lambda_1,…,\\lambda_n$是$A$的特征值，$x_1,…,x_n$是对应的线性无关的特征向量，有：\\[\\begin{aligned}AS=&amp;A[x_1,x_2,...x_n]\\\\=&amp;[\\lambda_1x_1,\\lambda_2x_2,...\\lambda_nx_n]\\\\=&amp;[x_1,x_2,...,x_n]diag(\\lambda_1,\\lambda_2,...,\\lambda_n)\\\\=&amp;S\\Lambda\\end{aligned} \\tag{8}\\]由(8)式易得(7)式由式(7)，易得\\[A^k=S\\Lambda^kS^{-1} \\tag{9}\\]也就是说，方阵的k次幂不改变特征向量，特征值是原来的k倍由式(9)，如果$A$所有的特征值\\[|\\lambda_i|&lt;1\\]那么$A^k\\to0\\ as\\ k\\to\\infty$  如果$n$阶方阵$A$没有$n$个线性无关的特征向量，则无法对角化，但可以借助若尔当标准型近似标准化$Au_k=u_{k+1}$的推导由$Au_k=u_{k+1}$可知\\[u_k=A^ku_0 \\tag{10}\\]令$x_1,x_2,…,x_n$是$A$的$n$个线性无关的特征向量，则$u_0$可分解为\\[u_0=c_1x_1+c_2x_2+...+c_nx_n \\tag{11}\\]式(11)两边左乘$A^k$，有\\[A^ku_0=u_k=c_1\\lambda_1^kx_1+c_2\\lambda_2^kx_2+...+c_n\\lambda_n^kx_n \\tag{12}\\]一个例子  斐波那契数列：$0,1,1,2,3,5,8…$，求$F_{100}$首先建立方程组\\[\\begin{aligned}F_{k+2}=&amp;F_{k+1}+F_k\\\\F_{k+1}=&amp;F_{k+1}\\end{aligned} \\tag{13}\\]（第二个等式的意义是使得$A$为方阵）令\\[u_k=\\left[\\begin{matrix}F_{k+1}\\\\F_k\\end{matrix}\\right]\\]则有\\[u_{k+1}=\\left[\\begin{matrix}1&amp;1\\\\1&amp;0\\end{matrix}\\right]u_k \\tag{14}\\]令\\[A=\\left[\\begin{matrix}1&amp;1\\\\1&amp;0\\end{matrix}\\right]\\]其特征值和对应的特征向量分别为：\\[\\lambda_1=\\frac{1}{2}(1+\\sqrt{5}),x_1=\\left[\\begin{matrix}\\lambda_1\\\\1\\end{matrix}\\right]\\\\\\lambda_2=\\frac{1}{2}(1-\\sqrt{5}),x_2=\\left[\\begin{matrix}\\lambda_2\\\\1\\end{matrix}\\right]\\]根据式(12)，有\\[u_{100}=c_1\\lambda_1^{100}x_1+c_2\\lambda_2^{100}x_2 \\tag{15}\\]又因为$\\lambda_2&lt;1$，所以$\\lambda_2^{100}\\to 0$，有\\[u_{100}\\approx c_1\\lambda_1^{100}x_1 \\tag{16}\\]即\\[\\left[\\begin{matrix}F_{101}\\\\F_{100}\\end{matrix}\\right]\\approx c_1\\lambda_1^{100}\\left[\\begin{matrix}\\lambda_1\\\\1\\end{matrix}\\right] \\tag{17}\\]从而$F_{100}\\approx c_1\\lambda_1^{100}$特征值在马尔科夫链中的应用马尔科夫链是指：在一个系统中某些因素第$n$次结果只受第$n-1$次结果的影响。描述这种状态转移的矩阵称为马尔科夫矩阵表1. 天气转移概率表                   晴      阴      雨                  晴      3/4      1/2      1/4              阴      1/8      1/4      1/2              雨      1/8      1/4      1/4      表1中行表示今天的天气，列表示明天的天气如行2列3的1/2表示今天阴明天雨的概率为1/2由表1得到对应的马尔科夫矩阵\\[A=\\left[\\begin{matrix}3/4&amp;1/2&amp;1/4\\\\1/8&amp;1/4&amp;1/2\\\\1/8&amp;1/4&amp;1/4\\end{matrix}\\right] \\tag{18}\\]观察上式可以得出马尔科夫矩阵的2条性质：1）所有值均不大于1\\[0\\leq a_{ij}\\leq 1 \\tag{19}\\]因为描述的都是概率2）矩阵每列之和为1\\[\\sum_{i=1}^n{a_{ij}} = 1 \\tag{20}\\]条件概率此外从特征分解方面来看，马尔科夫矩阵的特征值有2个性质：  $\\lambda=1$是一个特征值  其余特征值满足\\[|\\lambda_i|&lt;1\\]从定义可以看出，上一节中$u_{k+1}=Au_k$的$A$就是马尔科夫矩阵，因为它连接了前后两个状态由此，在已知马尔科夫矩阵后便可以利用式(10-12)计算状态的变化趋势一个例子：假设$A_{2\\times 2}$矩阵表示加州和麻省的人口迁移概率：\\[\\left[\\begin{matrix}U_加\\\\U_麻\\end{matrix}\\right]_{t=k+1}=\\left[\\begin{matrix}0.9&amp;0.2\\\\0.1&amp;0.8\\end{matrix}\\right]\\left[\\begin{matrix}U_加\\\\U_麻\\end{matrix}\\right]_{t=k} \\tag{21}\\]并假定人口初始状态为\\[\\left[\\begin{matrix}U_加\\\\U_麻\\end{matrix}\\right]_{t=0}=\\left[\\begin{matrix}0\\\\1000\\end{matrix}\\right]\\]式(21)表示0.9概率的人留加州0.1概率的人迁移麻省，0.8概率的人留麻省0.2概率的人迁移加州（可以看出，加州的人会越来越多）从式(21)可以得到2个特征值$\\lambda_1=1$，$\\lambda_2=0.7$根据式(12)，可以得到$k$次迁移后的人口状况：\\[u_k=c_1\\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]+c_2(0.7)^k\\left[\\begin{matrix}-1\\\\1\\end{matrix}\\right] \\tag{22}\\]式(22)代入初始状态，可以求得$c_1=1000/3$，$c_2=2000/3$又因为$0.7^k\\to 0$，因此有\\[u_{\\infty}=\\frac{1000}{3}\\left[\\begin{matrix}2\\\\1\\end{matrix}\\right]\\]傅里叶级数的求解求傅里叶级数展开式(23)各个系数$a_i$\\[f(x)=a_0+a_1\\cos{x}+a_2\\sin{x}+a_3\\cos{2x}+a_4\\sin{2x}+... \\tag{23}\\]可以发现，上式中各个组成是正交基：$1、\\cos{x}、\\sin{x}、\\cos{2x}…$对于连续函数，向量内积变成了函数积分：\\[\\int_0^{2\\pi}sinxcosx=0\\]将式(23)两边与$\\cos{x}$做积分，根据标准正交基性质，有\\[\\begin{aligned}\\int_0^{2\\pi}\\cos{x}f(x)=&amp;\\int_0^{2\\pi}a_0\\cos{x}+a_1\\cos^2{x}+a_2\\sin{x}\\cos{x}+...\\\\=&amp;a_1\\int_0^{2\\pi}\\cos^2{x}\\end{aligned} \\tag{24}\\]得$a_1=\\frac{\\int_0^{2\\pi}f(x)\\cos{x}dx}{\\pi}$以此类推，可以得到各个系数特征值在微分方程中的应用首先举例说明通过特征值解微分方程对方程组\\[\\begin{aligned}\\frac{du_1}{dt}&amp;=-u_1+2u_2\\\\\\frac{du_2}{dt}&amp;=u_1-2u_2\\end{aligned} \\tag{25}\\]首先有\\[A=\\left[\\begin{matrix}-1&amp;2\\\\1&amp;-2\\end{matrix}\\right]\\]求得其特征向量为$\\lambda_1=0$、$\\lambda_2=-3$，对应的特征向量为$x_1=[2,1]^T$、$x_2=[1,-1]^T$那么微分方程(25)的通解为\\[u(t)=c_1e^{\\lambda_1t}x_1+c_2e^{\\lambda_2t}x_2 \\tag{26}\\]其中  $e^{\\lambda_1t}x_1$和$e^{\\lambda_2t}x_2$为特解  $c_1$和$c_2$为常数项，通过已知的$u(0)$可以求得下面来说明为什么(26)式成立不妨把(25)通写为\\[\\frac{du}{dt}=Au \\tag{27}\\]令$u=Sv$，其中$S$为常数矩阵，则(27)可写为\\[\\begin{aligned}\\frac{du}{dt}&amp;=ASv\\\\\\Rightarrow S\\frac{dv}{dt}&amp;=ASv\\\\\\Rightarrow \\frac{dv}{dt}&amp;=S^{-1}ASv\\\\&amp;=\\Lambda v\\end{aligned}\\tag{28}\\]      比较式(7)，可知$\\Lambda=S^{-1}AS$为特征值组成的对角矩阵，$S$为特征向量组成的矩阵        需要注意的是，这里$A$需满足式(7)的前提：$A$为$n$阶方阵，并且有$n$个线性无关的特征向量        式(28)的意义是将(27)中的$A$解偶（对角化）  由式(28)，可进一步解得\\[\\begin{aligned}v(t)&amp;=e^{\\Lambda t}v(0)\\\\u(t)&amp;=Se^{\\Lambda t}S^{-1}u(0)\\end{aligned} \\tag{29}\\]考虑指数函数的泰勒展开，有\\[\\begin{aligned}e^{At}&amp;=I+At+\\frac{(At)^2}{2}+\\frac{(At)^3}{6}+...+\\frac{(At)^n}{n!}\\\\&amp;=SS^{-1}+S\\Lambda S^{-1}t+...+\\frac{S\\Lambda^nS^{-1}}{n!}t^n\\\\&amp;=Se^{\\Lambda t}S^{-1}\\end{aligned}\\tag{30}\\]因此有\\[u(t)=Se^{\\Lambda t}S^{-1}u(0)=e^{At}u(0) \\tag{31}\\]在这里只用(29)的结论，以(25)为例，其包含2个特征值，于是有\\[\\begin{aligned}u(t)&amp;=\\left[\\begin{matrix}x_1&amp;x_2\\end{matrix}\\right]\\left[\\begin{matrix}e^{\\lambda_1t}&amp;0\\\\0&amp;e^{\\lambda_2t}\\end{matrix}\\right]\\left[\\begin{matrix}x_1&amp;x_2\\end{matrix}\\right]^{-1}u(0)\\\\&amp;=\\left[\\begin{matrix}e^{\\lambda_1t}x_1&amp;0\\\\0&amp;e^{\\lambda_2t}x_2\\end{matrix}\\right]\\left[\\begin{matrix}x_1&amp;x_2\\end{matrix}\\right]^{-1}u(0)\\\\&amp;=c_1e^{\\lambda_1t}x_1+c_2e^{\\lambda_2t}x_2\\end{aligned}\\]其中$c_1$和$c_2$是$[x_1,x_2]^{-1}u(0)$"
  },
  
  {
    "title": "线性代数（4）",
    "url": "/posts/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-4/",
    "categories": "数据结构",
    "tags": "",
    "date": "2020-02-24 00:00:00 +0000",
    





    
    "snippet": "行列式  行列式是对方阵的求值\\[det(A)=|A|\\]行列式十条性质1) 单位矩阵的行列式为1\\[det(I)=1 \\tag{1}\\]2）每交换一次方阵的行，其行列式的正负改变一次3）常数乘以方阵某一行的行列式，等于常数乘以方阵的行列式\\[\\left|\\begin{matrix}ta&amp;tb\\\\c&amp;d\\end{matrix}\\right|=t\\left|\\begin{mat...",
    "content": "行列式  行列式是对方阵的求值\\[det(A)=|A|\\]行列式十条性质1) 单位矩阵的行列式为1\\[det(I)=1 \\tag{1}\\]2）每交换一次方阵的行，其行列式的正负改变一次3）常数乘以方阵某一行的行列式，等于常数乘以方阵的行列式\\[\\left|\\begin{matrix}ta&amp;tb\\\\c&amp;d\\end{matrix}\\right|=t\\left|\\begin{matrix}a&amp;b\\\\c&amp;d\\end{matrix}\\right| \\tag{2}\\]根据(2)式，易得$det(tA)=t^ndet(A)$方阵某一行的加法可以按(3)分解\\[\\left|\\begin{matrix}a+a'&amp;b+b'\\\\c&amp;d\\end{matrix}\\right|=\\left|\\begin{matrix}a&amp;b\\\\c&amp;d\\end{matrix}\\right|+\\left|\\begin{matrix}a'&amp;b'\\\\c&amp;d\\end{matrix}\\right| \\tag{3}\\]4）如果方阵的2行相等，其行列式为0交换相等的两行，其行列式只有为0，才遵循性质25）对方阵做初等行变换（消元），不改变行列式\\[\\left|\\begin{matrix}a&amp;b\\\\c+la&amp;d+lb\\end{matrix}\\right|=\\left|\\begin{matrix}a&amp;b\\\\c&amp;d\\end{matrix}\\right| \\tag{4}\\]式(4)根据性质3、4，很容易推得6）若方阵有一行为0，其行列式为0（令式(2)的$t=0$，易得）7）三角方阵的行列式是其对角线元素的乘积\\[det(U)=\\left|\\begin{matrix}d_1&amp;*&amp;*\\\\0&amp;d_2&amp;*\\\\0&amp;0&amp;d_3\\end{matrix}\\right|=d_1*d_2*d_3 \\tag{5}\\]式(5)提供了一种行列式的计算方法：首先初等行变换为三角阵，然后取对角线元素相乘（注意行变换正负的改变）8）如果$det(A)=0$，那么$A$不可逆（奇异）9）矩阵乘积的行列式等于矩阵行列式的乘积\\[det(AB)=det(A)det(B) \\tag{6}\\]由(6)式可推得$det(A^{-1})=\\frac{1}{det(A)}$对角矩阵\\[D=\\left[\\begin{matrix}d_1&amp;0\\\\0&amp;d_2\\end{matrix}\\right]\\]的逆矩阵为\\[D^{-1}=\\left[\\begin{matrix}1/d_1&amp;0\\\\0&amp;1/d_2\\end{matrix}\\right]\\]11）方阵的转置后行列式保持不变\\[det(A^T)=det(A) \\tag{7}\\]行列式计算1）通用公式\\[det(A)=\\left|\\begin{matrix}a_{11}&amp;a_{12}&amp;\\cdots&amp;a_{1n}\\\\a_{21}&amp;a_{22}&amp;\\cdots&amp;a_{2n}\\\\\\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\a_{n1}&amp;a_{n2}&amp;\\cdots&amp;a_{nn}\\end{matrix}\\right|=\\sum_{p_1p_2...p_n}{(-1)^{t(p_1p_2...p_n)}a_{1p_1}a_{2p_2}...a_{np_n}} \\tag{8}\\]式(8)中$p_1,…p_n$是$n$的全排列，因此一共有$n!$个子项累加$t(p_1p_2…p_n)$表示排列的逆序数。如$t(32514)=5$，因为3在2前面、3在1前面、2在1前面、5在1前面、5在4前面2）代数余子式  行列式的元素$a_{ij}$的余子式是把它所在行列的元素都删掉形成的行列式（维度$n-1$）$M_{ij}$，对应的代数余子式为\\[A_{ij}=(-1)^{i+j}M_{ij} \\tag{9}\\]行列式等于其任意某行或某列的各元素与其对应代数余子式乘积之和：\\[\\begin{aligned}det(A)=&amp;\\sum_j^n{a_{1j}A_{1j}}\\\\=&amp;\\sum_i^n{a_{i1}A_{i1}}\\end{aligned} \\tag{10}\\]式(10)中$1$可以换成任意行/列3）初等行变换为三角阵利用式(5)的性质，这是MATLAB的内部计算方式行列式求逆\\[A^{-1}=\\frac{1}{det(A)}C^T \\tag{11}\\]其中$C^T$称为伴随矩阵，$C$由$A$各个元素的代数余子式构成令\\[A=\\left[\\begin{matrix}1&amp;2&amp;3\\\\1&amp;0&amp;-1\\\\0&amp;1&amp;1\\end{matrix}\\right]\\]那么对应的\\[C^T=\\left[\\begin{matrix}1&amp;1&amp;-2\\\\-1&amp;1&amp;4\\\\1&amp;-1&amp;-2\\end{matrix}\\right]\\]克拉默法则解线性方程组对\\[\\left[\\begin{matrix}2&amp;1&amp;-5&amp;1\\\\1&amp;-3&amp;0&amp;-6\\\\0&amp;2&amp;-1&amp;2\\\\1&amp;4&amp;-4&amp;6\\end{matrix}\\right]x=\\left[\\begin{matrix}8\\\\9\\\\-5\\\\0\\end{matrix}\\right]\\]先计算方程组的系数行列式\\[D=\\left|\\begin{matrix}2&amp;1&amp;-5&amp;1\\\\1&amp;-3&amp;0&amp;-6\\\\0&amp;2&amp;-1&amp;2\\\\1&amp;4&amp;-4&amp;6\\end{matrix}\\right|=27\\]再分别用“常数项”替换D中的对应列，并计算其行列式：\\[D_1=\\left|\\begin{matrix}8&amp;1&amp;-5&amp;1\\\\9&amp;-3&amp;0&amp;-6\\\\-5&amp;2&amp;-1&amp;2\\\\0&amp;4&amp;-4&amp;6\\end{matrix}\\right|=81, D_2=\\left|\\begin{matrix}2&amp;8&amp;-5&amp;1\\\\1&amp;9&amp;0&amp;-6\\\\0&amp;-5&amp;-1&amp;2\\\\1&amp;0&amp;-4&amp;6\\end{matrix}\\right|=-108\\]\\[D_3=\\left|\\begin{matrix}2&amp;1&amp;8&amp;1\\\\1&amp;-3&amp;9&amp;-6\\\\0&amp;2&amp;-5&amp;2\\\\1&amp;4&amp;0&amp;6\\end{matrix}\\right|=-27, D_4=\\left|\\begin{matrix}2&amp;1&amp;-5&amp;8\\\\1&amp;-3&amp;0&amp;9\\\\0&amp;2&amp;-1&amp;-5\\\\1&amp;4&amp;-7&amp;0\\end{matrix}\\right|=27\\]得到原方程组的解：$x_1=\\frac{D_1}{D}=3$,$x_2=\\frac{D_2}{D}=-4$,$x_3=\\frac{D_3}{D}=-1$,$x_4=\\frac{D_4}{D}=1$注：通过行列式求逆（式11）和利用克拉默法则解方程组虽然公式简洁，但计算量大，一般还是推荐用消元法实现（MATLAB内部也是用消元法）行列式求体积、面积  行列式的物理意义是体积假设$\\vec{a}$、$\\vec{b}$、$\\vec{c}$为某个立方体的三条边，它们的交点是原点，那么其行向量组成的行列式就是立方体的体积\\[\\left|\\begin{matrix}a\\\\b\\\\c\\end{matrix}\\right|=Volume \\tag{12}\\]相应的，假设$\\vec{a}$、$\\vec{b}$为某个平行四边形的两条边，他们的交点是原点，那么其行向量组成的行列式就是平行四边形的面积\\[\\left|\\begin{matrix}a\\\\b\\end{matrix}\\right|=Area \\tag{13}\\]"
  },
  
  {
    "title": "线性代数（3）",
    "url": "/posts/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-3/",
    "categories": "数据结构",
    "tags": "",
    "date": "2020-02-19 00:00:00 +0000",
    





    
    "snippet": "$Ax=b$无解时的求解从线性代数（1）列空间的讨论可知，如果$b$不在$A$的列空间$C(A)$内，那么$Ax=b$无解在实际应用中这种无解的情况很多：1）因为观察/测量的数据总会有误差，无法保证每个$b_i$都是精确的；2）未知数少，而限制条件多先说结论：将$Ax=b$转为求\\[A^TA\\hat{x}=A^Tb \\tag{1}\\]  式(1)的解是$Ax=b$的最优解  如果$C(A)$...",
    "content": "$Ax=b$无解时的求解从线性代数（1）列空间的讨论可知，如果$b$不在$A$的列空间$C(A)$内，那么$Ax=b$无解在实际应用中这种无解的情况很多：1）因为观察/测量的数据总会有误差，无法保证每个$b_i$都是精确的；2）未知数少，而限制条件多先说结论：将$Ax=b$转为求\\[A^TA\\hat{x}=A^Tb \\tag{1}\\]  式(1)的解是$Ax=b$的最优解  如果$C(A)$线性无关，那么$A^TA$必然可逆空间投影先探讨一维向量空间的情况图1. 一维向量空间投影如图1，$\\vec{p}$是$\\vec{b}$在$\\vec{a}$上的投影，有\\[\\vec{p}=x\\vec{a}\\\\\\vec{e}=\\vec{b}-\\vec{p} \\tag{2}\\]显然，$\\vec{e}$垂直于$\\vec{a}$，根据正交向量点积为0的定理，有\\[\\begin{aligned}a^T(b-xa)&amp;=0\\\\\\Rightarrow x&amp;=\\frac{a^Tb}{a^Ta}\\\\ \\Rightarrow p&amp;=xa=a\\frac{a^Tb}{a^Ta}\\\\\\Rightarrow P&amp;=\\frac{aa^T}{a^Ta}\\end{aligned} \\tag{3}\\]式(3)中$P$为投影矩阵，将$\\vec{b}$投影到$\\vec{a}$上  投影矩阵3条性质：  1.秩为1（如式3，投影矩阵为列向量与行向量乘积，满足秩1矩阵的性质）  2.转置不变：$P^T=P$，说明投影矩阵是对称矩阵  3.平方不变：$P^2=P$（投影2次和投影1次结果一样）现在在二维空间中讨论$Ax=b$解的问题假设$A$的列空间基为$a_1$、$a_2$，那么当$b$不在$a_1$和$a_2$确定的平面内时，$Ax=b$无解现在假设$c$是$b$在$a_1$和$a_2$确定的平面上的投影，即\\[c=\\hat{x}_1a_1+\\hat{x}_2a_2 \\tag{4}\\]那么$A\\hat{x}=c$肯定有解，而且是$Ax=b$误差最小的解令$e$为从$b$到$c$的投影“垂线”，那么有\\[e=b-A\\hat{x} \\tag{5}\\]并且\\[a_1^T(b-A\\hat{x})=0\\\\a_2^T(b-A\\hat{x})=0 \\tag{6}\\]将式(6)表示为矩阵形式\\[A^T(b-A\\hat{x})=0 \\tag{7}\\]从而\\[\\hat{x}=(A^TA)^{-1}A^Tb \\tag{8}\\]式(8)就是$Ax=b$的最优解最小二乘上的应用假设有3组二维数据$[1\\ 1]^T$、$[2\\ 2]^T$、$[3\\ 2]^T$，需要拟合一条误差最小的直线令直线为$y=x_1+x_2t$，上述问题可以表示为式(9)的矩阵形式\\[\\left[\\begin{matrix}1&amp;1\\\\1&amp;2\\\\1&amp;3\\end{matrix}\\right]\\left[\\begin{matrix}x_1\\\\x_2\\end{matrix}\\right]=\\left[\\begin{matrix}1\\\\2\\\\2\\end{matrix}\\right] \\tag{9}\\]显然上式无解，应用式(8)，可求得最优解：$\\hat{x}=[\\frac{2}{3}\\ \\frac{1}{2}]^T$即拟合的直线方程为$y=\\frac{2}{3}+\\frac{1}{2}t$现在用传统的目标函数的方式求解：\\[min[(x_1+x_2-1)^2+(x_1+2x_2-2)^2+(x_1+3x_2-2)^2] \\tag{10}\\]式(10)分别对$x_1$和$x_2$求偏导，有\\[3x_1+6x_2=5\\\\6x_1+14x_2=11 \\tag{11}\\]求解上述方程组，得$x_1=\\frac{2}{3}$，$x_2=\\frac{1}{2}$，与式(8)得到的结果一致正交矩阵  标准正交向量：\\[q_i^Tq_j=\\begin{cases}0&amp;i\\neq j\\\\1&amp;i=j\\end{cases}\\]  正交矩阵：$Q=\\left[\\begin{matrix}q_1&amp;q_2&amp;…&amp;q_n\\end{matrix}\\right]$其中$q_i$互相为标准正交向量，并且$Q$为方阵需要注意正交矩阵必须为方阵，这个是历史原因，其实并没道理根据正交矩阵的定义，一定有\\[Q^TQ=I\\\\Q^T=Q^{-1} \\tag{12}\\]Gram-Schmidt正交化给定一个线性无关向量组$a_1,a_2,…,a_n$，可以通过施密特正交法将其转换为相互正交的向量组$b_1,b_2,…,b_n$\\[\\begin{aligned}b_1 &amp;=a_1\\\\b_2&amp;=a_2-\\frac{&lt;a_2,b_1&gt;}{&lt;b_1,b_1&gt;}b_1\\\\b_3 &amp;=a_3-\\frac{&lt;a_3,b_1&gt;}{&lt;b_1,b_1&gt;}b_1-\\frac{&lt;a_3,b_2&gt;}{&lt;b_2,b_2&gt;}b_2\\\\...\\\\b_n &amp;=a_n-\\frac{&lt;a_n,b_1&gt;}{&lt;b_1,b_1&gt;}b_1-\\frac{&lt;a_n,b_2&gt;}{&lt;b_2,b_2&gt;}b_2-...-\\frac{&lt;a_n,b_{n-1}&gt;}{&lt;b_{n-1},b_{n-1}&gt;}b_{n-1}\\end{aligned}  \\tag{13}\\]式(13)的$\\frac{b_i}{||b_i||}$便得到标准正交向量式(13)的\\[\\frac{&lt;a_n,b_{n-1}&gt;}{&lt;b_{n-1},b_{n-1}&gt;}=\\frac{b_{n-1}^Ta_n}{b_{n-1}^Tb_{n-1}} \\tag{14}\\]回忆空间投影里的式(2)和(3)，可以发现(13)的形式与其一致事实上式(13)可以解释为：$b_2$是$a_2$投影到$a_1$的那个$e$（图1）$A=QR分解$对任意$A\\in \\mathbb{R}^{m\\times n}(m\\geq n)$，总有\\[A=QR \\tag{15}\\]其中$Q\\in \\mathbb{R}^{m\\times n}$，其列空间是$A$列空间的标准正交向量其中$R\\in \\mathbb{R}^{n\\times n}$是上三角矩阵\\[\\left[\\begin{matrix}r_1&amp;r_2\\\\0&amp;r_3\\end{matrix}\\right]\\]"
  },
  
  {
    "title": "线性代数（2）",
    "url": "/posts/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-2/",
    "categories": "数据结构",
    "tags": "",
    "date": "2020-02-12 00:00:00 +0000",
    





    
    "snippet": "线性相关性给定一组向量$\\vec{x}_1,\\vec{x}_2,\\vec{x}_3,…,\\vec{x}_n$，如果存在不全为0的数$c_1,c_2,c_3,…,c_n$，使得\\[c_1\\vec{x}_1+c_2\\vec{x}_2+...+c_n\\vec{x}_n=0\\]则称这组向量线性相关，否则称其线性无关设$\\vec{v}_1,…\\vec{v}_n$是矩阵$A$的列向量（组成列空间$C(A...",
    "content": "线性相关性给定一组向量$\\vec{x}_1,\\vec{x}_2,\\vec{x}_3,…,\\vec{x}_n$，如果存在不全为0的数$c_1,c_2,c_3,…,c_n$，使得\\[c_1\\vec{x}_1+c_2\\vec{x}_2+...+c_n\\vec{x}_n=0\\]则称这组向量线性相关，否则称其线性无关设$\\vec{v}_1,…\\vec{v}_n$是矩阵$A$的列向量（组成列空间$C(A)$）当$A$的零空间$N(A)$仅包含零向量时，这些列向量线性无关，矩阵A的秩$r(A)=n$，$C(A)$的维数是$n$否则，这些列向量线性相关，矩阵A的秩$r(A)&lt;n$，$C(A)$的维数$&lt;n$(秩是矩阵才有的概念，对应向量组的维数)(回忆秩的定义：矩阵$A$的主列数目)基  给定一组向量$\\vec{v}_1,\\vec{v}_2,…\\vec{v}_d$，如果满足  1）线性无关  2）可以生成(通过线性组合)一个向量空间$p$  则称这组向量是空间$p$的一组基1）一个空间的基有无数组2）一个空间的所有基向量个数相等，称为这个空间的维数（如定义里空间$p$的维数是$d$）四个基本子空间给定一个矩阵$A_{m\\times n}$，可以构造如下4种基本子空间            名称      符号      维数      基                  列空间      $C(A)$      $r$      消元后得到的$r$个主列              零空间      $N(A)$      $n-r$      消元后得到的$n-r$个自由列              行空间      $C(A^T)$      $r$      最简型$R$的前$r$行              左零空间      $N(A^T)$      $m-r$      左乘$E$矩阵的后$m-r$行        矩阵列空间的维数 = 行空间的维数=矩阵的秩  列空间的秩与零空间的秩参考线性代数（1）的“零空间”行空间初等行变换化简\\[\\begin{aligned}A&amp;=\\left[\\begin{matrix}1&amp;2&amp;3&amp;1\\\\1&amp;1&amp;2&amp;1\\\\1&amp;2&amp;3&amp;1\\end{matrix}\\right]\\\\&amp;\\Rightarrow \\left[\\begin{matrix}1&amp;2&amp;3&amp;1\\\\0&amp;-1&amp;-1&amp;0\\\\0&amp;0&amp;0&amp;0\\end{matrix}\\right]\\\\&amp;\\Rightarrow \\left[\\begin{matrix}1&amp;0&amp;1&amp;1\\\\0&amp;1&amp;1&amp;0\\\\0&amp;0&amp;0&amp;0\\end{matrix}\\right]\\end{aligned} \\tag{1}\\]式(1)第一行最右的矩阵是常见的阶梯型，根据该矩阵就可以获知主列和自由列的情况第二行的矩阵是最简型$R$因为$R$是由$A$行向量通过线性变换得到的，因此并没有改变行空间$C(A^T)$，非0行就是行空间的基，非0行的数量就是行空间的维数（同时也是列空间的维数）左零空间先解释“左零”的由来：$N(A^T)$表示的是$A^TX=0$的解空间，而\\[A^TX=0\\Rightarrow (A^TX)^T=0\\Rightarrow X^TA=0 \\tag{2}\\]左零空间的基可以通过先转置$A$然后求其自由列获得，也可以由以下方式获得：以(1)式的$A$为例1）初等行变换\\[[A_{m\\times n}|I_{m\\times m}]\\]左侧消元为最简型\\[[R_{m\\times n}|E_{m\\times m}]\\]\\[\\left[\\begin{array}{lccr|lcr}1&amp;2&amp;3&amp;1&amp;1&amp;0&amp;0\\\\1&amp;1&amp;2&amp;1&amp;0&amp;1&amp;0\\\\1&amp;2&amp;3&amp;1&amp;0&amp;0&amp;1\\end{array}\\right]\\Rightarrow \\left[\\begin{array}{lccr|ccr}1&amp;0&amp;1&amp;1&amp;-1&amp;2&amp;0\\\\0&amp;1&amp;1&amp;0&amp;1&amp;-1&amp;0\\\\0&amp;0&amp;0&amp;0&amp;-1&amp;0&amp;1\\end{array}\\right] \\tag{3}\\]可以验证，$EA=R$2）取$E$的后$m-r$行即为基对比$EA=R$与式(2)，可以发现$E$的最后$m-r$行可以使得$A$的左乘为0在式(3)的例子中就是最后$3-2=1$行$[-1\\ 0\\ 1]$为$N(A^T)$的基矩阵空间向量空间表示的是一个对向量加法和数乘封闭的向量集合，实际上空间这个概念可以扩展：由于矩阵也支持加法和数乘计算，因此  对矩阵加法和数乘封闭的矩阵集合组成矩阵空间记$M$为$3\\times 3$的矩阵空间，显然对应的基有9个，可以为：\\[\\left[\\begin{matrix}1&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;1&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],...,\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;1\\end{matrix}\\right] \\tag{4}\\]记$S$为$3\\times 3$的对称矩阵组成的矩阵空间，显然$S$是$M$的子空间，对应的基有6个，可以为：\\[\\left[\\begin{matrix}1&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;1\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;1&amp;0\\\\1&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;1\\\\0&amp;0&amp;0\\\\1&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;0&amp;1\\\\0&amp;1&amp;0\\end{matrix}\\right] \\tag{5}\\]记$U$为$3\\times 3$的上三角矩阵组成的矩阵空间，$U$是$M$的子空间，对应的基有6个，可以为：\\[\\left[\\begin{matrix}1&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;1\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;1&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;1\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;0&amp;1\\\\0&amp;0&amp;0\\end{matrix}\\right] \\tag{6}\\]记$D$为$3\\times 3$的对角矩阵组成的矩阵空间，$D$是$M$的子空间，同时也是$S$和$U$的交集，对应的基有3个，可以为：\\[\\left[\\begin{matrix}1&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;1&amp;0\\\\0&amp;0&amp;0\\end{matrix}\\right],\\left[\\begin{matrix}0&amp;0&amp;0\\\\0&amp;0&amp;0\\\\0&amp;0&amp;1\\end{matrix}\\right] \\tag{7}\\]秩1矩阵  任何一个秩为1的矩阵都可以分解为列向量与行向量的乘积，即$A=UV^T$令\\[A=\\left[\\begin{matrix}1&amp;4&amp;5\\\\2&amp;8&amp;10\\end{matrix}\\right]\\]有\\[A=\\left[\\begin{matrix}1&amp;4&amp;5\\\\2&amp;8&amp;10\\end{matrix}\\right]=\\left[\\begin{matrix}1\\\\2\\end{matrix}\\right]\\left[\\begin{matrix}1&amp;4&amp;5\\end{matrix}\\right] \\tag{8}\\]正交向量  两条向量正交的充要条件是 $X^TY=0$设$X$和$Y$分别是直角三角形的两条直角边，显然有\\[\\begin{aligned}||X||^2+||Y||^2&amp;=||X+Y||^2\\\\&amp;\\Rightarrow X^TX+Y^TY\\\\&amp;=X^TX+Y^TY+2X^TY\\\\&amp;\\Rightarrow X^TY=0\\end{aligned} \\tag{9}\\]  如果两个子空间$A$与$B$正交，那么$A$中的任意向量与$B$的任意向量正交由此可得：行空间$C(A^T)$正交于零空间$N(A)$"
  },
  
  {
    "title": "线性代数（1）",
    "url": "/posts/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0-1/",
    "categories": "数据结构",
    "tags": "",
    "date": "2020-02-06 00:00:00 +0000",
    





    
    "snippet": "方程组的“行图像”与“列图像”对一个二元二次方程组\\[2x-y=0\\\\-x+2y=3 \\tag{1}\\]可以记为$AX=b$的行图像形式\\[\\left[\\begin{matrix}2 &amp; -1\\\\-1 &amp; 2\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right]=\\left[\\begin{matrix}0\\...",
    "content": "方程组的“行图像”与“列图像”对一个二元二次方程组\\[2x-y=0\\\\-x+2y=3 \\tag{1}\\]可以记为$AX=b$的行图像形式\\[\\left[\\begin{matrix}2 &amp; -1\\\\-1 &amp; 2\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\3\\end{matrix}\\right] \\tag{2}\\]在图像上这种方式表示两条直线，其交点就是方程的解同时方程组(1)也可记为列图像形式\\[x\\left[\\begin{matrix}2\\\\-1\\end{matrix}\\right]+y\\left[\\begin{matrix}-1\\\\2\\end{matrix}\\right]=\\left[\\begin{matrix}0\\\\3\\end{matrix}\\right] \\tag{3}\\]在图像上这种方式表示两条向量$[2\\ -1]^T$和$[-1\\ \\ 2]^T$的某种线性组合得到$[0\\ \\ 3]^T$可以看到，“列图像”的描述方式更直观，但$AX=b$的形式更方便求解。逆矩阵什么情况下$AX=b$有解呢？从“列图像”上看就是涉及到的向量不共线，相应的$A$可逆（非奇异）  设A是n阶方阵，如果存在n阶方阵B，使得$AB=BA=I$，那么称A可逆，B是其逆矩阵  如果能找到一个非0向量$X$，使得$AX=0$成立，那么A不可逆高斯-约旦消元法求逆  假设\\[A=\\left[\\begin{matrix}1&amp;3\\\\2&amp;7\\end{matrix}\\right]\\]  首先将$A$与$I$合为增广矩阵\\[\\widehat{A}=\\left[\\begin{array}{lr|lr}1&amp;3&amp;1&amp;0\\\\2&amp;7&amp;0&amp;1\\end{array}\\right]\\]  然后对$\\widehat{A}$做消元使得其左部分变成$I$：\\[\\left[\\begin{array}{lr|lr}1&amp;0&amp;7&amp;-3\\\\0&amp;1&amp;-2&amp;1\\end{array}\\right]\\]  消元（行变换）后右侧即为所求：\\[A^{-1}=\\left[\\begin{matrix}7&amp;-3\\\\-2&amp;1\\end{matrix}\\right]\\]简单的证明：  对增广阵左乘某个矩阵$B$\\[B[A | I]\\]  消元后有$BA=I$，因此$B=A^{-1}$  又因为右侧$BI=B$，因此消元后右侧即为所求逆矩阵LU分解  LU分解是将一个矩阵分解为一个单位下三角矩阵和一个上三角矩阵乘积对任意一个非奇异的方阵A，其LU分解总是存在的LU分解主要应用在数值分析中，用来解线性方程、求逆矩阵或计算行列式1）消元法（Doolittle算法）求LU      首先对矩阵A通过初等行变换将其变为一个上三角矩阵U，这个过程中对应的变换矩阵就变成一个单位下三角矩阵        对  \\[A=\\left[\\begin{matrix}2&amp;1\\\\8&amp;7\\end{matrix}\\right]\\]行二加上行一的-4倍，有：\\[\\left[\\begin{matrix}1&amp;0\\\\-4&amp;1\\end{matrix}\\right]\\left[\\begin{matrix}2&amp;1\\\\8&amp;7\\end{matrix}\\right]=\\left[\\begin{matrix}2&amp;1\\\\0&amp;3\\end{matrix}\\right]\\\\\\Rightarrow A=\\left[\\begin{matrix}1&amp;0\\\\-4&amp;1\\end{matrix}\\right]^{-1}\\left[\\begin{matrix}2&amp;1\\\\0&amp;3\\end{matrix}\\right]=\\left[\\begin{matrix}1&amp;0\\\\4&amp;1\\end{matrix}\\right]\\left[\\begin{matrix}2&amp;1\\\\0&amp;3\\end{matrix}\\right]\\]2）直接法  由于\\[A=\\left[\\begin{matrix}a_{11}&amp;a_{12}\\\\a_{21}&amp;a_{22}\\end{matrix}\\right]=\\left[\\begin{matrix}1&amp;0\\\\l_{21}&amp;1\\end{matrix}\\right]\\left[\\begin{matrix}u_{11}&amp;u_{12}\\\\0&amp;u_{22}\\end{matrix}\\right]\\\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  \\ \\ \\ \\  \\ \\ =\\left[\\begin{matrix}u_{11}&amp;u_{12}\\\\l_{21}u_{11}&amp;l_{21}u_{12}+u_{22}\\end{matrix}\\right] \\tag{4}\\]  直接根据上式就可以求$l_{ij}$和$u_{ij}$向量空间与子空间  设$R^n$是n维实向量构成的集合，如果该集合对于向量加法、数乘封闭，那么称$R^n$为向量空间  子空间是向量空间的一部分，这个集合也具有加法、数乘封闭性显然，只包含零向量的集合也构成向量空间零空间  零空间是$AX=0$的解的集合显然零空间是向量空间令\\[A=\\left[\\begin{matrix}1&amp;2&amp;2&amp;2\\\\2&amp;4&amp;6&amp;8\\\\3&amp;6&amp;8&amp;10\\end{matrix}\\right]\\]求解其零空间$N(A)$：STEP1: 初等行变换为上三角形式\\[A=\\left[\\begin{matrix}1&amp;2&amp;2&amp;2\\\\2&amp;4&amp;6&amp;8\\\\3&amp;6&amp;8&amp;10\\end{matrix}\\right]\\to \\left[\\begin{matrix}1&amp;2&amp;2&amp;2\\\\0&amp;0&amp;2&amp;4\\\\0&amp;0&amp;0&amp;0\\end{matrix}\\right]  \\tag{5}\\]SETP2：求特解从式(5)可以看到，$A$的主元（秩）有2个：$a_{11}=1$，$a_{23}=2$主列是$col_1$、$col_3$自由列是$col_2$、$col_4$对自由列对应的解指定0或1，带入(5)式，可以求得2组（与自由列数量一致）特解：$[2\\ 0\\ -2\\ 1]^T$, $[-2\\ 1\\ 0\\ 0]^T$STEP3：线性组合对上述特解做线性组合就得到了$A$的零空间\\[N(A)=c_1\\left[\\begin{matrix}2\\\\0\\\\-2\\\\1\\end{matrix}\\right]+c_2\\left[\\begin{matrix}-2\\\\1\\\\0\\\\0\\end{matrix}\\right] \\tag{6}\\]列空间对\\[A=\\left[\\begin{matrix}1&amp;1&amp;2\\\\2&amp;1&amp;3\\\\3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]\\]其所有列向量$[1\\ 2\\ 3\\ 4]^T$、$[1\\ 1\\ 1\\ 1]^T$和$[2\\ 3\\ 4\\ 5]^T$张成的空间称为列空间，记为$C(A)$1）显然，通过线性扩展的列空间是向量空间2）上述C(A)是$R^4$的2维子空间（$[1\\ 2\\ 3\\ 4]^T+[1\\ 1\\ 1\\ 1]^T$与$[2\\ 3\\ 4\\ 5]^T$线性相关）  列空间研究的是，什么样的b使得$AX=b$有解上述例子可以改写为\\[b=AX=\\left[\\begin{matrix}1&amp;1&amp;2\\\\2&amp;1&amp;3\\\\3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]\\left[\\begin{matrix}x_1\\\\x_2\\\\x_3\\end{matrix}\\right]=x_1\\left[\\begin{matrix}1\\\\2\\\\3\\\\4\\end{matrix}\\right]+x_2\\left[\\begin{matrix}1\\\\1\\\\1\\\\1\\end{matrix}\\right]+x_3\\left[\\begin{matrix}2\\\\3\\\\4\\\\5\\end{matrix}\\right] \\tag{7}\\]也就是说，如果$b$在$A$的列空间$C(A)$内，$AX=b$有解仍以零空间中的矩阵$A$为例，求$AX=b$的所有解集合STEP1：增广矩阵初等行变换为上三角形式\\[[A|b]=\\left[\\begin{array}{lccc|c}1&amp;2&amp;2&amp;2&amp;b_1\\\\2&amp;4&amp;6&amp;8&amp;b_2\\\\3&amp;6&amp;8&amp;10&amp;b_3\\end{array}\\right]\\Rightarrow \\left[\\begin{array}{lccc|c}1&amp;2&amp;2&amp;2&amp;b_1\\\\0&amp;0&amp;2&amp;4&amp;b_2-2b_1\\\\0&amp;0&amp;0&amp;0&amp;b_3-b_2-b_1\\end{array}\\right] \\tag{8}\\]注意上式中$b_i$为行向量STEP2：令所有自由列对应的解元素为0，求特解易得\\[X_p=\\left[\\begin{matrix}b_2-b_3\\\\0\\\\\\frac{b_3-b_2-b_1}{2}\\\\0\\end{matrix}\\right]\\]STEP3：求A的零空间$N(A)$（使得$AX=0$成立的所有解空间）如上述零空间内步骤，得\\[N(A)=c_1\\left[\\begin{matrix}2\\\\0\\\\-2\\\\1\\end{matrix}\\right]+c_2\\left[\\begin{matrix}-2\\\\1\\\\0\\\\0\\end{matrix}\\right]\\]STEP4：求所有解集合将特解$X_p$与零空间$N(A)$相加即得：\\[X=\\left[\\begin{matrix}b_2-b_3\\\\0\\\\\\frac{b_3-b_2-b_1}{2}\\\\0\\end{matrix}\\right]+c_1\\left[\\begin{matrix}2\\\\0\\\\-2\\\\1\\end{matrix}\\right]+c_2\\left[\\begin{matrix}-2\\\\1\\\\0\\\\0\\end{matrix}\\right] \\tag{9}\\]需要注意的是$AX=b$的所有解集合不一定是一个向量空间了（不一定过0点）秩如零空间内所述，秩$rank(A)$指A的主元个数  对尺寸$m\\times n$的矩阵A，秩$r\\leq m$ &amp;&amp; $r\\leq n$1）$r=m=n$此时称A为满秩      A是方阵，并且可逆（非奇异）        $AX=0$仅包含0向量（自由列数量为0）        $AX=b$有且只有1个解  2）$r=n&lt;m$此时称A为列满秩      $AX=0$仅包含0向量（自由列数量为0）        $AX=b$有可能存在1个解，也可能无解（方程数多于未知数）  3）$r=m&lt;n$此时称A为行满秩      $AX=0$的特解个数为$n-r$        $AX=b$有无穷多解（方程数少于未知数）  4）$r&lt;m$&amp;&amp;$r&lt;n$此时$AX=b$有可能存在无穷多解，也可能无解这里无解的情况是\\[A\\Rightarrow\\left[\\begin{matrix}I&amp;F\\\\0&amp;0\\end{matrix}\\right]\\]而对应的\\[b\\Rightarrow\\left[\\begin{matrix}b_{up}\\\\b_{down}\\end{matrix}\\right]\\]中$b_{down}\\neq0$否则就特化为情况3，拥有无穷多解"
  },
  
  {
    "title": "KITTI双目数据集使用",
    "url": "/posts/KITTI%E5%8F%8C%E7%9B%AE%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8/",
    "categories": "自动驾驶",
    "tags": "",
    "date": "2018-05-28 00:00:00 +0000",
    





    
    "snippet": "KITTI是面向自动驾驶的标准测试数据集，这里关注其中双目数据如图1所示，KITTI使用4个相机采集图像数据，两个为灰度相机，另外两个为彩色相机。图1. 双目系统包含两个灰度相机和两个彩色相机数据格式KITTI目前有2012和2015两个双目数据集stereo2015里可以下载立体校正后的图像对和标定文件；raw data里可以下载原始未校正的图像对和标定文件一个典型的标定文件 calib_...",
    "content": "KITTI是面向自动驾驶的标准测试数据集，这里关注其中双目数据如图1所示，KITTI使用4个相机采集图像数据，两个为灰度相机，另外两个为彩色相机。图1. 双目系统包含两个灰度相机和两个彩色相机数据格式KITTI目前有2012和2015两个双目数据集stereo2015里可以下载立体校正后的图像对和标定文件；raw data里可以下载原始未校正的图像对和标定文件一个典型的标定文件 calib_cam_to_cam.txt 如下：calib_time: 09-Jan-2012 13:57:47corner_dist: 9.950000e-02S_00: 1.392000e+03 5.120000e+02K_00: 9.842439e+02 0.000000e+00 6.900000e+02 0.000000e+00 9.808141e+02 2.331966e+02 0.000000e+00 0.000000e+00 1.000000e+00D_00: -3.728755e-01 2.037299e-01 2.219027e-03 1.383707e-03 -7.233722e-02R_00: 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00T_00: 2.573699e-16 -1.059758e-16 1.614870e-16S_rect_00: 1.242000e+03 3.750000e+02R_rect_00: 9.999239e-01 9.837760e-03 -7.445048e-03 -9.869795e-03 9.999421e-01 -4.278459e-03 7.402527e-03 4.351614e-03 9.999631e-01P_rect_00: 7.215377e+02 0.000000e+00 6.095593e+02 0.000000e+00 0.000000e+00 7.215377e+02 1.728540e+02 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00S_01: 1.392000e+03 5.120000e+02K_01: 9.895267e+02 0.000000e+00 7.020000e+02 0.000000e+00 9.878386e+02 2.455590e+02 0.000000e+00 0.000000e+00 1.000000e+00D_01: -3.644661e-01 1.790019e-01 1.148107e-03 -6.298563e-04 -5.314062e-02R_01: 9.993513e-01 1.860866e-02 -3.083487e-02 -1.887662e-02 9.997863e-01 -8.421873e-03 3.067156e-02 8.998467e-03 9.994890e-01T_01: -5.370000e-01 4.822061e-03 -1.252488e-02S_rect_01: 1.242000e+03 3.750000e+02R_rect_01: 9.996878e-01 -8.976826e-03 2.331651e-02 8.876121e-03 9.999508e-01 4.418952e-03 -2.335503e-02 -4.210612e-03 9.997184e-01P_rect_01: 7.215377e+02 0.000000e+00 6.095593e+02 -3.875744e+02 0.000000e+00 7.215377e+02 1.728540e+02 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00S_02: 1.392000e+03 5.120000e+02K_02: 9.597910e+02 0.000000e+00 6.960217e+02 0.000000e+00 9.569251e+02 2.241806e+02 0.000000e+00 0.000000e+00 1.000000e+00D_02: -3.691481e-01 1.968681e-01 1.353473e-03 5.677587e-04 -6.770705e-02R_02: 9.999758e-01 -5.267463e-03 -4.552439e-03 5.251945e-03 9.999804e-01 -3.413835e-03 4.570332e-03 3.389843e-03 9.999838e-01T_02: 5.956621e-02 2.900141e-04 2.577209e-03S_rect_02: 1.242000e+03 3.750000e+02R_rect_02: 9.998817e-01 1.511453e-02 -2.841595e-03 -1.511724e-02 9.998853e-01 -9.338510e-04 2.827154e-03 9.766976e-04 9.999955e-01P_rect_02: 7.215377e+02 0.000000e+00 6.095593e+02 4.485728e+01 0.000000e+00 7.215377e+02 1.728540e+02 2.163791e-01 0.000000e+00 0.000000e+00 1.000000e+00 2.745884e-03S_03: 1.392000e+03 5.120000e+02K_03: 9.037596e+02 0.000000e+00 6.957519e+02 0.000000e+00 9.019653e+02 2.242509e+02 0.000000e+00 0.000000e+00 1.000000e+00D_03: -3.639558e-01 1.788651e-01 6.029694e-04 -3.922424e-04 -5.382460e-02R_03: 9.995599e-01 1.699522e-02 -2.431313e-02 -1.704422e-02 9.998531e-01 -1.809756e-03 2.427880e-02 2.223358e-03 9.997028e-01T_03: -4.731050e-01 5.551470e-03 -5.250882e-03S_rect_03: 1.242000e+03 3.750000e+02R_rect_03: 9.998321e-01 -7.193136e-03 1.685599e-02 7.232804e-03 9.999712e-01 -2.293585e-03 -1.683901e-02 2.415116e-03 9.998553e-01P_rect_03: 7.215377e+02 0.000000e+00 6.095593e+02 -3.395242e+02 0.000000e+00 7.215377e+02 1.728540e+02 2.199936e+00 0.000000e+00 0.000000e+00 1.000000e+00 2.729905e-03根据文章《Vision meets robotics: The KITTI dataset》【1】，各参数解释如下图2. 各参数意义其中$i=0,1$时为左右灰度相机图像，$i=2,3$时为左右彩色相机图像。原始双目数据的校正根据图2的信息，有两种方式对原始数据做立体校正（基于OpenCV）直接使用R、P数据R、P为校正后的旋转矩阵和映射矩阵，结合相机内参和畸变参数可以直接使用OpenCV的函数生成映射mapvoid initUndistortRectifyMap( InputArray cameraMatrix, InputArray distCoeffs,                           InputArray R, InputArray newCameraMatrix,                           Size size, int m1type, OutputArray map1, OutputArray map2 );需要说明的是，KITTI的畸变向量$K^i$和OpenCV的组织方式一样，为$[k1,k2,p1,p2,k3]$使用相机内参、畸变参数、旋转平移矩阵先生成R、P数据使用OpenCV的函数void stereoRectify( InputArray cameraMatrix1, InputArray distCoeffs1,                               InputArray cameraMatrix2, InputArray distCoeffs2,                               Size imageSize, InputArray R, InputArray T,                               OutputArray R1, OutputArray R2,                               OutputArray P1, OutputArray P2,                               OutputArray Q, int flags=CALIB_ZERO_DISPARITY,                               double alpha=-1, Size newImageSize=Size(),                               CV_OUT Rect* validPixROI1=0, CV_OUT Rect* validPixROI2=0 );这里需要注意的是，KITTI的$R^i$和$t^i$是0号相机到$i$号相机的旋转、平移矩阵，而实际我们需要的是0号和1号相机的旋转、平移；2号和3号相机的旋转、平移因此对于彩色相机（2、3），需要对其做如下变换：\\[\\begin{cases}R=R_3R_2^{-1} \\\\ T=t_3-Rt_2\\end{cases}\\]%% camera 0 与 camera 1 外参%YAML:1.0---K1: !!opencv-matrix   rows: 3   cols: 3   dt: d   data: [ 9.8424390000000005e+02, 0., 690., 0., 9.8081410000000005e+02,       2.3319659999999999e+02, 0., 0., 1. ]D1: !!opencv-matrix   rows: 1   cols: 5   dt: d   data: [ -3.7287550000000003e-01, 2.0372989999999999e-01,       2.2190270000000002e-03, 1.3837070000000001e-03,       -7.2337219999999994e-02 ]K2: !!opencv-matrix   rows: 3   cols: 3   dt: d   data: [ 9.8952670000000001e+02, 0., 702., 0., 9.8783860000000004e+02,       2.4555900000000000e+02, 0., 0., 1. ]D2: !!opencv-matrix   rows: 1   cols: 5   dt: d   data: [ -3.6446610000000002e-01, 1.7900189999999999e-01,       1.1481070000000000e-03, -6.2985630000000003e-04,       -5.3140620000000000e-02 ]R: !!opencv-matrix   rows: 3   cols: 3   dt: d   data: [ 9.9935130000000005e-01, 1.8608659999999999e-02,       -3.0834870000000000e-02, -1.8876620000000000e-02,       9.9978630000000002e-01, -8.4218729999999999e-03,       3.0671560000000000e-02, 8.9984669999999996e-03,       9.9948899999999996e-01 ]T: !!opencv-matrix   rows: 3   cols: 1   dt: d   data: [ -5.3700000000000025e-01, 4.8220610000001118e-03,       -1.2524880000000169e-02 ]ImageSize: [ 1392, 512 ]%% camera 2 与 camera 3 外参%YAML:1.0---K1: !!opencv-matrix   rows: 3   cols: 3   dt: d   data: [ 9.5979100000000005e+02, 0., 6.9602170000000001e+02, 0.,       9.5692510000000004e+02, 2.2418060000000000e+02, 0., 0., 1. ]D1: !!opencv-matrix   rows: 1   cols: 5   dt: d   data: [ -3.6914809999999998e-01, 1.9686809999999999e-01,       1.3534730000000000e-03, 5.6775870000000004e-04,       -6.7707050000000005e-02 ]K2: !!opencv-matrix   rows: 3   cols: 3   dt: d   data: [ 9.0375959999999998e+02, 0., 6.9575189999999998e+02, 0.,       9.0196529999999996e+02, 2.2425090000000000e+02, 0., 0., 1. ]D2: !!opencv-matrix   rows: 1   cols: 5   dt: d   data: [ -3.6395580000000000e-01, 1.7886510000000000e-01,       6.0296939999999999e-04, -3.9224239999999998e-04,       -5.3824600000000000e-02 ]R: !!opencv-matrix   rows: 3   cols: 3   dt: d   data: [ 9.9955680159618387e-01, 2.2327521266637482e-02,       -1.9686805086377379e-02, -2.2302255763561471e-02,       9.9975012819823550e-01, 1.5017206478461062e-03,       1.9715413297725075e-02, -1.0619950079608239e-03,       9.9980512458368798e-01 ]T: !!opencv-matrix   rows: 3   cols: 1   dt: d   data: [ -5.3260054863554218e-01, 6.5861189686626026e-03,       -9.0016532205258477e-03 ]ImageSize: [ 1392, 512 ]需要注意的是，官方提供的平移矩阵$T$单位为m，因此三维重建后$XYZ$的单位已经是m参考文献【1】Geiger A, Lenz P, Stiller C, et al. Vision meets robotics: The KITTI dataset[J]. International Journal of Robotics Research, 2013, 32(11):1231-1237."
  },
  
  {
    "title": "Gabor滤波器",
    "url": "/posts/Gabor%E6%BB%A4%E6%B3%A2%E5%99%A8/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-11-06 00:00:00 +0000",
    





    
    "snippet": "Gabor是一个用于边缘提取的线性滤波器，其频率和方向表达与人类视觉系统类似，能够提供良好的方向选择和尺度选择特性，而且对于光照变化不敏感，因此十分适合纹理分析。图1是Gabor滤波器和脊椎动物视觉皮层感受野响应的比较　　　　　　　　　　　　　　　　　　　　图1. Gabor与脊椎动物视觉皮层感受野响应的比较图1中第一行是脊椎动物的视觉响应，第二行是Gabor滤波器的响应，可以看到，二者相差...",
    "content": "Gabor是一个用于边缘提取的线性滤波器，其频率和方向表达与人类视觉系统类似，能够提供良好的方向选择和尺度选择特性，而且对于光照变化不敏感，因此十分适合纹理分析。图1是Gabor滤波器和脊椎动物视觉皮层感受野响应的比较　　　　　　　　　　　　　　　　　　　　图1. Gabor与脊椎动物视觉皮层感受野响应的比较图1中第一行是脊椎动物的视觉响应，第二行是Gabor滤波器的响应，可以看到，二者相差极小。基于以上特性，Gabor滤波器被广泛应用于人脸识别的预处理。Gabor理论及公式我们知道，傅里叶变换可以将信号从时域转换到频域，但无法获得频谱中不同频率之间的先后关系。然而实际应用中我们更多的关心信号局部范围内的的特性，Gabor和小波变换突破了傅里叶变换的局限性。Gabor变换是D.Gabor于1946年提出的，为了提取傅里叶变换的局部信息，引入了时间局部化的窗函数（把信号划分成许多小的时间间隔，用傅里叶变换分析每一个间隔）。因此Gabor变换又称为窗口傅里叶变换（短时傅里叶变换）。二维Gabor滤波器在空间域，一个二维的Gabor滤波器是一个正弦平面波和高斯核函数的乘积。前者是调谐函数，后者是窗口函数。\\[g(x,y;\\lambda,\\theta,\\psi,\\sigma,\\gamma)=e^{-\\frac{x^{'^2}+\\gamma^2y^{'^2}}{2\\sigma^2}}e^{i(2\\pi\\frac{x^{'}}{\\lambda}+\\psi)}\\]可以分为实部与虚部的形式\\[\\begin{cases}g_{real}(x,y;\\lambda,\\theta,\\psi,\\sigma,\\gamma)=e^{-\\frac{x^{'^2}+\\gamma^2y^{'^2}}{2\\sigma^2}}cos(2\\pi\\frac{x^{'}}{\\lambda}+\\psi)\\\\g_{imag}(x,y;\\lambda,\\theta,\\psi,\\sigma,\\gamma)=e^{-\\frac{x^{'^2}+\\gamma^2y^{'^2}}{2\\sigma^2}}sin(2\\pi\\frac{x^{'}}{\\lambda}+\\psi)\\end{cases}\\]其中\\[\\begin{cases}x^{'}=xcos\\theta+ysin\\theta\\\\y^{'}=-xsin\\theta+ycos\\theta\\end{cases}\\]　　　　　　　　　　　　表1. 二维Gabor滤波器参数解释            参数      物理意义      描述                  $\\lambda$      波长      直接影响滤波器的滤波尺度，通常大于等于2              $\\theta$      方向      滤波器的方向              $\\psi$      相位偏移      调谐函数的相位偏移，取值-180到180              $\\gamma$      空间纵横比      决定滤波器的形状，取1时为圆形，通常取0.5              $\\sigma$      带宽      高斯滤波器的方差，通常取2$\\pi$      实验本节参考自【图像处理】Gabor滤波器、Gabor滤波器学习、Gabor的OpenCV代码　　　　　　　　　　　　　　　　　　　　　　　　　　　　　图2. 取不同参数的Gabor滤波器图2对比了不同参数的Gabor滤波器，从第1到第5行$\\lambda$取3,6,9,12,15，从第1到第8列$\\theta$取0,$\\frac{\\pi}{8}$,$\\frac{\\pi}{4}$,$\\frac{3\\pi}{8}$,$\\frac{\\pi}{2}$,$\\frac{5\\pi}{8}$,$\\frac{3\\pi}{4}$,$\\frac{7\\pi}{8}$，其他参数保持不变：$\\psi=0, \\sigma=2\\pi, \\gamma=0.5$。图4使用图2的滤波器对图3中两幅人脸做卷积　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　图3. FERET中同一人在不同光照不同角度的人脸　　　　　　　　　　　　　　　　　　　　　　　　　　图4.1. 使用图2滤波器对图3左卷积结果　　　　　　　　　　　　　　　　　　　　　　　　　　图4.2. 使用图2滤波器对图3右卷积结果可以看到，不同方向不同尺度的Gabor滤波器可以提取人脸中不同特征此外还可以看到，Gabor滤波结果对不同光照也能保持较稳定结果代码// gaborfilter.cpp#include \"gaborfilter.h\"int CGabor::getFilterSize(float theta, float sigma, float gamma){    float sigma_x = sigma*sigma;    float sigma_y = sigma_x/(gamma*gamma);    float sqrt_sigma_y = sqrt(sigma_y);    float c_theta = cos(theta);    float s_theta = sin(theta);    // calculate filter size (3sigma)    int nstds = 3;    int xmax = max(abs(nstds*sigma*c_theta), abs(nstds*sqrt_sigma_y*s_theta));    int ymax = max(abs(nstds*sigma*s_theta), abs(nstds*sqrt_sigma_y*c_theta));    int half_filter_size = xmax&gt;ymax?xmax:ymax;    int filter_size = 2*half_filter_size+1;    return filter_size;}void CGabor::getGaborFilter(float lambda, float theta, float fhi, float sigma, float gamma,     Mat&amp; realGabor, Mat&amp; imagGabor, Mat&amp; mag){    if(abs(lambda-0.0f) &lt; 1e-6)        lambda = 1.0f;    float sigma_x = sigma*sigma;    float sigma_y = sigma_x/(gamma*gamma);    float sqrt_sigma_y = sqrt(sigma_y);    float c_theta = cos(theta);    float s_theta = sin(theta);    int filter_size = getFilterSize(theta, sigma, gamma);    int half_filter_size = (filter_size-1)/2;    realGabor = Mat(filter_size, filter_size, CV_32F);    imagGabor = Mat(filter_size, filter_size, CV_32F);    mag       = Mat(filter_size, filter_size, CV_32F);    for(int i=0; i&lt;filter_size; i++)    {        float* p_real = realGabor.ptr&lt;float&gt;(i);        float* p_imag = imagGabor.ptr&lt;float&gt;(i);        float* p_mag  = mag.ptr&lt;float&gt;(i);        int y = i - half_filter_size;        for(int j=0; j&lt;filter_size; j++)        {            int x = j - half_filter_size;            float x_theta = x*c_theta + y*s_theta;            float y_theta = y*c_theta - x*s_theta;            float value_ = exp(-0.5*(x_theta*x_theta/sigma_x + y_theta*y_theta/sigma_y));            p_real[j] = value_ * cos(2*CV_PI*x_theta/lambda + fhi);            p_imag[j] = value_ * sin(2*CV_PI*x_theta/lambda + fhi);            p_mag[j]  = sqrt(p_real[j]*p_real[j] + p_imag[j]*p_imag[j]);        }    }}Mat CGabor::crossGaborFilter(Mat gaborFilter, Mat image){    CV_Assert(!gaborFilter.empty() &amp;&amp; !image.empty());    int half_filter_size = (max(gaborFilter.rows, gaborFilter.cols)-1)/2;    Mat filtered_img(image.rows, image.cols, CV_32F);    for(int i=0; i&lt;image.rows; i++)    {        float* p_fil = filtered_img.ptr&lt;float&gt;(i);        for(int j=0; j&lt;image.cols; j++)        {            float sum_value = 0.0f;            for(int m=0; m&lt;gaborFilter.rows; m++)            {                float* p_gab = gaborFilter.ptr&lt;float&gt;(m);                                int img_i = i + m - half_filter_size;                img_i = img_i&lt;0 ? 0 : img_i;                img_i = img_i&gt;=image.rows ? (image.rows-1) : img_i;                uchar* p_img = image.ptr&lt;uchar&gt;(img_i);                for(int n=0; n&lt;gaborFilter.cols; n++)                {                    int img_j = j + n - half_filter_size;                    img_j = img_j&lt;0 ? 0 : img_j;                    img_j = img_j&gt;=image.cols ? (image.cols-1) : img_j;                    sum_value += ((float)p_img[img_j] * p_gab[n]);                }            }            p_fil[j] = sum_value;        }    }    return filtered_img;}Mat CGabor::normalizer(Mat src){    CV_Assert(!src.empty());    float min_ = FLT_MAX;    float max_ = FLT_MIN;    for(int i=0; i&lt;src.rows; i++)    {        float* p_src = src.ptr&lt;float&gt;(i);        for(int j=0; j&lt;src.cols; j++)        {            if(p_src[j] &gt; max_)                max_ = p_src[j];            if(p_src[j] &lt; min_)                min_ = p_src[j];        }    }    Mat src_show(src.size(), CV_8UC1);    float scale = max_ - min_;    for(int i=0; i&lt;src.rows; i++)    {        float* p_src = src.ptr&lt;float&gt;(i);        uchar* p_src_show = src_show.ptr&lt;uchar&gt;(i);        for(int j=0; j&lt;src.cols; j++)        {            if(scale &gt; 0.01)                p_src_show[j] = (uchar)((p_src[j]-min_)*255/scale);            else                p_src_show[j] = 255;        }    }    return src_show;}\\\\ main.cpp#include \"stdafx.h\"#include \"gaborfilter.h\"void main(){    Mat img[7];    string path = \"C:\\\\Users\\\\sq\\\\Desktop\\\\humanface\\\\FaceDataSet\\\\FERET_80_80\\\\FERET-001\\\\\";    for(int i=0; i&lt;7; i++)    {        char num[7];        sprintf(num, \"0%d.tif\", i+1);        string wholepath = num;        wholepath = path + wholepath;        Mat tmp = imread(wholepath, 0);        resize(tmp, img[i], Size(200, 200)); // 为了确保滤波器尺寸小于图片大小    }    // calc gabor    CGabor m_gabor;    float lambda = 5;    float theta = 0;    float fhi = 0;    float sigma = 2*CV_PI;    float gamma = 0.5;    Mat realGabor[40], imagGabor, magGabor;        int size_ = INT_MAX;    for(int i=0; i&lt;8; i++)    {        theta = i*CV_PI/8;        int _size_ = m_gabor.getFilterSize(theta, sigma, gamma);        if(_size_ &lt; size_)            size_ = _size_;    }    Mat showit((size_+3)*5-3, (size_+3)*8-3, CV_8UC1, Scalar::all(255));    for(int i=0; i&lt;8; i++)    {        theta = i*CV_PI/8;        for(int j=0; j&lt;5; j++)        {            lambda = (j+1)*3;            m_gabor.getGaborFilter(lambda, theta, fhi, sigma, gamma, realGabor[j*8+i], imagGabor, magGabor);            Mat saveimg = m_gabor.normalizer(realGabor[j*8+i]);            Mat saveimg_roi(saveimg, Rect(saveimg.rows/2-size_/2, saveimg.rows/2-size_/2, size_, size_));            int start_row = j*(size_+3);            int start_col = i*(size_+3);            for(int m=0; m&lt;size_; m++)            {                uchar* p_img = saveimg_roi.ptr&lt;uchar&gt;(m);                uchar* p_show = showit.ptr&lt;uchar&gt;(start_row + m);                for(int n=0; n&lt;size_; n++)                   p_show[start_col + n] = p_img[n];            }        }    }        imshow(\"gabor\", showit);    imwrite(\"gabor.jpg\", showit);    waitKey(1);    // filter face    for(int i=0; i&lt;7; i++)    {        Mat face((img[0].rows+3)*5-3, (img[0].cols+3)*8-3, CV_8UC1, Scalar::all(255));        for(int m=0; m&lt;5; m++)        {            int start_row = m*(img[0].rows+3);            for(int n=0; n&lt;8; n++)            {                Mat filted = m_gabor.crossGaborFilter(realGabor[m*8+n], img[i]);                Mat filted_norm = m_gabor.normalizer(filted);                int start_col = n*(img[0].cols+3);                for(int ro=0; ro&lt;img[0].rows; ro++)                {                    uchar* p_filted = filted_norm.ptr&lt;uchar&gt;(ro);                    uchar* p_face = face.ptr&lt;uchar&gt;(start_row + ro);                    for(int co=0; co&lt;img[0].cols; co++)                        p_face[start_col + co] = p_filted[co];                }            }        }        char num[7];        sprintf(num, \"_%d.jpg\", i);        string filename = num;        filename = \"face\"+filename;        imwrite(filename, face);        imshow(filename, face);        waitKey(1);    }    waitKey(0);}"
  },
  
  {
    "title": "图像分割技术（2）",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%8A%80%E6%9C%AF-2/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-03-24 00:00:00 +0000",
    





    
    "snippet": "全局阈值分割的改进方法通常待处理的图片会有噪声和光照的影响，这些因素会对阈值分割造成很大的干扰对光照不均可以考虑直方图均衡化，或者图像形态学的顶帽变换。后者效果更好。对噪声干扰一般来说先做个低通滤波会有很好的改善；但对于前景物很小的情况，其在直方图上完全被噪声淹没，做低通滤波甚至可能直接把前景抹去。这时可考虑以下方法：      对图A做高通滤波（梯度，拉普拉斯），得B        选取一...",
    "content": "全局阈值分割的改进方法通常待处理的图片会有噪声和光照的影响，这些因素会对阈值分割造成很大的干扰对光照不均可以考虑直方图均衡化，或者图像形态学的顶帽变换。后者效果更好。对噪声干扰一般来说先做个低通滤波会有很好的改善；但对于前景物很小的情况，其在直方图上完全被噪声淹没，做低通滤波甚至可能直接把前景抹去。这时可考虑以下方法：      对图A做高通滤波（梯度，拉普拉斯），得B        选取一个阈值T对B做阈值化，目的是筛选出强边缘，得C        仅对A中在C位置大于0的点统计直方图，做阈值分割  局部阈值分割局部阈值意味着阈值可变，其主要针对受噪声、不均匀光照影响的图像1）一种很容易想到的方法就是：对图像分块，在每块内做全局阈值分割。但子块的数目和划分方法不好确定。2）另一种是考虑每个像素的邻域，通过计算其邻域的均值和方差决定一个阈值（一个像素一个阈值），OpenCV的adaptiveThreshold()就是这个思路：3）移动平均算法这是一种很有意思的方法，主要针对前景物细小的情况，如文档、文本图像（图2-1）图2-1. 光照不均（左），正弦亮度污染（右，通常从电视屏幕拍摄会出现）算法逐行扫描每个像素，令$z_{k+1}$为步骤$k+1$中点的灰度值，该点的移动平均值为：\\[m(k+1)=m(k)+\\frac{1}{n}(z_{k+1}-z_{k-n})\\]对初始的n个点，直接取$m(k)=z(k)/n$，图2-2是对图2-1左应用移动平均的结果（代码参考基于移动平均的图像分割）图2-2. 移动平均阈值化此外，对图2-1右的情况，由于是周期噪声的污染，还可以考虑频域滤波来解决基于区域的分割基于边缘的分割主要依据像素灰度的不连续性，试图找到区域边界；基于阈值的分割主要依据像素灰度的分布，对其进行聚类；基于区域的方法直接查找区域，通过设置种子点，实现区域的可控性。基于区域的方法主要可以分为两类：区域生长、区域分类与聚合区域生长基本思路是：从种子点出发，轮询其4/8邻域，如果像素与种子点属性一致则扩展对于种子点，可以对原始图先以最大亮度的98%（假设）为阈值做二值化，然后腐蚀（去除噪声点），必要时还可做点形态学细化来得到//根据img对seed做区域生长//flag=4： 4连通//flag=8:  8连通int *nDx = NULL, *nDy = NULL;if (flag == 4){\tnDx = new int[4];\tnDy = new int[4];\tnDx[0] = -1, nDx[1] = 0, nDx[2] = 1, nDx[3] = 0;\tnDy[0] = 0, nDy[1] = -1, nDy[2] = 0, nDy[3] = 1;}else{\tflag = 8;\tnDx = new int[8];\tnDy = new int[8];\tnDx[0] = -1, nDx[1] = -1, nDx[2] = 0, nDx[3] = 1;\tnDx[4] = 1, nDx[5] = 1, nDx[6] = 0, nDx[7] = -1;\tnDy[0] = 0, nDy[1] = -1, nDy[2] = -1, nDy[3] = -1;\tnDy[4] = 0, nDy[5] = 1, nDy[6] = 1, nDy[7] = 1;}out = Mat(img.size(), img.type(), Scalar::all(0));bool *handed = (bool*)calloc(img.rows*img.cols, sizeof(bool));Point current(0, 0);for (int i = 0; i &lt; seed.rows; i++){\tuchar* ptrSeed = seed.ptr&lt;uchar&gt;(i);\tfor (int j = 0; j &lt; seed.cols; j++)\t{\t\tif (int(ptrSeed[j] == 0))\t\t\tcontinue;\t\tout.at&lt;uchar&gt;(i, j) = 255;\t\thanded[i*img.cols + j] = true;\t\tPoint* queue_ = new Point[seed.rows*seed.cols];\t\tint start = 0, end = 0;\t\tqueue_[end] = Point(j, i);\t\twhile (start &lt;= end)\t\t{\t\t\tcurrent = queue_[start];\t\t\tfor (int m = 0; m &lt; flag; m++)\t\t\t{\t\t\t\tPoint nowP(current.x + nDx[m], current.y + nDy[m]);\t\t\t\tif (nowP.x &lt; 0 || nowP.x &gt;= img.cols)\t\t\t\t\tcontinue;\t\t\t\tif (nowP.y &lt;0 || nowP.y &gt;= img.rows)\t\t\t\t\tcontinue;\t\t\t\tif (handed[nowP.y*img.cols+nowP.x] == true)\t\t\t\t\tcontinue;\t\t\t\t\t\t\t\t\tint currValue = int(img.at&lt;uchar&gt;(i, j));\t\t\t\tint nowValue = int(img.at&lt;uchar&gt;(nowP)); //应该跟原始种子点比\t\t\t\tif (abs(currValue - nowValue) &gt;= threshold)\t\t\t\t\tcontinue;\t\t\t\tqueue_[++end] = nowP;\t\t\t\tout.at&lt;uchar&gt;(nowP) = 255;\t\t\t\thanded[nowP.y*img.cols + nowP.x] = true;\t\t\t}\t\t\tstart++;\t\t}\t\tdelete[] queue_;\t}}delete[] nDx;delete[] nDy;free(handed);代码参考的基于区域的图像分割———–区域生长一文，原文中阈值对比用的是当前邻域和当前种子点，我认为应该是与原始种子点比较，不然很容易出现过度生长的情况图3-1. 区域生长（左：原始，中：腐蚀后的种子图，右：生长结果）区域分裂与聚合一个流行的方法是四叉分裂树：对原图不断做四分裂，当某区域内像素都满足同一属性时该区域停止分裂，或者当该区域尺寸达到规定最小尺寸时停止。图3-2. 四叉分裂树（《数字图像处理》）分裂完成后考察邻接区域，如果两区域属性一致则合并（这一步可在分裂时做）图3-3是分裂合并的效果，代码参考基于区域的图像分割———–区域分裂合并，如果区域尺寸小于等于1则置0，这里因为满足要求的区域置255（也就是说只有一种区域），因此隐含了合并的效果图3-3. 区域分裂合并图3-3的目标是分割不太致密的环状区域，因此属性标准包括了均值和方差基于分水岭的分割1）分水岭算法将像素灰度值视作海拔高度，不断地往里面注水，这样处在盆地（区域内部）的水面会不断上升，同时将每个区域最小值的地方打通，确保整个图像所有区域的水面能同步上升。当水面继续上升，会从一个盆地溢出到另一个盆地，这时不断的加高堤坝防止溢出（主要是因为每条边缘的高度不一致）直到水面达到最高（255），停止，这时没被淹没的像素就是原图的区域边缘。2）较前3种方法，分水岭算法能更稳定的分割区域，并且能确保边界是连续的。3）由于噪声和其他梯度的不规则性，分水岭方法很容易过度分割。解决这个问题的方法是引入标记图：一个标记是属于一幅图像的一个连通分量，与前景相联系的为内部标记，与背景相关联的为外部标记。算法通过标记来约束分割的数量4）分水岭算法有多种版本，经典分水岭算法的 C++ 实现是对Soille 和 Vincent 1991年提出的模拟浸没的分水岭算法的实现；OpenCV的watershed()是对Meyer的《Color image segmentation》的实现，输入限定为3通道（稍微改下源码里的c_diff等函数就可实现灰度图分割了），基本原理OpenCV学习(9) 分水岭算法(3)分析的很详细图4-1. 分水岭分割图4-1为应用上述两套算法得到的结果。a为原始图，b为二值化的结果，c为经典分水岭得到的分割结果，d为OpenCV的watershed（做了灰度兼容修改）中对区域做标记的结果，e为根据标记图得到的分割结果。可以看到由于少了标记图，经典分水岭算法出现了过分割的情况。5）基于经典的分水岭算法还有很多的改进方法。最大稳定极值区域（MSER）检测介绍的MSER算法改变了注水方式（固定一个位置注水且不互相打通）；Camille Couprie给出了几种新的分割算法及相应代码实现。"
  },
  
  {
    "title": "图像分割技术（1）",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%8A%80%E6%9C%AF-1/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-03-22 00:00:00 +0000",
    





    
    "snippet": "分割是将图像细分为子区域，这些子区域互不重叠，并集为初始图像，每个子区域内的像素分布符合预定规则。用数学描述可表示为：\\[\\bigcup_{i=1}^nR_i=R\\]其中  $R_i$是连通集  对$i\\ne j$，有$R_i\\cap R_j=\\emptyset$  $Q(R_i)=TRUE$  $Q(R_i\\cup R_j)=FALSE$一般有4种分割思路：基于点线、边缘的分割；基于阈值的...",
    "content": "分割是将图像细分为子区域，这些子区域互不重叠，并集为初始图像，每个子区域内的像素分布符合预定规则。用数学描述可表示为：\\[\\bigcup_{i=1}^nR_i=R\\]其中  $R_i$是连通集  对$i\\ne j$，有$R_i\\cap R_j=\\emptyset$  $Q(R_i)=TRUE$  $Q(R_i\\cup R_j)=FALSE$一般有4种分割思路：基于点线、边缘的分割；基于阈值的分割（二值化）；基于区域的分割；基于分水岭的分割基于点线、边缘的分割这种分割的目的是把图像中灰度变化剧烈的区域找出来，这也是大部分机器视觉应用的必要步骤。无论是点、线还是边缘的分割，都是基于图像微分（《图像局部不变性特征与描述》阅读笔记（3）– 点与边缘检测）由(LOG)Laplacian of Guassian &amp; (DOH)Determinant of Hessian 斑点检测对高斯一阶和二阶微分的讨论，可以知道：      一阶导数得到的边缘比二阶要粗        二阶导数对细线、噪声更敏感        二阶导数在灰度过渡处会产生双边缘，此外通过二阶导数的符号可以推断边缘的过渡是从暗到亮还是相反  基于以上讨论      孤立点的检测一般用二阶导数（laplace），如Harris角点检测，除此之外还有很多别的角点检测算法，如SUSAN，SIFT，SURF        线的检测主要针对空间上连续的线段，一般使用对方向敏感的模板对线做方向筛选  图1-1. 各向互异的Laplace线检测算子  边缘检测比点线检测要复杂，因为边缘一般是弯曲的，因此需要首先检测点或小线段，然后做拟合，其大致步骤是：1）平滑图像以过滤噪声2）使用微分获得边缘点3）基于一定规则对边缘点进行筛选并连接简单的方法一般有Roberts、Prewitt、Sobel等线检测算子，之后做点膨胀连接下断点也就可以了更有效的方法是Canny算法，效果很好  然后再补充下边缘连接的方法吧，虽然觉得canny已经足够了，而这些方法往往会增加很多计算量：1）局部处理方式：对每个边缘点A，考虑其邻域的点B，如果两个点无论梯度幅值还是梯度角度都在误差范围内，那么认为B在A代表的边缘上2）多边形拟合：找到一个多边形，尽可能的反映当前离散点的分布3）全局处理方式：比较有针对性的场合，比如需要找直线，找圆，用Hough基于阈值的分割常用的是单阈值分割，这样得到的图像就是二值的，当然也有多阈值分割以区分多种区域。而基于阈值的作用域又可分为全局阈值分割和局部阈值分割。阈值分割的主要参考是图像直方图，阈值选择在波谷以区分不同区域全局阈值分割主要介绍几种主流的全局阈值分割方法（单阈值，多阈值在这个基础上做二次迭代就好了）：最大熵、基本全局、迭代法、OTSU（以上4种思路及代码主要参考SkySeraph的文章《图像算法：图像阈值分割》，手动选择阈值的方法就不说了）最大熵阈值依次选择每个灰度级（0~255），将直方图分成两部分，分别计算两部分的熵再相加，取使得熵为最大值的那个灰度级为阈值。令Ai为直方图A部分第i级的值，At为A部分所有灰度级的和，那么A部分的熵为：\\[En(A)=\\sum_{i=start}^{end}-\\frac{A_i}{A_t}log(\\frac{A_i}{A_t})\\]// 计算当前位置的能量熵 double caculateCurrentEntropy(CvHistogram * Histogram1,int cur_threshold,int state) {\tint start,end;\tint total =0;\tdouble cur_entropy =0.0;\tif(state == 1)           // 1是计算当前点之前的能量熵\t{\t\tstart =0;\t\tend = cur_threshold; \t}\telse \t{\t\tstart = cur_threshold;\t\tend =256; \t} \tfor(int i=start;i&lt;end;i++) \t\t total += (int)cvQueryHistValue_1D(Histogram1,i);//查询直方块的值 P304\tfor(int j=start;j&lt;end;j++)\t{\t\tif((int)cvQueryHistValue_1D(Histogram1,j)==0)\t\tcontinue;\t\tdouble percentage = cvQueryHistValue_1D(Histogram1,j)/total;\t\t/*熵的定义公式*/\t\tcur_entropy +=-percentage*logf(percentage);\t}\treturn cur_entropy; }  //寻找最大熵阈值并分割 int MaxEntropy(IplImage *src) {\tint HistogramBins=256;\tCvHistogram * hist = cvCreateHist(1,&amp;HistogramBins,CV_HIST_ARRAY,0,1);//创建一个指定尺寸的直方图\t//参数含义：直方图包含的维数、直方图维数尺寸的数组、直方图的表示格式、方块范围数组、归一化标志\tcvCalcHist(&amp;src,hist);//计算直方图\tdouble maxentropy =-1.0;\tint max_index =-1;\t// 循环测试每个分割点，寻找到最大的阈值分割点\tfor(int i=0;i&lt;HistogramBins;i++) \t{\t\tdouble cur_entropy = caculateCurrentEntropy(hist,i,0)+caculateCurrentEntropy(hist,i,1);\t\tif(cur_entropy&gt;maxentropy)\t\t{\t\t\tmaxentropy = cur_entropy;\t\t\tmax_index = i;\t\t}\t}\tcvReleaseHist(&amp;hist);\treturn max_index; }基本全局阈值基本思路是首先设定一个阈值，然后通过计算质量矩调整阈值，直到阈值稳定/*=============================================================================  代码内容：基本全局阈值法                              ==============================================================================*/int BasicGlobalThreshold(int*pg,int start,int end){                                               int  i,t,t1,t2,k1,k2;    double u,u1,u2;        t=0;         u=0;    for (i=start;i&lt;end;i++)     {        t+=pg[i];                u+=i*pg[i];    }    k2=(int) (u/t);                          //  计算此范围灰度的平均值        do     {        k1=k2;        t1=0;            u1=0;\tfor (i=start;i&lt;=k1;i++)         {            //  计算低灰度组的累加和            t1+=pg[i];                u1+=i*pg[i];        }        t2=t-t1;        u2=u-u1;\tif (t1)             u1=u1/t1;                     //  计算低灰度组的平均值\telse             u1=0;\tif (t2)             u2=u2/t2;                     //  计算高灰度组的平均值\telse             u2=0;        k2=(int) ((u1+u2)/2);                 //  得到新的阈值估计值    }    while(k1!=k2);                           //  数据未稳定，继续   //cout&lt;&lt;\"The Threshold of this Image in BasicGlobalThreshold is:\"&lt;&lt;k1&lt;&lt;endl;   return(k1);                              //  返回阈值}迭代法基本思路和全局阈值法差不多，也是基于一个初始阈值不停迭代直到稳定，只是衡量准则有点不同/*======================================================================*//* 迭代法*//*======================================================================*/// nMaxIter：最大迭代次数；nDiffRec：使用给定阀值确定的亮区与暗区平均灰度差异值int DetectThreshold(IplImage*img, int nMaxIter, int&amp; iDiffRec) {     //图像信息    int height = img-&gt;height;    int width = img-&gt;width;    int step = img-&gt;widthStep/sizeof(uchar);    uchar *data = (uchar*)img-&gt;imageData;    iDiffRec =0;    int F[256]={ 0 }; //直方图数组    int iTotalGray=0;//灰度值和    int iTotalPixel =0;//像素数和    byte bt;//某点的像素值    uchar iThrehold,iNewThrehold;//阀值、新阀值    uchar iMaxGrayValue=0,iMinGrayValue=255;//原图像中的最大灰度值和最小灰度值    uchar iMeanGrayValue1,iMeanGrayValue2;    //获取(i,j)的值，存于直方图数组F    for(int i=0;i&lt;width;i++)    {\tfor(int j=0;j&lt;height;j++)        {            bt = data[i*step+j];            if(bt&lt;iMinGrayValue)                iMinGrayValue = bt;\t    if(bt&gt;iMaxGrayValue)                iMaxGrayValue = bt;            F[bt]++;        }    }    iThrehold =0;//    iNewThrehold = (iMinGrayValue+iMaxGrayValue)/2;//初始阀值    iDiffRec = iMaxGrayValue - iMinGrayValue;    for(int a=0;(abs(iThrehold-iNewThrehold)&gt;0.5)&amp;&amp;a&lt;nMaxIter;a++)//迭代中止条件    {\t\tiThrehold = iNewThrehold;\t\t//小于当前阀值部分的平均灰度值\t\tfor(int i=iMinGrayValue;i&lt;iThrehold;i++)\t\t{\t\t\tiTotalGray += F[i]*i;//F[]存储图像信息\t\t\tiTotalPixel += F[i];\t\t}\t\tiMeanGrayValue1 = (uchar)(iTotalGray/iTotalPixel);\t\t//大于当前阀值部分的平均灰度值\t\tiTotalPixel =0;\t\tiTotalGray =0;\t\tfor(int j=iThrehold+1;j&lt;iMaxGrayValue;j++)\t\t{\t\t\tiTotalGray += F[j]*j;//F[]存储图像信息\t\t\tiTotalPixel += F[j];    \t\t}\t\tiMeanGrayValue2 = (uchar)(iTotalGray/iTotalPixel);\t\tiNewThrehold = (iMeanGrayValue2+iMeanGrayValue1)/2;        //新阀值\t\tiDiffRec = abs(iMeanGrayValue2 - iMeanGrayValue1);     }\t//cout&lt;&lt;\"The Threshold of this Image in imgIteration is:\"&lt;&lt;iThrehold&lt;&lt;endl;\treturn iThrehold;}OTSU法这应该是阈值分割里最有名的方法了.OTSU(大津算法)将类间方差作为区域分割的衡量标准，认为方差越大分割越精确/* OTSU 算法可以说是自适应计算单阈值（用来转换灰度图像为二值图像）的简单高效方法。 下面的代码最早由 Ryan Dibble提供，此后经过多人Joerg.Schulenburg, R.Z.Liu 等修改，补正。 算法对输入的灰度图像的直方图进行分析，将直方图分成两个部分，使得两部分之间的距离最大。 划分点就是求得的阈值。 */ int otsu (IplImage *image) {\t int thresholdValue=1;              // 阈值                  \t int i,k;                           // various counters\t int n, n1, n2;     &lt;span style=\"white-space:pre\"&gt;\t&lt;/span&gt; double m1, m2, sum, csum, fmax, sb;\t // 生成直方图\t int HistogramBins=256;\t CvHistogram * hist = cvCreateHist(1,&amp;HistogramBins,CV_HIST_ARRAY,0,1);//创建一个指定尺寸的直方图\t cvCalcHist(&amp;image,hist);//计算直方图\t // set up everything\t sum = csum =0.0;\t n =0;\t for (k =0; k &lt;256; k++) \t {\t\ti=cvQueryHistValue_1D(hist,k);\t\tsum += k * i; /* x*f(x) 质量矩*/\t\tn += i; /* f(x) 质量 */\t }\tif (!n) \t{\t // if n has no value, there is problems...\t thresholdValue =160;\t}    &lt;span style=\"white-space:pre\"&gt;\t&lt;/span&gt;// do the otsu global thresholding method\tfmax =-1.0;\tn1 =0;\tfor (k =0; k &lt;255; k++) \t{\t\ti=cvQueryHistValue_1D(hist,k);\t\tn1 += i;\t\tif (!n1) { continue; }\t\tn2 = n - n1;\t\tif (n2 ==0) { break; }\t\tcsum += k *i;\t\tm1 = csum / n1;\t\tm2 = (sum - csum) / n2;\t\tsb = n1 * n2 *(m1 - m2) * (m1 - m2);\t\t/* bbg: note: can be optimized. */\t\tif (sb &gt; fmax)\t\t{\t\t\tfmax = sb;\t\t\tthresholdValue = k;\t\t}\t}\tcvReleaseHist(&amp;hist);\treturn(thresholdValue); }"
  },
  
  {
    "title": "图像形态学处理（3End）",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%86-3End/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-03-17 00:00:00 +0000",
    





    
    "snippet": "灰度级图像形态学灰度级图像形态学适用于这两种情形：应用希望保留图像灰度阶、需要在二值化前做预处理以突出某些特征基础操作腐蚀灰度级的结构元较二值结构元多了一个维度，根据结构元有没有灰度变化（或者说在实际应用中考不考虑灰度变化），可以分为平坦结构元和不平坦结构元使用平坦结构元b在(x,y)处对f做腐蚀：\\[[f\\ominus b](x,y)=\\min_{(s,t)\\in b}\\{f(x+s,y+...",
    "content": "灰度级图像形态学灰度级图像形态学适用于这两种情形：应用希望保留图像灰度阶、需要在二值化前做预处理以突出某些特征基础操作腐蚀灰度级的结构元较二值结构元多了一个维度，根据结构元有没有灰度变化（或者说在实际应用中考不考虑灰度变化），可以分为平坦结构元和不平坦结构元使用平坦结构元b在(x,y)处对f做腐蚀：\\[[f\\ominus b](x,y)=\\min_{(s,t)\\in b}\\{f(x+s,y+t)\\}\\]使用不平坦结构元b对f做腐蚀：\\[[f\\ominus b_N](x,y)=\\min_{(s,t)\\in b}\\{f(x+s,y+t)-b_N(s,t)\\}\\]一般还是用的平坦结构元可以看到，灰度形态学操作就是把二值形态学里的交换成取最小，或换成取最大。而对0/1取最小或最大就等价于交和或，因此图像形态学处理（1）里OpenCV的形态学公式能兼容二值和灰度。膨胀使用平坦结构元b在(x,y)处对f做膨胀：\\[[f\\oplus b](x,y)=\\max_{(s,t)\\in b}\\{f(x-s,y-t)\\}\\]使用不平坦结构元b对f做膨胀：\\[[f\\oplus b_N](x,y)=\\max_{(s,t)\\in b}\\{f(x-s,y-t)+b_N(s,t)\\}\\]开操作、闭操作与二值操作一样，开操作是先腐蚀后膨胀，闭操作是先膨胀后腐蚀\\[f\\circ b=(f\\ominus b)\\oplus b\\\\f\\bullet b=(f\\oplus b)\\ominus b\\]开操作会抑制比结构元小的亮细节；闭操作会抑制暗细节基本的形态学算法形态学平滑由于开操作和闭操作对亮细节和暗细节的抑制作用，对图像先后做开操作和闭操作会有效抑制细节，起到平滑作用形态学梯度\\[g=(f\\oplus b)-(f\\ominus b)\\]顶帽、底帽变换顶帽：$T_{hat}=f-(f\\circ b)$底帽：$B_{hat}=(f\\bullet b)-f$主要应用是：用一个结构元通过开或闭操作从一幅图像中删除物体顶帽变换的一个重要用途是校正不均匀光照的影响图2-1. 顶帽变换校正不均匀光照图2-1左是不均匀光照下得到的，如果用这幅图做二值化，底部暗区域很容易丢失目标；中图是用半径41的圆形结构元做开操作得到的，由于尺度足够大，原图没有任何目标被命中，因此得到一个完全模糊的背景（但重要的是反映了光照的变化）；右图是用左图减去中图（顶帽变换）得到的，已经把光照影响去除了粒度测定利用了开操作的特性：某个尺寸的开操作会对包含了这个尺寸的区域产生最大的效果（对应的，闭操作可以删除尺寸小于结构元的黑色区域）图2-2. 粒度测定考虑图2-2，假设需要测定图中木钉的尺寸。设置一个半径从小到大的结构元序列，依次对其做开操作，并且统计图像总灰度值，计算相邻灰度值的差，由于当结构元尺寸与木钉尺寸吻合时会产生一个局部灰度高峰，因此比较不同尺寸的灰度差序列，选取高峰对应的尺寸就是木钉的大致尺寸形态学重建与二值图像类似，将与换成取最小，或换成取最大标记图像f关于模板图像g的大小为n的测地膨胀：\\[D_g^{(n)}(f)=D_g^{(1)}[D_g^{(n-1)}(f)]\\]其中\\[D_g^{(1)}(f)=(f\\oplus b)\\land g\\\\D_g^{(0)}(f)=f\\]$\\land$表示取最小测地腐蚀、形态学重建的开、闭也与二值类似。图2-3是一个利用灰度形态学重建开等操作去除不规则背景的案例图2-3. 灰度形态学重建图2-3b为使用$1\\times 71$的结构元对a做重建开操作，目的是选取到横向长条形的反光；c图是用$21\\times 1$的结构元选取纵向反光；由于目标中有些区域如“SIN”的“I”也会在c中被选中，这样a-c后“I”会丢失，因此在d中对a-c的结果做横向膨胀（依据是“I”离其他目标很近），然后用这个结果和a-b取最小值得到结果e。注：文章算法及图片素材取自冈萨雷斯《数字图像处理》P.S. 一本关注图像形态学的书：崔屹-《图像处理与分析–数学形态学方法及应用》"
  },
  
  {
    "title": "图像形态学处理（2）",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%86-2/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-03-16 00:00:00 +0000",
    





    
    "snippet": "细化、粗化基本思路是：使用能反映边界的模式击中图像，然后从原图中减去，从而达到细化的目的；粗化就与原图取或。使用模式B细化A：\\[A\\times B=A\\cap (A\\otimes B)^c\\]使用模式B粗化A：\\[A\\cdot B=A\\cup (A\\otimes B)\\]一般对模式B做45度旋转形成一组模式：图1-6使用该组模式对左图做细化图1-6. 细化骨架骨架能反映一个前景的结构，考虑...",
    "content": "细化、粗化基本思路是：使用能反映边界的模式击中图像，然后从原图中减去，从而达到细化的目的；粗化就与原图取或。使用模式B细化A：\\[A\\times B=A\\cap (A\\otimes B)^c\\]使用模式B粗化A：\\[A\\cdot B=A\\cup (A\\otimes B)\\]一般对模式B做45度旋转形成一组模式：图1-6使用该组模式对左图做细化图1-6. 细化骨架骨架能反映一个前景的结构，考虑集合A（图1-7左），用一个能刚好被包含的最大盘遍历，其中心轨迹就是A的骨架（图1-7右）图1-7. 骨架（《数字图像处理》）骨架提取由腐蚀和开操作组成：\\[S(A)=\\bigcup_{k=0}^KS_k(A)\\]其中\\[S_k(A)=(A\\ominus kB)-(A\\ominus kB)\\circ B\\]$kB$表示连续对A做k次腐蚀，K值取A被B腐蚀为空集所需次数-1图1-8. 骨架抽取裁剪裁剪是对细化和骨架算法的补充，这两种方法都会产生寄生分量（毛刺），需要使用裁剪去掉裁剪算法步骤：  使用一系列检测端点的模式B对A做细化（这里重复做3次），得到细化结果X1：$X_1=A\\times {B}$其中${B}$一般取  细化结果$X_1$可能会丢失一些必要的端点，需要做一些补偿首先用${B}$分别对$X_1$做击中，结果取或：\\[X_2=\\bigcup_{k=1}^8(X_1\\otimes B^k)\\]然后以A为限定，对$X_2$做条件膨胀：$X_3=(X_2\\oplus H)\\cap A$，这里$H$取3*3  取$X_1$和$X_3$的并集就是最终结果：$X_4=X_1\\cup X_3$图1-9. 裁剪形态学重建形态学重建涉及两幅图像F、G和一个结构元B；F是标记图像，决定变换的起始，G是模板图像，约束变换。形态学重建算法一般凭借约束和标记，迭代的对图像处理，直到收敛，达到处理自动化的目的（上文孔洞填充）测地膨胀、测地腐蚀标记图像F关于模板图像G的大小为n的测地膨胀：\\[D_G^{(n)}(F)=D_G^{(1)}[D_G^{(n-1)}(F)]\\]其中\\[D_G^{(1)}(F)=(F\\oplus B)\\cap G\\]由于G的约束，迭代最终会趋于收敛，此时令$R_G^D(F)$为测地膨胀的结果\\[R_G^D(F)=D_G^{(k)}(F)\\]类似的，标记图像F关于模板G的测地腐蚀为：\\[E_G^{(n)}(F)=E_G^{(1)}[E_G^{(n-1)}(F)]\\]其中\\[E_G^{(1)}(F)=(F\\ominus B)\\cup G\\]最终取\\[R_G^E(F)=E_G^{(k)}(F)\\]重建开操作形态学开操作首先删除小物体，再通过膨胀试图恢复遗留前景重建开操作由于有了G的约束，解决了传统开操作高度依赖准确结构元才能正确恢复形状的缺点重建开操作的作用是：准确提取图像中与结构元（腐蚀时的，膨胀时用一般的就可以了）相似的模式使用结构元B对F做大小为n的重建开操作：\\[O_R^{(n)}(F)=R_F^D[F\\ominus nB]\\]图1-10. 重建开操作（中间为一般开操作，作为对比）（结构元取51*1）边界清除过程与孔洞填充（在上文已有介绍）类似，以原图I为模板，根据I生成标记图像F\\[F(x,y)=\\begin{cases}I(x,y),&amp; (x,y)位于I边界上\\\\0, &amp; 其他\\end{cases}\\]然后计算测地膨胀$R_I^D(F)$，最后计算差，得到没有边界接触的结果\\[X=I-R_I^D(F)\\]"
  },
  
  {
    "title": "图像形态学处理（1）",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%BD%A2%E6%80%81%E5%AD%A6%E5%A4%84%E7%90%86-1/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-03-15 00:00:00 +0000",
    





    
    "snippet": "按操作对象的不同，形态学操作可分为二值形态学和灰度形态学。但在算法实现上，大部分情况二者是可以兼容的，MATLAB和OpenCV的膨胀、腐蚀、开闭等函数对二值和灰度都是一套。二值图像形态学基础操作腐蚀集合B对集合A的腐蚀：\\[A\\ominus B=\\{ z|B_z\\cap A^c=\\varnothing \\}\\]其中$z$表示坐标系，$A^c$为A的补集OpenCV的公式（灰度腐蚀也一样）更...",
    "content": "按操作对象的不同，形态学操作可分为二值形态学和灰度形态学。但在算法实现上，大部分情况二者是可以兼容的，MATLAB和OpenCV的膨胀、腐蚀、开闭等函数对二值和灰度都是一套。二值图像形态学基础操作腐蚀集合B对集合A的腐蚀：\\[A\\ominus B=\\{ z|B_z\\cap A^c=\\varnothing \\}\\]其中$z$表示坐标系，$A^c$为A的补集OpenCV的公式（灰度腐蚀也一样）更直观：\\[dst(x,y)=\\min_{(x',y'):element(x',y')\\ne 0}src(x+x',y+y')\\]考虑B中非0值（0值为无关点）对应位置下的所有A点，取其最小值为anchor位置的腐蚀结果（对于二值图像，只要A某个位置为0则腐蚀结果为0）。显然，腐蚀操作会缩小前景。膨胀集合B对集合A的膨胀：\\[A\\oplus B=\\{z|\\hat{B}_z\\cap A=\\varnothing\\}\\]OpenCV的公式（灰度膨胀也一样）：\\[dst(x,y)=\\max_{(x',y'):element(x',y')\\ne 0}src(x+x',y+y')\\]考虑B中非0值（0值为无关点）对应位置下的所有A点，取其最大值为anchor位置的膨胀结果（对于二值图像，只要A某个位置为1则膨胀结果为1）。显然，膨胀操作会扩大前景。开操作使用B对A先腐蚀再膨胀：\\[A\\circ B=(A\\ominus B)\\oplus B\\]腐蚀可以断开狭颈，膨胀可以消除突出物，所以叫做“开”，是不是很形象~闭操作使用B对A先膨胀再腐蚀：\\[A\\bullet B=(A\\oplus B)\\ominus B\\]与开相反，闭操作可以弥合较窄的间断击中击不中用于形状检测，在A中寻找符合B的模式：\\[A\\otimes B=(A\\ominus B_1)\\cap (A^c\\ominus B_2)\\]其中$B_1$和$B_2$由$B$得到，举个栗子：$B$中x表示无关点，在$B_1$和$B_2$中都设为0，其他位置$B_2$为对$B_1$取反图1-1为应用上述结构B的击中结果图1-1. 击中击不中基本的形态学算法边界提取对图做腐蚀处理，然后与原图做差：\\[\\beta (A)=A-(A\\ominus B)\\]很好理解，因为腐蚀缩小了前景孔洞填充一个孔洞可解释为被前景连通域包围的背景区域（图1-2左）令$X_0$为与待填充图像$A$大小一致的基准图，$A$孔洞处的某个位置在$X_0$上用1标记，其余位置为0，然后应用公式不断更新，直到$X_k$不再变化，此时$X_k$即为填充后的结果图1-2. 孔洞填充原理也很好理解：$X_0$中的1为种子点，对其不断的膨胀，然后用A的补集做限制，防止其超出连通域。因此也叫条件膨胀但种子点的设置实在是很不方便的一件事，因此上述算法没有实际应用价值基于形态学重建的孔洞填充则是“全自动”的：首先根据A生成一幅标记图像F：\\[F(x,y)=\\begin{cases}1-I(x,y), &amp;(x,y)在I的边界上\\\\0,&amp;其他\\end{cases}\\]然后以A的补集为模板图像，做测地腐蚀，取补集即为填充结果：\\[H=\\left [R_{A^c}^D(F)\\right ]^c\\]图1-3. 基于形态学重建的孔洞填充提取连通分量与孔洞填充互为对偶，前者提取背景，后者提取前景因此公式也相似：\\[X_k=(X_{k-1}\\oplus B)\\cap A\\]$X_0$为基准图，在待提取连通域某个位置标记1图1-4. 连通分量提取$X_0$只标记了图1-4左上部分连通域，因此仅提取了上部分。类似孔洞填充，连通分量也可用形态学重建的方式实现自动化凸壳如果在集合A内连接任意两点的线段都在A内，则称A为凸集；任意集合S的凸壳H是包含了S的最小的凸集。参考上述凸集的描述，可知凸壳的生成只需对S中（主要是边缘）凹陷部分做填充即可因此基本思想是：使用能描述凹陷模式的模板对S做击中，然后与S取或，直到不再变化为止。\\[X_k^i=(X_{k-1}\\otimes B^i)\\cup A\\]其中$X_0^i=A$，模板$B$一般取4组：当迭代收敛时即$X_k^i=X_{k-1}^i$时停止，赋值$D^i=X_k^i$，最终A的凸壳为\\[C(A)=\\bigcup_{i=1}^4D^i\\]图1-5. 凸壳生成过程注：图1-5外层应用了多次迭代"
  },
  
  {
    "title": "图像小波分析",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%B0%8F%E6%B3%A2%E5%88%86%E6%9E%90/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-02-22 00:00:00 +0000",
    





    
    "snippet": "本文旨在对图像处理中的小波分析做一个概要性的记录和介绍背景      傅里叶变换可以将信号表示为无限三角函数的累加形式，从而实现将信号从空间域到频率域的转换。然而这种转换丢失了信号时空域的信息（只知道频率及其幅值，但不知道该频率发生的空间位置，可以类比直方图），因此无法做局部分析。        短时傅里叶变换通过引入一个时间窗函数试图改进傅里叶的局部缺陷，但由于窗函数的尺寸是固定的，不能同...",
    "content": "本文旨在对图像处理中的小波分析做一个概要性的记录和介绍背景      傅里叶变换可以将信号表示为无限三角函数的累加形式，从而实现将信号从空间域到频率域的转换。然而这种转换丢失了信号时空域的信息（只知道频率及其幅值，但不知道该频率发生的空间位置，可以类比直方图），因此无法做局部分析。        短时傅里叶变换通过引入一个时间窗函数试图改进傅里叶的局部缺陷，但由于窗函数的尺寸是固定的，不能同时对信号高频和低频做精确分析。        小波变换基于可自动调节尺寸的窗函数（图像金字塔），在时域和频域均具有良好的局部化性能，被誉为“数学显微镜”。        小波变换在图像处理上可用于去噪、边缘提取（实质就是突出低频或高频），但最主要的应用在于图像压缩  小波变换基本原理傅里叶变换将信号分解为不同频率的三角函数之和的形式，小波变换则以尺度函数和小波函数为基，将信号分解。在这里，尺度是通过不断对图像做下2采样以建立图像金字塔得到的尺度函数由低通滤波器构造，小波函数由高通滤波器实现。一次分解有一组小波函数组成（类似傅里叶变换中不同频率的三角函数），这组小波函数由一个母小波函数通过缩放和平移生成。图2-1. 二维离散快速小波变换如图2-1所示，$h_0$为尺度函数，$h_1$为小波函数，相应的操作为卷积。结果的$h_0$为上一级的低频近似，$h_1$为上一级水平方向的高频近似，$h_2$为上一级垂直方向的高频近似，$h_3$为上一级对角线方向的高频近似。图2-2. 二维离散小波变换结果这里每次的分解都是从上级的低频近似开始，因为图像的大部分信息在低频区域；而小波包分解则对低频和高频都做分解。小波变换应用去噪和边缘增强：通过对小波变换后的高频、低频做相应抑制或提升来实现。在这里其实空间域或傅里叶频域也可处理，优势不大。图像压缩：由于图像主要信息在低频，因此可以对高频做稀疏化处理（甚至全设为0）实现压缩。傅里叶变换虽然也可提取不同频率，但因为丢失了空间信息因此无法做复原。代码Matlab的可以参考晨宇思远博客的小波系列博文C的可以参考http://eeweb.poly.edu/~onur/source.html，里面包含了小波及小波包分解重构的实现图4-1. 二级小波分解参考1) 晨宇思远博客小波变换系列2) 清华小波变换课件3) 小波分析,小波函数与尺度函数"
  },
  
  {
    "title": "图像压缩",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%8E%8B%E7%BC%A9/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-02-14 00:00:00 +0000",
    





    
    "snippet": "理论压缩率：$C=\\frac{b}{b’}$，其中b是原始数据大小，b’为压缩后数据大小数据冗余的原因1）编码冗余用于表示数据的码字所占空间比该数据本身空间更大。压缩算法中一般对高概率的数据用短码字，低概率的数据用长码字。2）空间和时间冗余除了边缘等少数情况，图像数据一般是邻域（空间）相关的；类似的，视频帧之间大部分情况也是时间相关的。3）不相关信息对于应用领域（对于图像，有些信息人类视觉可...",
    "content": "理论压缩率：$C=\\frac{b}{b’}$，其中b是原始数据大小，b’为压缩后数据大小数据冗余的原因1）编码冗余用于表示数据的码字所占空间比该数据本身空间更大。压缩算法中一般对高概率的数据用短码字，低概率的数据用长码字。2）空间和时间冗余除了边缘等少数情况，图像数据一般是邻域（空间）相关的；类似的，视频帧之间大部分情况也是时间相关的。3）不相关信息对于应用领域（对于图像，有些信息人类视觉可能无法分辨的信息）几乎无影响的信息。图像的熵图像信息的度量，表示不丢失信息的充分描述一幅图像的最小数据量\\[H=-\\sum_{k=0}^{L-1}p_r(r_k)log_2p_r(r_k)\\]其中L为灰度级，一般为256，$p_r$表示某灰度级在图像中出现的概率，以2为底使得H的单位为 比特/像素，也就是每像素平均最少可用多少比特表示图像压缩系统图1-1. 图像压缩系统（《数字图像处理》）该系统包含编码和解码器；映射器根据空间或时间冗余原则对原始数据进行映射，该过程是无损的；量化器根据无关信息原则对数据做采样（如视频的帧率调整），该过程是有损的；符号编码器根据编码冗余原则使用变长编码方法生成数据的表示，该过程是无损的。常用图像压缩标准图1-2. 常用图像压缩标准（《数字图像处理》）知乎上有个帖子对4种压缩方案进行了对比：$JPEG&lt;JPEG2000&lt;AVC Intra-Frame&lt;HEVC Intra-Frame(BPG)$并给出了原因  JPEG-&gt;JPEG2000多尺度分析降低块效应小波系数更稀疏  JPEG2000-&gt;AVC Intra-Frame降低空间结构冗余  AVC Intra-Frame-&gt;BPG 进一步降低空间结构冗余但由于版权专利等原因，目前主流仍然是JPEG基本压缩方法针对编码冗余主要有 Huffman编码、Golomb编码、算术编码Huffman编码：a. 按出现概率从高到低对待编码符号排序b. 从低到高每次选取两个符号进行组合，组合的概率为两者之和，对于2进制编码，当最后剩2个符号时停止c. 以2进制为例，对最后的2个符号分别编码为0和1，反溯编码图2-1. Huffman编码 a,b步骤（《数字图像处理》）图2-2. Huffman编码 c步骤（《数字图像处理》）考虑到计算复杂度，JPEG和MPEG规定了默认的Huffman编码表Golomb编码只能编码非负整数a. 指定一个Golomb码（正整数）m，对于待编码n，计算n/m，对结果向下取整，将其一元编码作为输出（例如对取整结果2，其一元编码为2个1加1个0：110）b. 计算$log2(m)$，对其向上取整得k；计算$c=2^k-m$，$r=n\\%m$。如果r&gt;=0且r&lt;c，那么取r的k-1 bit位（低位到高位）作为输出，其他情况取r+c的k bit位作为输出c. 连接a与b的输出作为最终编码（二进制）Golomb编码的压缩效果能达到Huffman的96%，优势是不需要像Huffman一样计算编码表算术编码算术编码对任意长度的信息输出都是一个数值，可以接近无损压缩的熵极限具体算法和分析可参考http://www.cnblogs.com/liuokay/archive/2011/04/19/2020856.html针对空间冗余主要有 LZW编码、行程编码、基于符号的编码、预测编码等LZW编码基本思想是，建立一个字典记录每个模式（灰度序列），整个图像由这些模式组成LZW编码不需要符号出现概率的先验知识，GIF,PDF,PNG均使用了该方法行程编码基本思想是，对连续重复数据用重复次数和该数据值表示，如aaabccccd的行程编码为3a1b4c1d显然，对随机度高的数据序列，行程编码可能适得其反基于符号的编码与LZW思路差不多，但模式不再是序列而是符号（如’a’字符），然后通过模板匹配的方法进行压缩编码（同时，符号本身还可以进行再编码）预测编码根据某种模型，利用已收到的一个或几个样本，对正在接收的样本进行预测，将预测和实际值之差做编码进行传输一般来说预测值和实际值差别较小（帧内空间冗余，帧间时间冗余），这样就可以用较小的位数表示并做传输针对不相关信息主要有 块变换编码、小波编码等块变换编码将图像划分为大小相等且互不重叠的子块；应用某种变换单独处理这些子块（到这里还是无损的），将其映射为变换系数的集合；丢弃一些对应用贡献较小的系数（如高频）做编码变换方式一般选择 Walsh-Hadamard变换、离散傅里叶变换（DFT）或离散余弦变换（DCT）其中DCT的信息携带能力比DFT和WHT都要强，因此多数变换编码系统都基于DCT（包括JPEG）子块尺寸一般选择 88 和 1616块变换编码的一个问题是：容易产生块效应（相邻子块间的边界变得可见）小波编码对图像做小波变换，然后对高低频系数做量化（主要是截掉一些高频信息）以减小数据量。由于是基于全局的因此没有了块效应，JPEG2000应用了小波编码"
  },
  
  {
    "title": "彩色图像处理",
    "url": "/posts/%E5%BD%A9%E8%89%B2%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-02-04 00:00:00 +0000",
    





    
    "snippet": "色彩空间关于色彩空间，看过一个Photoshop培训系列视频，第一集的内容就是这个，讲的很透彻，可惜不记得名字了按面向对象，色彩空间大致可以分为3类:面向显示一般是针对显示器这种自己发光的应用，因此属于加色模式（光的三原色：红绿蓝RGB）该类比较有代表性的是RGB模型和YUV模型1. RGB模型基于笛卡尔坐标系，xyz每个轴分别代表RGB，起源于采用阴极射线管的彩色电视图1-1. RGB模型...",
    "content": "色彩空间关于色彩空间，看过一个Photoshop培训系列视频，第一集的内容就是这个，讲的很透彻，可惜不记得名字了按面向对象，色彩空间大致可以分为3类:面向显示一般是针对显示器这种自己发光的应用，因此属于加色模式（光的三原色：红绿蓝RGB）该类比较有代表性的是RGB模型和YUV模型1. RGB模型基于笛卡尔坐标系，xyz每个轴分别代表RGB，起源于采用阴极射线管的彩色电视图1-1. RGB模型由于是加色模型，坐标原点表示黑色，三色叠加最大值表示白色2. YUV      是电视PAL制式采用的色彩空间，主要为了减小信号传输带宽，向下兼容黑白电视        Y表示明度，UV表示色度；对黑白电视，只需解析Y信号即可；UV信号可以压缩（如YUV420，每4个Y共用一组UV）  \\[\\begin{bmatrix}Y\\\\ U\\\\ V\\end{bmatrix}=\\begin{bmatrix}0.299 &amp; 0.587 &amp; 0.114\\\\ -0.1678 &amp; -0.3313 &amp; 0.5\\\\ 0.5 &amp; -0.4187 &amp; -0.0813\\end{bmatrix}\\begin{bmatrix}R\\\\ G\\\\ B\\end{bmatrix}\\]\\[\\begin{bmatrix}R\\\\ G\\\\ B\\end{bmatrix}=\\begin{bmatrix}1 &amp; 0 &amp; 1.402\\\\ 1 &amp; -0.34414 &amp; -0.71414\\\\ 1 &amp; 1.1772 &amp; 0\\end{bmatrix}\\begin{bmatrix}Y\\\\ U\\\\ V\\end{bmatrix}\\]      上面两个式子是YUV和RGB互转的方法，但YUV一般会以某种采样规则进行压缩（毕竟就是为了压缩信号而生的），因此YUV转RGB首先还需按采样规则做插值        按采样规则，YUV可以细分为很多格式（如YUV444，YUV422，YUV420），“图文详解YUV420数据格式”一文有详述  面向印刷  印刷品的内容需要光线照在上面，经过颜料吸收后剩下的波段反射回人眼，因此属于减色模式（颜料的三原色：青品黄CMY）  基于以上描述，CMY模型就是RGB的反色：\\[\\begin{bmatrix}C\\\\ M\\\\ Y\\end{bmatrix}=\\begin{bmatrix}1\\\\ 1\\\\ 1\\end{bmatrix}-\\begin{bmatrix}R\\\\ G\\\\ B\\end{bmatrix}\\]  理论上等量CMY三色相加就是黑色，但由于工艺原因这种方法无法获得纯黑色（但黑色又是最常用色），因此工业上单独添加黑色颜料K，组成CMYK四色模型面向人眼前两类模型实际都是为了机器描述方便而设计的，但人眼更偏向于从色调、饱和度、亮度（HSI）三个方面来描述色彩将RGB正方体模型的对角线作为z轴即是HSI的模型图1-2. HSI色彩空间  从RGB到HSI\\[\\begin{matrix}H=&amp;\\begin{cases}\\theta,  &amp;B\\le G \\\\ 360-\\theta,  &amp;B&gt;G\\end{cases}\\\\S=&amp;1-\\frac{3}{(R+G+B)}[min(R,G,B)]\\\\I=&amp;\\frac{1}{3}(R+G+B)\\end{matrix}\\]其中$\\theta=arccos\\left(\\frac{0.5[(R-G)+(R-B)]}{\\sqrt{[(R-G)^2+(R-B)(G-B)]}}\\right)$  从HSI到RGB            若$0\\le H&lt;120$      若$120\\le H&lt;240$      若$240\\le H&lt;360$                  -      $H=H-120$      $H=H-240$              $B=I(1-S)$      $R=I(1-S)$      $G=I(1-S)$              $R=I[1+\\frac{ScosH}{cos(60-H)}]$      $G=I[1+\\frac{ScosH}{cos(60-H)}]$      $B=I(1+\\frac{ScosH}{cos(60-H)})$              $G=3I-(R+B)$      $B=3I-(R+G)$      $R=3I-(B+G)$      补充此外还有一种色彩空间CIE Lab*，由于工艺等原因，不同设备的色彩表现是存在差异的。CIE实际是一套标准，便于设备的色彩校正以保证设备间色彩的一致性彩色图像处理围绕彩色空间，主要有4个方向的图像处理方式基于伪彩色的显示增强理论依据是：人眼对灰度级的分辨力只有20几种，但可分辨几千种色调，因此对兴趣区域按灰度值赋色彩可大大提升区域的分辨度这里除了按灰度级定义一个颜色查询表以对整幅图像进行着色的方式外，还有一种常用方式（彩色分层）：对灰度处于某个范围内的像素着色，其他的像素统一置成一个较低的灰度，这样可以最大程度的突出目标区域彩色图像的直方图处理、平滑、锐化此时不推荐对每个通道单独处理再组合，因为每个通道的灰度值分布有差异，单独处理后可能造成图像色彩偏移等失真现象。较好的方式是：先将图像转成HSI，仅对亮度值I做处理，避免了色彩的失真基于彩色的图像分割这是一块大内容，相关的算法很多，以后再做详述看到《数字图像处理》中有介绍彩色图像的边缘检测，觉得应该可以参考（平时都是先转为灰度图再做的）:对RGB空间，其x和y方向二阶导数为：\\[\\begin{cases}g_{xx}=u^Tu=\\left |\\frac{\\partial R}{\\partial x}\\right|^2+\\left |\\frac{\\partial G}{\\partial x}\\right|^2+\\left |\\frac{\\partial B}{\\partial x}\\right|^2\\\\g_{yy}=v^Tv=\\left |\\frac{\\partial R}{\\partial y}\\right|^2+\\left |\\frac{\\partial G}{\\partial y}\\right|^2+\\left |\\frac{\\partial B}{\\partial y}\\right|^2\\\\g_{xy}=u^Tv=\\frac{\\partial R}{\\partial x}\\frac{\\partial R}{\\partial y}+\\frac{\\partial G}{\\partial x}\\frac{\\partial G}{\\partial y}+\\frac{\\partial B}{\\partial x}\\frac{\\partial B}{\\partial y}\\end{cases}\\]依据导数，可得到其梯度方向为$F_{\\theta}(x,y)=0.5arctan\\left[\\frac{2g_{xy}}{g_{xx}-g_{yy}}\\right]$在角度$\\theta$上，其变化率为$F_{\\theta}=\\sqrt{(g_{xx}+g_{yy})+(g_{xx}-g_{yy})cos2\\theta(x,y)+2g_{xy}sin2\\theta(x,y)}$在实际中（图片是离散信号）导数值可用Sobel算子得到"
  },
  
  {
    "title": "空间滤波&频域滤波(3End)",
    "url": "/posts/%E7%A9%BA%E9%97%B4%E6%BB%A4%E6%B3%A2&%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2(3End)/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-02-03 00:00:00 +0000",
    





    
    "snippet": "图像的退化与复原图像的退化过程一般可归纳为退化函数和噪声的影响：\\[g(x,y)=h(x,y)\\times f(x,y)+\\eta(x,y)\\tag{1}\\]其中$h(x,y)$是退化函数，$\\eta(x,y)$是噪声函数，转换到频率域可将卷积运算简化为乘积运算：\\[G(u,v)=H(u,v)F(u,v)+N(u,v)\\tag{2}\\]常见的噪声模型一般有高斯噪声、瑞利噪声、伽马噪声、指数噪...",
    "content": "图像的退化与复原图像的退化过程一般可归纳为退化函数和噪声的影响：\\[g(x,y)=h(x,y)\\times f(x,y)+\\eta(x,y)\\tag{1}\\]其中$h(x,y)$是退化函数，$\\eta(x,y)$是噪声函数，转换到频率域可将卷积运算简化为乘积运算：\\[G(u,v)=H(u,v)F(u,v)+N(u,v)\\tag{2}\\]常见的噪声模型一般有高斯噪声、瑞利噪声、伽马噪声、指数噪声、均匀噪声、脉冲（椒盐）噪声图1-1. 噪声概率密度函数非周期噪声分布的参数估计：截取图像窗口（或整张图），计算其直方图，调整图1-1中噪声函数参数，使其分布逼近直方图以上噪声一般在空间域处理（非周期），根据不同的噪声分布选用不同的空间滤波器（在这里仅考虑噪声为图像退化的唯一原因）图1-2. 空间滤波器（参考《数字图像处理》）周期噪声图像中的周期信号在傅里叶域为对称的二维点，因此非常适合频域滤波器频域处理的理论分析及常用滤波器见空间滤波&amp;频域滤波(2)，这里再补充一个最佳陷波滤波器“当存在几种干扰分量时，前面讨论的方法有时就不能采用了，因为它们在滤波的过程中可能会消除太多的图像信息。。。最佳陷波滤波在一定意义上最小化了复原估计值的局部方差”（《数字图像处理》）最佳陷波滤波主要分两步：  通过频谱分析确定一个陷波带通滤波器，以提取噪声模型，逆傅里叶变换后得到空间域噪声模型的估计\\[\\eta(x,y)=\\xi^{-1}(H_{NP}(u,v)G(u,v))\\]  为了更逼近真实噪声模型，最佳陷波滤波对上一步的估计设置加权系数，这样最终的恢复结果为\\[\\hat{f}(x,y)=g(x,y)-w(x,y)\\eta(x,y)\\]其中加权系数$w(x,y)$的确定主要考虑使得恢复结果和真实结果的窗口内方差取极小值得到。\\[w(x,y)=\\frac{\\overline{g(x,y)\\eta(x,y)}-\\overline g(x,y)\\overline \\eta(x,y)}{\\overline{\\eta^2}(x,y)-\\overline{\\eta}^2(x,y)}\\]为了减小运算量，一般将窗口内的所有权重取相同的值，即对一个窗口来说，$w(x,y)$为常数，此时上式每个值取窗口中心值计算即可考虑退化函数的图像复原前面对图像复原的分析和处理仅考虑噪声的影响，即假定的系统模型为$G(u,v)=F(u,v)+N(u,v)$，但实际情况下由于相机等硬件和环境影响，系统为$G(u,v)=H(u,v)F(u,v)+N(u,v)$，因此需要对退化函数做出估计。退化函数的估计有3种方法：  基于图像观察的估计选取退化图像中高对比度的区域（为了降低噪声的影响），对其进行处理以得到清晰的样本，然后利用两者得到一个估计\\[H_s(u,v)=\\frac{G_s(u,v)}{\\hat{F}_s(u,v)}\\]$H_s(u,v)$为选取的高对比度区域，$\\hat{F}_s(u,v)$为处理后的清晰样本。个人认为这是最直接（很多情况下是无法对退化函数做数学建模的）最有效（效果取决于参照样本）的方法。  基于试验的估计如果手上有与获取退化图像的设备相似（甚至同一套），那么基于试验方法理论上能得到准确的退化函数试验过程是使用相同的系统对一个冲击（小亮点，尽可能量以降低噪声影响）成像，得到退化的冲击响应\\[H(u,v)=\\frac{G(u,v)}{A}\\]$G(u,v)$为观察到的退化图像，A是冲击的傅里叶变换（在频域为一个常数）  建模估计如果已知图像退化的原因，可以尝试从数学上对其进行建模。如Hufnagel等人基于大气湍流的物理特性提出的一个退化模型\\[H(u,v)=e^{-k(u^2+v^2)^{\\frac{5}{6}}}\\]这里以运动模糊为例介绍其退化模型的推导：令T为快门时间，x’(t)和y’(t)为时间t内x方向和y方向的位移，那么清晰图像f(x,y)和模糊图像g(x,y)的关系为\\[g(x,y)=\\int_0^Tf[x-x'(t),y-y'(t)]dt\\]对其做傅里叶变换，整理，得\\[G(u,v)=\\int_0^TF(u,v)e^{-j2\\pi [ux'(t)+vy'(t)]}dt\\\\=F(u,v)\\int_0^Te^{-j2\\pi [ux'(t)+vy'(t)]}dt\\]即退化模型为\\[H(u,v)=\\int_0^Te^{-j2\\pi [ux'(t)+vy'(t)]}dt\\\\=\\frac{T}{\\pi (ua+vb)}sin[\\pi(ua+vb)]e^{-j\\pi (ua+vb)}\\]其中a、b为时间T内x、y方向的总位移基于退化函数的频域图像复原基于退化函数的复原方法主要有逆滤波、维纳滤波、约束最小二乘方滤波、几何均值滤波等  逆滤波直接忽略噪声的影响，复原公式为\\[\\hat{F}(u,v)=\\frac{G(u,v)}{H(u,v)}\\]这种方法的缺点显而易见：如果$H(u,v)$较小，那么噪声将显著影响复原效果（考虑真实情况带噪声$N(u,v)$的复原）  维纳滤波考虑噪声，以复原图和原始图的最小均方误差为依据\\[\\hat{F}(u,v)=\\left [\\frac{1}{H(u,v)}\\frac{|H(u,v)|^2}{|H(u,v)|^2+\\frac{S_{\\eta}(u,v)}{S_f(u,v)}}\\right ]G(u,v)\\]其中$S_{\\eta}(u,v)$和$S_f(u,v)$分别为噪声功率谱和未退化图像功率谱，然而在实际中一般是不知道噪声的情况的，因此一般用参数K来近似，K的选取采用交互式方法\\[\\hat{F}(u,v)=\\left [\\frac{1}{H(u,v)}\\frac{|H(u,v)|^2}{|H(u,v)|^2+K}\\right ]G(u,v)\\]  约束最小二乘方滤波\\[\\hat{F}(u,v)=\\left [\\frac{H^*(u,v)}{|H(u,v)|^2+\\gamma|P(u,v)|^2}\\right ]G(u,v)\\]其中$P(u,v)$为3*3的拉普拉斯算子的频域形式，与维纳滤波一样，$\\gamma$的取值也采用交互式方法确定实验以lena图像为原始数据，对其进行运动模糊，然后添加高斯噪声；对退化的图像采用维纳滤波图4-1. 维纳滤波复原运动模糊如图4-1，a为原始图像，b为在a的基础上做运动模糊（x方向a=0.1，y方向b=0，取T=1），然后加上高斯噪声（均值为0，方差为0.05）得到；c为使用维纳滤波器取K=0时（此时相当于逆滤波器）的结果，d为使用维纳滤波器取K=0.01的结果，e为取K=0.021的结果注：1）在模拟运动模糊时若x和y方向同时有位移（C++实现），得到的图像与MATLAB的效果不同，后者与理论效果更近（参数维纳滤波一文有两者对比）2）在做$F(u,v)H(u,v)$时，$F(u,v)$的实部和虚部均与$H(u,v)$的实部相乘，否则会产生周期重影代码及参考涉及代码主要在FFT上，参考空间滤波&amp;频域滤波(2)参数维纳滤波(Parametric Wiener Filter)和图像去模糊（维纳滤波）有相关理论分析及MATLAB实现文章大部分内容参考自冈萨雷斯的《数字图像处理》"
  },
  
  {
    "title": "空间滤波&频域滤波(2)",
    "url": "/posts/%E7%A9%BA%E9%97%B4%E6%BB%A4%E6%B3%A2&%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2(2)/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-01-25 00:00:00 +0000",
    





    
    "snippet": "频域滤波数字图像本质上是数字信号，图像的滤波就是对信号的不同频率进行筛选。体现在图像上，模糊操作实际是抑制高频，锐化操作实际是抑制低频。因此，从频域上对图像信号做滤波更贴近信号处理的本质，同时也能实现空间域滤波很难达到的效果（如带通带阻）。那么，如何将一幅数字图像在空间域和频率域中做转换呢？离散傅里叶变换（DFT）离散傅里叶变换傅里叶提出：任何周期函数都可以用一系列正余弦函数的累加来逼近。因...",
    "content": "频域滤波数字图像本质上是数字信号，图像的滤波就是对信号的不同频率进行筛选。体现在图像上，模糊操作实际是抑制高频，锐化操作实际是抑制低频。因此，从频域上对图像信号做滤波更贴近信号处理的本质，同时也能实现空间域滤波很难达到的效果（如带通带阻）。那么，如何将一幅数字图像在空间域和频率域中做转换呢？离散傅里叶变换（DFT）离散傅里叶变换傅里叶提出：任何周期函数都可以用一系列正余弦函数的累加来逼近。因此对于一个空间域的信号（比如图像），可以用频率、幅值、相位来描述\\[\\begin{cases}F(u,v)=\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{-j2\\pi(\\frac{ux}{M}+\\frac{vy}{N})}\\\\f(x,y)=\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}F(u,v)e^{j2\\pi(\\frac{ux}{M}+\\frac{uy}{N})}\\end{cases}\\tag{1}\\]上面两个公式即为二维离散傅里叶变换（具体的理论推导是数字信号处理中的内容，网上内容很多，冈萨雷斯的数字图像处理也有详述），u,v表示频率采样位置（在这里将频率范围从0-2$\\pi$换到图像的高/宽），x,y表示空间采样位置。由复变函数的知识可以知道，$e^{j\\theta}$可以表示成实部+虚部，这样上述傅里叶正变换可表示为\\[F(u,v)=R(u,v)+I(u,v)\\]由此，信号在不同频率的幅值为\\[|F(u,v)|=\\sqrt{R(u,v)^2+I(u,v)^2}\\]每个频率对应的相位为\\[\\phi(u,v)=arctan\\left [\\frac{I(u,v)}{R(u,v)}\\right ]\\]图1-1. DFT（左：原图，中：幅值，右：相位）      上图使用不同的素材做DFT，相应的傅里叶谱和相位图都做了中心化转换（频率为0的点由左上角移至中心，具体做法是将得到的傅里叶谱图切分成2*2的子窗口，左上角的窗口和右下角的窗口互换，右上角的窗口和左下角的窗口互换；当然也可以通过将$原图 * (-1)^{(x+y)}$实现中心化转换）。        可以看到，在傅里叶谱中中心的点（代表频率为0）灰度值最高，其表示的是图像的平均灰度值；从整体形状上看，傅里叶谱指示着原图梯度的方向，并且这种指示不随图像的平移发生变化，然而此时相位图会发生很大变化。        从公式可以看到，DFT的每个点都需要遍历所有像素，时间复杂度为O(MNMN)，很显然无法实用，好在还有快速傅里叶变换（FFT），其时间复杂度为O(MNlog(MN))。FFT的具体算法可以参考FFT算法的完整DSP实现。  频域滤波器      在空间域上实现的平滑、锐化滤波器对应频域上的低通、高通滤波器，两者的效果相似，但应用了FFT的频域操作较空间域操作更便捷（空间域上的卷积对应频域上的点乘）        此外频域操作还能实现空间域不能完成的带通、带阻滤波（这是频域滤波的意义所在）  基本步骤1）对原图的高宽进行扩充，一般是原始高宽均扩大2倍，多出来的部分补0。这么做的原因是为了将数字图像模拟成周期信号时避免信号混淆。2）对扩充后的图像 f(x,y)，以$(-1)^{x+y}$相乘，这样转换得到的傅里叶谱就是以中心点为原点的了；或者也可以省略这步，将得到的傅里叶谱（此时四个角的位置频率最低）左上角与右下角互换，右上角与左下角互换（OpenCV的例程就是这么处理的）。3）设计一个频域滤波函数 H(u,v)，其尺寸与扩充后的图像尺寸一致，中心为低频。将FFT后的图像 F(u,v) 与 H(u,v)点乘得滤波后的结果G(u,v)。4）对G(u,v)做反傅里叶变换以转换到空间域，取其实部的值做归一化等操作为图像的值（不使用虚部值是为了避免由于计算误差导致的寄生复变量；如果之前是用$(-1)^{x+y}$实现的中心化，在这里还需再乘一次以换到图像坐标系）。5）对图像做裁剪，取自左上角开始的原始图像高宽的矩形区域为最终图像。常用滤波器低通滤波器\\[\\begin{cases}H(u,v)=\\frac{1}{1+\\left [\\frac{D(u,v)}{D_0}\\right ]^{2n}}\\\\H(u,v)=e^{-\\frac{D(u,v)^2}{2D_0^2}}\\end{cases}\\tag{2}\\]上边为布特沃斯低通滤波器，下边是高斯低通滤波器。D(u,v)为点(u,v)到中心的欧式距离，$D_0$为截止频率高通滤波器\\[\\begin{cases}H(u,v)=\\frac{1}{1+\\left [\\frac{D_0}{D(u,v)}\\right ]^{2n}}\\\\H(u,v)=1-e^{-\\frac{D(u,v)^2}{2D_0^2}}\\end{cases}\\tag{3}\\]上边为布特沃斯高通滤波器，下边是高斯高通滤波器。拉普拉斯增强\\[H(u,v)=-4\\pi^2(u^2+v^2)\\tag{4}\\]      反傅里叶变换后即为原图像的拉普拉斯变换，再加上原图即实现图像增强：$g(x,y)=f(x,y)+c\\nabla^2f(x,y)$        公式$g(x,y)=\\xi^{-1}{[k_1+k_2H_{HP}(u,v)]F(u,v)}$是个更一般的高频强调滤波方案，$k_2$控制高频贡献。        类似于高频强调滤波器，同态滤波器也能衰减低频增强高频  \\[H(u,v)=(\\gamma_H-\\gamma_{L})\\left [1-e^{-\\frac{D(u,v)^2}{2D_0^2}}\\right ]+\\gamma_L\\tag{5}\\]选择性滤波器\\[\\begin{cases}H(u,v)=\\frac{1}{1+\\left [\\frac{DW}{D^2+D_0^2}\\right ]^{2n}}\\\\H(u,v)=1-e^{-\\left [\\frac{D^2-D_0^2}{DW}\\right ]^2}\\end{cases}\\tag{6}\\]上边为布特沃斯带阻滤波器，下边是高斯带阻滤波器，W是带宽。用1减去上述公式即得相应的带通滤波器。陷波滤波器\\[\\begin{cases}H_{NR}(u,v)=\\prod_{k=1}^3\\left [\\frac{1}{1+\\left [\\frac{D_{0k}}{D_k(u,v)}\\right ]^{2n}}\\right ]\\left [\\frac{1}{1+\\left [\\frac{D_{0k}}{D_{-k}(u,v)}\\right ]^{2n}}\\right ]\\\\D_k(u,v)=\\sqrt{(u-0.5M-u_k)^2+(v-0.5N-v_k)^2}\\\\D_{-k}(u,v)=\\sqrt{(u-0.5M+u_k)^2+(v-0.5N+v_k)^2}\\end{cases}\\tag{7}\\]$H_{NR}(u,v)$为一个包含了3个陷波对（考虑频域的对称关系，一个中心位于(u,v)的陷波在(-u,-v)上有一个对应的陷波）的** n 阶布特沃斯陷波带阻滤波器**。其中$D_{0k}$为每组陷波对的截止频率，$(u_k,v_k)$为陷波中心位置。类似的，$1-H_{NR}(u,v)$为对应的带通滤波器。图2-1. 1个陷波对的2阶布特沃斯陷波带阻滤波上图是使用1个陷波对的2阶布特沃斯陷波带阻滤波的部分效果。左上是原图，由于莫尔效应，上面包含了具有周期性的网状噪声（在空间域上很难消除）左下是原图的频谱，可以看到网状噪声反映在频谱上是关于中心对称的多组亮点（每组包含4个亮点，对应1个陷波对，这里仅对最亮的一组做处理）。显然，只需我们的频域滤波器在这些亮点的部分取低值就可压制噪声。上图中间为当陷波中心取15，截止频率取20的效果，可以看到有4个代表陷波的黑点。上图右边继续调整陷波中心（29），直至其覆盖4个亮点，增大截止频率（149）以扩大黑点半径，可以看到，经过滤波后图像的网状噪声得到有效抑制。代码FFT的实现网上有很多，这里参考了Paul Bourke的实现顺便贴个OpenCV的例程（opencv/samples/cpp/dft.cpp）    Mat padded;                                //expand input image to optimal size\tint m = getOptimalDFTSize(in.rows);\tint n = getOptimalDFTSize(in.cols); \t// on the border add zero values\tcopyMakeBorder(in, padded, 0, m - in.rows, 0, n - in.cols, BORDER_CONSTANT, Scalar::all(0));\tMat planes[] = { Mat_&lt;float&gt;(padded), Mat::zeros(padded.size(), CV_32F) };\tMat complexI;\tmerge(planes, 2, complexI);         // Add to the expanded another plane with zeros\tdft(complexI, complexI);            // this way the result may fit in the source matrix\t// compute the magnitude and switch to logarithmic scale\t// =&gt; log(1 + sqrt(Re(DFT(I))^2 + Im(DFT(I))^2))\tsplit(complexI, planes);                   // planes[0] = Re(DFT(I), planes[1] = Im(DFT(I))\tmagnitude(planes[0], planes[1], planes[0]);// planes[0] = sqrt(planes[0]^2 + planes[1]^2)\tMat magI = planes[0];\tmagI += Scalar::all(1);                    // switch to logarithmic scale\tlog(magI, magI);\t// crop the spectrum, if it has an odd number of rows or columns\tmagI = magI(Rect(0, 0, magI.cols &amp; -2, magI.rows &amp; -2));\t// rearrange the quadrants of Fourier image  so that the origin is at the image center\tint cx = magI.cols / 2;\tint cy = magI.rows / 2;\tMat q0(magI, Rect(0, 0, cx, cy));   // Top-Left - Create a ROI per quadrant\tMat q1(magI, Rect(cx, 0, cx, cy));  // Top-Right\tMat q2(magI, Rect(0, cy, cx, cy));  // Bottom-Left\tMat q3(magI, Rect(cx, cy, cx, cy)); // Bottom-Right\tMat tmp;                           // swap quadrants (Top-Left with Bottom-Right)\tq0.copyTo(tmp);\tq3.copyTo(q0);\ttmp.copyTo(q3);\tq1.copyTo(tmp);                    // swap quadrant (Top-Right with Bottom-Left)\tq2.copyTo(q1);\ttmp.copyTo(q2);\tnormalize(magI, magI, 0, 1, CV_MINMAX); // Transform the matrix with float values into a\t// viewable image form (float between values 0 and 1).\tmagI.copyTo(out);参考冈萨雷斯 《数字图像处理》"
  },
  
  {
    "title": "空间滤波&频域滤波(1)",
    "url": "/posts/%E7%A9%BA%E9%97%B4%E6%BB%A4%E6%B3%A2&%E9%A2%91%E5%9F%9F%E6%BB%A4%E6%B3%A2(1)/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-01-24 00:00:00 +0000",
    





    
    "snippet": "空间滤波空间滤波器由一个邻域小窗口（奇数边长），对邻域内的像素执行预定义操作（线性情况则为点乘求和），最后的结果赋给邻域中心的像素。空间滤波器主要可分为平滑空间滤波和锐化空间滤波两种      平滑空间滤波有均值滤波、中值滤波等方法，中值滤波为非线性方法，对椒盐噪声有较好效果        锐化空间滤波有一阶微分、二阶微分（见《图像局部不变性特征与描述》阅读笔记（3）– 点与边缘检测）、非锐...",
    "content": "空间滤波空间滤波器由一个邻域小窗口（奇数边长），对邻域内的像素执行预定义操作（线性情况则为点乘求和），最后的结果赋给邻域中心的像素。空间滤波器主要可分为平滑空间滤波和锐化空间滤波两种      平滑空间滤波有均值滤波、中值滤波等方法，中值滤波为非线性方法，对椒盐噪声有较好效果        锐化空间滤波有一阶微分、二阶微分（见《图像局部不变性特征与描述》阅读笔记（3）– 点与边缘检测）、非锐化掩蔽  一二阶微分的基本公式：\\[g(x,y)=f(x,y)+c[\\nabla^2f(x,y)]\\]非锐化掩蔽：模糊原图像 -&gt; 从原图中减去模糊图像（结果成为模板） -&gt; 将模板加在原图上\\[g_{mask}(x,y)=f(x,y)-\\hat{f}(x,y) \\rightarrow g(x,y)=f(x,y)+k*g_{mask}(x,y)\\]一个混合使用两种方法的案例：需要对一幅噪声较大，整体偏暗的图像做细节增强考虑到1阶算子对边缘响应比2阶好，而2阶算子对细节响应比1阶好（但同时会带来大量噪声）；对原图平滑后做1阶（Sobel）锐化，将其作为模板（对边缘响应强，对平坦区域响应弱），再对原图做2阶（Laplace）锐化，然后将其与模板相乘以增强细节，最后将结果加在原图上并做幂律变换得到增强的图像。使用模糊集合做灰度变换与空间滤波模糊集合就不说了，很基础，网上一搜一大把灰度变换      以对比度增强为例，首先需要制定模糊规则：IF 一个像素是暗的，THEN  使它较暗IF 一个像素是灰的，THEN  使它仍为灰IF 一个像素是亮的，THEN  使它较亮        然后需要对模糊规则内的模糊变量指定隶属度函数:    为每个模糊变量指定一个标准输出值:如 $v_d = 0, v_g = 127, v_b = 255$这样对于一个像素可用如下公式求得模糊输出\\[v_0=\\frac{\\mu_{dark}(z_0)*v_d+\\mu_{gray}(z_0)*v_g+\\mu_{bright}(z_0)*v_b}{\\mu_{dark}(z_0)+\\mu_{gray}(z_0)+\\mu_{bright}(z_0)}\\]空间滤波以一个3*3的邻域为例，此时邻域如图2-1所示图2-1. 邻域，$d_i = z_i - z_5$      制定模糊规则：IF d2是0 AND d6是0 THEN z5是白色IF d6是0 AND d8是0 THEN z5是白色IF d8是0 AND d4是0 THEN z5是白色IF d4是0 AND d2是0 THEN z5是白色                                ELSE z5是黑色        以ZE、WH、BL分别表示模糊变量0、白色、黑色的隶属度函数  白色和黑色的标准输出值为255, 0类似灰度变换介绍的输出公式计算可得结果"
  },
  
  {
    "title": "基本的灰度变换方法",
    "url": "/posts/%E5%9F%BA%E6%9C%AC%E7%9A%84%E7%81%B0%E5%BA%A6%E5%8F%98%E6%8D%A2%E6%96%B9%E6%B3%95/",
    "categories": "图像处理",
    "tags": "",
    "date": "2016-01-14 00:00:00 +0000",
    





    
    "snippet": "图像反转对灰度范围为[0, L-1]的图像，其反转图像为 d = L-1-s。反转后的图像就是底片效果，在一些情况下有利于图像分析对数变换基本式$d=c*log(1+s)$与指数变换$d=c*(e^s-1)$相对应，可以实现像素值的扩展和压缩（图2-1）图2-1. 对数变换和指数变换对数变换主要扩展低像素值的部分，指数变换主要压缩低像素值部分但指数变换很容易溢出，相较于对、值数变换，伽马（幂...",
    "content": "图像反转对灰度范围为[0, L-1]的图像，其反转图像为 d = L-1-s。反转后的图像就是底片效果，在一些情况下有利于图像分析对数变换基本式$d=c*log(1+s)$与指数变换$d=c*(e^s-1)$相对应，可以实现像素值的扩展和压缩（图2-1）图2-1. 对数变换和指数变换对数变换主要扩展低像素值的部分，指数变换主要压缩低像素值部分但指数变换很容易溢出，相较于对、值数变换，伽马（幂律）变换更常用。伽马（幂律）变换基本式$d=c*s^\\gamma$图3-1为伽马变换中$\\gamma$取不同值（c=1）的曲线，可以实现对指数变换的效果图3-1. 伽马变换（摘自《数字图像处理》）CRT显示器的伽马校正就是幂律变换，之所以需要校正是因为对CRT，电压和显示亮度之间不是线性关系而是幂律关系。分段线性变换对比度拉伸对选定的灰度范围做线性拉伸，范围外的灰度做线性压缩（极端的是将它们二值化）图4-1. 对比度拉伸（摘自《数字图像处理》），更通用的做法是s1=0, s2=L-1灰度级分层高亮某个灰度范围的值，范围外的值或者置0（图4-2左），或者不变（图4-2右）图4-2. 灰度级分层（摘自《数字图像处理》）比特平面分层以8位图像为例，将其按从低位到高位分成8幅二值图图4-3. 8位图像从低到高每一位生成的二值图可以看到，低位反映了图像细节信息，高位反映了图像轮廓信息，取其中的某几张相加可以获得质量更好的图像。显然，比特位分层可以用于图像压缩（有损）"
  },
  
  {
    "title": "《图像局部不变性特征与描述》阅读笔记（5End）-- 特征点匹配、各种检测算子的性能评估",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E7%89%B9%E5%BE%81%E4%B8%8E%E6%8F%8F%E8%BF%B0-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-5End-%E7%89%B9%E5%BE%81%E7%82%B9%E5%8C%B9%E9%85%8D-%E5%90%84%E7%A7%8D%E6%A3%80%E6%B5%8B%E7%AE%97%E5%AD%90%E7%9A%84%E6%80%A7%E8%83%BD%E8%AF%84%E4%BC%B0/",
    "categories": "图像处理, 《图像局部不变性特征与描述》阅读笔记",
    "tags": "《图像局部不变性特征与描述》阅读笔记",
    "date": "2016-01-07 00:00:00 +0000",
    





    
    "snippet": "特征点匹配算法以SIFT算法为例，当对两幅图像提取了SIFT特征向量后，需要对其建立一一的对应关系。这里就涉及到两个步骤：特征向量的最近邻搜索、匹配对的提纯特征向量的最近邻搜索对一幅图像的每个SIFT特征向量，搜索另一幅图像中与之最相邻的特征向量（欧式距离最短）这一步实际上就是K近邻搜索，考虑到特征点数目一般较大，需要使用快速K近邻搜索算法，文章K近邻快速算法 – KD树、BBF改进算法介绍...",
    "content": "特征点匹配算法以SIFT算法为例，当对两幅图像提取了SIFT特征向量后，需要对其建立一一的对应关系。这里就涉及到两个步骤：特征向量的最近邻搜索、匹配对的提纯特征向量的最近邻搜索对一幅图像的每个SIFT特征向量，搜索另一幅图像中与之最相邻的特征向量（欧式距离最短）这一步实际上就是K近邻搜索，考虑到特征点数目一般较大，需要使用快速K近邻搜索算法，文章K近邻快速算法 – KD树、BBF改进算法介绍的KD树及BBF算法就是SIFT采用的方案。当特征维数较高时，原始的KD树搜索算法性能逼近穷举法，需要考虑BBF算法。当特征数目较少时，KD树方法较穷举法提升不大，此时KD树构建的用时比例占大头；如果应用中基准图是固定的（意味着特征点不变），可以提前离线构建KD树。由于KD树在回溯步骤耗时较大，本章还简要介绍了一种改进方案Spill树：其通过冗余分割方式（两个子空间有一部分是重叠的），减少了相当部分的回溯搜索。该步骤对每个特征向量都会找到最近的匹配特征，但在实际中往往存在某些特征向量在另一幅图中不存在的情况。因此需要对这些匹配对进行筛选，即匹配对提纯。匹配对的提纯匹配对提纯有两种方案，一般将这两种串联起来一起用  比值提纯法思路很简单，需要在最近邻搜索时保存2个近邻（就是2近邻咯）– 最近邻和次近邻。一般认为，只有当待匹配点与其最近邻的距离比其与次近邻的距离小的比较多时，该匹配可信（唯一性更强）Lowe的统计表明，当最近邻/次近邻&lt;0.8时匹配正确率较高；书里还给了另一个公式：最近邻&gt;=0.49*次近邻（个人感觉有问题，最近邻和次近邻相等时也显然满足）  一致性提纯法基本思想是：首先通过初始匹配对（肯定是冗余的）得到一个较精确的图像透视变换模型（待匹配的两幅图像是一个透视变换的关系），然后逐个检查每个匹配对，如果在给定阈值范围内不满足这个变换模型则认为是错误的匹配，应该过滤掉令X和X’为具有透视变换关系的两幅图，H为透视矩阵，有\\[X'\\sim HX=\\begin{bmatrix}h_1 &amp; h_2 &amp; h_3\\\\ h_4 &amp; h_5 &amp; h_6\\\\ h_7 &amp; h_8 &amp; h_9\\end{bmatrix}\\]令(x,y)和(x’,y’)为其中的点，有$x’=\\frac{h_0x+h_1y+h_2}{h_6x+h_7y+h_8}$，$y’=\\frac{h_3x+h_4y+h_5}{h_6x+h_7y+h_8}$, 这9个参数就是上面提到的需要拟合的变换模型。考虑到噪声对拟合效果的影响，这里没有用常见的最小二乘，而是RANSAC算法：以直线拟合为例，因为两个点可确定一条直线，RANSAC首先选取2个点，得到一条直线，再计算其他所有点到这条直线的距离，统计距离在给定阈值内的点数（范围内的点叫内点，范围外的点叫外点），最后选取内点数最多的那一条直线（最多$C_n^2$条）作为最终模型，再次计算所有点到该直线的距离，过滤外点得到提纯结果。匹配算子的性能比较章节从尺度变换、仿射变换、光照变化和图像模糊4个方面对SIFT、SURF（特征点检测）；Harris-Affine、MSER、IBR、EBR（区域检测）等算子进行了实验对比。特征点检测对于尺度缩放，SIFT和SURF均具有一定鲁棒性，且相对于纹理图像，两个算法在结构型图像中性能更好（纹理有重复）对于仿射变换，SIFT和SURF具有一定鲁棒性，整体效果SIFT稍微好于SURF对于光照变化，SIFT和SURF具有一定鲁棒性，两者性能基本持平对于图像模糊，SIFT和SURF具有一定鲁棒性，两者性能基本持平区域检测对于尺度缩放、仿射变换、光照变化，MSER均有理想表现（尺度缩放中Hessian-Affine优于MSER，其他情况MSER为最优）对于图像模糊，其他算子都较稳定，但MSER性能迅速变差总的来说，特征点用SIFT或SURF，区域用MSER比较理想"
  },
  
  {
    "title": "《图像局部不变性特征与描述》阅读笔记（4）-- 快速斑点检测、区域检测",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E7%89%B9%E5%BE%81%E4%B8%8E%E6%8F%8F%E8%BF%B0-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-4-%E5%BF%AB%E9%80%9F%E6%96%91%E7%82%B9%E6%A3%80%E6%B5%8B-%E5%8C%BA%E5%9F%9F%E6%A3%80%E6%B5%8B/",
    "categories": "图像处理, 《图像局部不变性特征与描述》阅读笔记",
    "tags": "《图像局部不变性特征与描述》阅读笔记",
    "date": "2015-12-26 00:00:00 +0000",
    





    
    "snippet": "快速斑点检测算法本章主要介绍了两个算法：SIFT（SIFT特征检测）和SURF（SURF特征检测），它们分别是基于LOG和DOH的改进算法。LOG和DOH都需要对高斯函数做二阶微分以生成卷积核，然后对图像上每个点做卷积以得到局部响应极值为了节省卷积运算带来的时间消耗，SIFT对两个相邻高斯尺度空间的图像做差，得到一个DOG的响应图像，搜索该响应的局部极值点就是待选位置；SURF引入积分图像来...",
    "content": "快速斑点检测算法本章主要介绍了两个算法：SIFT（SIFT特征检测）和SURF（SURF特征检测），它们分别是基于LOG和DOH的改进算法。LOG和DOH都需要对高斯函数做二阶微分以生成卷积核，然后对图像上每个点做卷积以得到局部响应极值为了节省卷积运算带来的时间消耗，SIFT对两个相邻高斯尺度空间的图像做差，得到一个DOG的响应图像，搜索该响应的局部极值点就是待选位置；SURF引入积分图像来近似Hessian矩阵行列式，比SIFT效率更高此外，SIFT和SURF还将特征点的方向特征加入特征向量内（SIFT使用梯度直方图的方法统计方向，SURF使用Haar模板计算方向），使得两者具备了旋转不变性。由于SIFT和SURF具有的高效、尺度不变、旋转不变性，一般用做图像间的特征匹配，SIFT代码中使用了BBF算法进行查找（书里将SIFT、SURF算法描述，特征描述和特征匹配分成了3章）区域检测算法区域检测主要是为了实现区域分割，相较于特征点，其更稳定，更易实现仿射不变性通常区域检测分3个步骤：检测具有同类性质的区域并进行连接和标记；将检测到的非规则区域近似成一个椭圆区域（便于特征描述）；对椭圆区域进行仿射归一化处理（仿射变换为圆形）本章重点分析了最大稳定极值区域（MSER）算法，简要介绍了基于边缘区域（EBR）、基于密度极值区域（IBR）和显著性区域（SR，使用信息熵）MSER算法基本思想很简单：使用一个从0到255的阈值对图像二值化（类似分水岭），过程中一定范围内面积保持稳定的区域即为所求。这个算法的重点是实现技巧。椭圆区域仿射归一化这一步，我的理解是：无论是哪个角度的图像，得到的椭圆区域，都将其仿射并缩放成同一尺寸的圆形，这样在一定程度上实现了仿射不变性下面是椭圆到圆的仿射实现，对于一个标准椭圆$\\frac{x^2}{a^2}+\\frac{y^2}{b^2}=1$，如果令$x’=x, y’=\\frac{a}{b}y$（相当于y方向做缩放），则变为圆$x^2+y^2=a^2$，注意标准椭圆的长短轴是分别平行于XY轴的，所以对一般椭圆，还需做旋转void main(){\tcv::Mat img = cv::imread(\"../file/spots.jpg\");\t\t//截取椭圆区域\tcv::Mat ellipImg;\tfloat sita = 30;\tint a = img.cols/2;\tint b = img.rows/4;\tgetEllipse(img, ellipImg, a/2, b/2, sita);\t//旋转pi-sita\tcv::Mat rotateImg(img.size(), img.type(), cv::Scalar::all(0));\trotate_(ellipImg, rotateImg, sita);\t//调整b轴\tcv::Mat affImg(img.size(), img.type(), cv::Scalar::all(0));\taffine_(rotateImg, affImg, a, b);\t//反向旋转pi-sita\tcv::Mat affineImg(img.size(), img.type(), cv::Scalar::all(0));\trotate_(affImg, affineImg, 180-sita);\tcv::namedWindow(\"source\", 0);\tcv::namedWindow(\"step1\", 0);\tcv::namedWindow(\"step2\", 0);\tcv::namedWindow(\"step3\", 0);\tcv::namedWindow(\"dst\", 0);\tcv::imshow(\"source\", img);\tcv::imshow(\"step1\", ellipImg);\tcv::imshow(\"step2\", rotateImg);\tcv::imshow(\"step3\", affImg);\tcv::imshow(\"dst\", affineImg);\tcv::waitKey(0);}void getEllipse(cv::Mat src, cv::Mat&amp; dst, int a, int b, float sita){\tcv::Point center(src.cols/2, src.rows/2);\tdst = cv::Mat(src.size(), src.type(), cv::Scalar::all(0));\tfor (int i = 0; i &lt; dst.rows; i++)\t{\t\tuchar* ptr = dst.ptr&lt;uchar&gt;(i);\t\tfor (int j = 0; j &lt; dst.cols; j++)\t\t{\t\t\tcv::Point tmp(j-center.x, i-center.y);\t\t\tcv::Point realP;\t\t\t// 因为图像坐标系转成的坐标系左下角才是正（对应笛卡尔左上角的第一象限）\t\t\t// 所以正负关系与笛卡尔下的相反，下同\t\t\trealP.x = tmp.x*cos(sita*PI / 180) - tmp.y*sin(sita*PI / 180);\t\t\trealP.y = tmp.y*cos(sita*PI / 180) + tmp.x*sin(sita*PI / 180);\t\t\tfloat value = float(realP.x*realP.x) / (a*a) + float(realP.y*realP.y) / (b*b);\t\t\tif (value &lt;= 1)\t\t\t{\t\t\t\tptr[3 * j] = src.at&lt;cv::Vec3b&gt;(i, j)[0];\t\t\t\tptr[3 * j + 1] = src.at&lt;cv::Vec3b&gt;(i, j)[1];\t\t\t\tptr[3 * j + 2] = src.at&lt;cv::Vec3b&gt;(i, j)[2];\t\t\t}\t\t}\t}}void rotate_(cv::Mat src, cv::Mat&amp; dst, float sita){\tfor (int i = 0; i &lt; dst.rows; i++)\t{\t\tuchar* ptr = dst.ptr&lt;uchar&gt;(i);\t\tfor (int j = 0; j &lt; dst.cols; j++)\t\t{\t\t\tcv::Point tmp(j - dst.cols / 2, i - dst.rows/2);\t\t\tcv::Point srcP;\t\t\tsrcP.x = int(tmp.x*sin(sita*PI / 180) - tmp.y*cos(sita*PI / 180) + 0.5);\t\t\tsrcP.y = int(tmp.y*sin(sita*PI / 180) + tmp.x*cos(sita*PI / 180) + 0.5);\t\t\tsrcP.x += src.cols / 2;\t\t\tsrcP.y += src.rows / 2;\t\t\tif ((srcP.x &gt;= 0 &amp;&amp; srcP.x &lt; src.cols) &amp;&amp;\t\t\t\t(srcP.y &gt;= 0 &amp;&amp; srcP.y &lt; src.rows))\t\t\t{\t\t\t\tptr[3 * j] = src.at&lt;cv::Vec3b&gt;(srcP)[0];\t\t\t\tptr[3 * j + 1] = src.at&lt;cv::Vec3b&gt;(srcP)[1];\t\t\t\tptr[3 * j + 2] = src.at&lt;cv::Vec3b&gt;(srcP)[2];\t\t\t}\t\t}\t}}void affine_(cv::Mat src, cv::Mat&amp; dst, int a, int b){\tfloat rate = b / float(a);\tfor (int i = 0; i &lt; dst.rows; i++)\t{\t\tuchar* ptr = dst.ptr&lt;uchar&gt;(i);\t\tfor (int j = 0; j &lt; dst.cols; j++)\t\t{\t\t\tcv::Point srcP;\t\t\tsrcP.y = i;\t\t\tsrcP.x = int(rate*(j-src.cols/2) + 0.5);\t\t\tsrcP.x += src.cols / 2;\t\t\tif ((srcP.x &gt;= 0 &amp;&amp; srcP.x &lt; src.cols) &amp;&amp;\t\t\t\t(srcP.y &gt;= 0 &amp;&amp; srcP.y &lt; src.rows))\t\t\t{\t\t\t\tptr[3 * j] = src.at&lt;cv::Vec3b&gt;(srcP)[0];\t\t\t\tptr[3 * j + 1] = src.at&lt;cv::Vec3b&gt;(srcP)[1];\t\t\t\tptr[3 * j + 2] = src.at&lt;cv::Vec3b&gt;(srcP)[2];\t\t\t}\t\t}\t}}特征描述方法重点介绍了SIFT和SURF的特征组织方法（详细分析见相应算法文章），实验表明SURF比SIFT的特征描述更好:  一是前者维数为444=64，小于后者（一般为448=128），匹配更快速；  二是前者是基于模板求和得到的梯度方向，而后者是基于单个像素统计的梯度，抗噪能力更好。PCA-SIFT特征描述法：通过对同类图像的学习（PCA主成分分析），得到一个投影矩阵，对原始的SIFT特征进行维数约减GLOH特征描述法：将SIFT中4*4的子块改成一组同心圆，将其按方向划分成不同的区块（图3-1），统计每个区块的梯度方向，然后使用类似PCA-SIFT的方法，对特征进行维数约减旋转图像（Spin-image）特征描述法：将区域描述为二维直方图，横轴为该点距中心的距离，纵轴为该点的亮度（这样就具有了旋转不变性）"
  },
  
  {
    "title": "SIFT特征检测",
    "url": "/posts/SIFT%E7%89%B9%E5%BE%81%E6%A3%80%E6%B5%8B/",
    "categories": "图像处理",
    "tags": "",
    "date": "2015-12-09 00:00:00 +0000",
    





    
    "snippet": "Lowe提出的SIFT（Scale Invariant Feature Transform）是计算机视觉里影响力非常大的一个算法，其使用LOG的近似算法DOG（Difference of Gaussians）实现快速特征检测，然后对待选点进行位置的精调和筛选，最后考察其邻域，获得特征的矢量性描述。SIFT特征效率高（优化后能达到实时的效果），具有尺度不变性、旋转不变性、部分的仿射不变性。理论...",
    "content": "Lowe提出的SIFT（Scale Invariant Feature Transform）是计算机视觉里影响力非常大的一个算法，其使用LOG的近似算法DOG（Difference of Gaussians）实现快速特征检测，然后对待选点进行位置的精调和筛选，最后考察其邻域，获得特征的矢量性描述。SIFT特征效率高（优化后能达到实时的效果），具有尺度不变性、旋转不变性、部分的仿射不变性。理论总的来说，SIFT主要分为以下4个步骤1：（1）尺度空间极值检测：搜索所有尺度上的图像位置。通过高斯微分函数（近似）来识别潜在的对于尺度和旋转不变的兴趣点。（2）关键点的定位：在每个候选的位置上，通过一个拟合精细的模型来确定位置和尺度。关键点的选择依据于它们的稳定程度。（3）方向的确定：基于图像局部的梯度方向，分配给每个关键点位置一个或多个方向。所有后面的对图像数据的操作都相对于关键点的方向、尺度和位置进行变换，从而提供对于这些变换的不变性。（4）关键点描述：在每个关键点周围的邻域内，在选定的尺度上测量图像局部的梯度。这些梯度被变换成一种表示，这种表示允许比较大的局部形状的变形和光照变化。尺度空间极值检测出发点与LOG一样，为了使特征具有尺度不变性。SIFT首先建立图像金字塔：将金字塔分成O组，每一组称为一个Octave；每一个Octave又分成S层，每层的图像是由不同方差的高斯滤波器滤波的结果（越往上图像越模糊）；每一个Octave的底层图像由上一个Octave的第S层图像高宽下2采样得到（缩小1/4）；然后对高斯塔相邻两层做差得到DOG响应金字塔（是对LOG的近似）图1-1. 图像金字塔如图1-1:  由图片大小决定Octave的数目（Rob Hess代码默认当图片缩小到4个像素则停止），每Octave的层数设为3-5层（代码默认3）；  对于第 i 个（i=1…）Octave的第 n 层（n=1…S）图片，其对应的高斯尺度为$2^{i-1}k^{n-1}\\sigma$，其中，$\\sigma$一般预设为1.6；  在实际中，为了保证上一组Octave与当前Octave的连续性，需要对每Octave的顶层再加3层，也就是实际上每一Octave的层数为S+3（假设S=3，对第1个Octave，gauss塔尺度为$\\sigma,k\\sigma,k^2\\sigma$ ， DoG塔尺度为$\\sigma,k\\sigma$，而DOG空间中的极值比较需要3层，因此考虑加上3层，这样第1个Octave的DOG空间尺度为$\\sigma,k\\sigma,k^2\\sigma,k^3\\sigma,k^4\\sigma$，第2个Octave的DOG空间尺度为 $2\\sigma,2k\\sigma,2k^2\\sigma,2k^3\\sigma,2k^4\\sigma$，进行极值比较时底层和顶层无法计算，由于$2k^3\\sigma=2k\\sigma$实现了尺度变化的连续性）SIFT尺度空间的极值检测在DOG塔里进行，对每个点，比较其相邻的26个点，若为最大或最小值则记为待选极值点通常需要将原始图像做高斯平滑（$\\sigma=0.5$），考虑到因此损失的高频信息，第1Octave的第1层为原始图像resize 2倍再平滑的结果，这样建立高斯塔的高斯尺度为$\\sigma(o,s)=\\sqrt{\\sigma(o,s)^2-\\sigma_n^2}$，其中$\\sigma_n=2*0.5$关键点的定位      对于DOG金字塔的每个点，（Rob Hess代码里先经过一次对比度阈值筛选）检测其邻近26个点，如果为极值则记为待选点；        以上极值点是在离散空间中搜索的，SIFT对每个待选点，通过插值得到极值点再连续空间中的精确位置（包括 行，列，尺度）：令$x=[x\\ y\\ o]^T$，$f(x)$为该向量在DOG空间中的响应，将其泰勒展开，有  \\[f(X)=f(0)+\\frac{\\partial f}{\\partial X}X+\\frac{1}{2}X\\frac{\\partial^2 f}{\\partial X^2}X^T\\]其中\\(\\frac{\\partial f}{\\partial X}=[\\frac{\\partial f}{\\partial x}\\ \\frac{\\partial f}{\\partial y}\\ \\frac{\\partial f}{\\partial z}]^T\\)\\[\\frac{\\partial^2 f}{\\partial X^2}=\\left[\\begin{matrix}\\frac{\\partial^2f}{\\partial x^2} &amp; \\frac{\\partial^2f}{\\partial x\\partial y} &amp; \\frac{\\partial^2f}{\\partial x\\partial o} \\\\ \\frac{\\partial^2f}{\\partial x\\partial y} &amp;  \\frac{\\partial^2f}{\\partial y^2} &amp; \\frac{\\partial^2f}{\\partial y\\partial o} \\\\ \\frac{\\partial^2f}{\\partial x\\partial o} &amp;  \\frac{\\partial^2f}{\\partial y \\partial o} &amp; \\frac{\\partial^2f}{\\partial o^2}\\end{matrix}\\right]\\]对$f(X)$求导，取其导数为0时候的点，即为所求精确位置距当前位置的偏移向量：$\\hat{X}=-\\frac{\\partial^2f^{-1}}{\\partial X^2}\\frac{\\partial f}{\\partial X}$。当计算得到的偏移向量每一维都小于0.5时认为位置调整完毕。  根据（2）更新的极值点位置及偏移向量，计算对应的DOG响应（理论上f(X)的极值）\\[f(\\hat{X})=f(X)+0.5\\frac{\\partial f}{\\partial X}\\hat{X}\\]如果该响应小于给定阈值（0.03），认为其不够稳定，删除。  过滤可能的边缘点。SIFT认为边缘点不好定位，并且易受噪声影响，因此需要去除。注意到边缘点在横跨边缘的地方有较大曲率，而在垂直边缘的方向有较小曲率。曲率通过DOG响应的Hessian矩阵求得（见DOH分析）：\\[H(x,y)=\\left[\\begin{matrix}D_{xx}(x,y) &amp; D_{xy}(x,y)\\\\ D_{xy}(x,y) &amp; D_{yy}(x,y)\\end{matrix}\\right]\\]其两个特征值反映了x,y两个方向的长度，当两个特征值相等时特征区域为圆形，此时公式$\\frac{tr(H)^2}{det(H)}=\\frac{(\\lambda_1+\\lambda_2)^2}{\\lambda_1\\lambda_2}=\\frac{(\\gamma+1)^2}{\\gamma}$取最小值，其中$\\lambda_1=\\gamma\\lambda_2$。Lowe论文中取r=10，大于该式的特征将被去掉。方向的确定及关键点描述对每个特征点，需要根据公式\\[\\begin{matrix}mag(x,y)&amp;=&amp;\\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}\\\\ ori(x,y)&amp;=&amp;arctan\\left(\\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}\\right)\\end{matrix}\\]计算其方向及该方向的强度。注意L表示在高斯空间的图像金字塔内计算。      为了获得稳定的方向，SIFT考虑当前点邻域内点的方向及强度，乘以高斯分布参数（近似圆形），并将方向离散化为45度（或10度）一柱，用8柱（36柱）的直方图描述邻域内所有点的方向，以此作为该特征点的方向描述（对8柱直方图，如果邻域半径为4，则该特征的方向特征维度为448=128），最后将方向特征向量归一化以排除光照影响，取最大的那个分量作为该关键点的主方向。        在这里一般还会对方向描述直方图做高斯平滑以弥补仿射带来的影响；还可以选取第二大的分量作为关键点的辅助方向。        在最后生成的关键点描述向量（128维）时，还需以其主方向为参考，旋转其它方向，以此来确保SIFT特征的旋转不变性  关于SIFT特征匹配      SIFT特征必有的内容就是匹配不同视角下同一特征，基本思想很简单，分别对两幅图做SIFT检测，一一计算特征向量的欧式距离。        但穷举法效率太低，Rob Hess代码中应用了KD树（K近邻的一种实现）组织特征向量，使用BBF算法进行查询，匹配效率达到实时的效果。  相应的算法分析见JULY博客代码代码实现见大神Rob Hess的github项目，源码分析网上很多，比如masikkk的博客需要说明的是，Rob代码里默认直方图是36柱，这样特征维数应该是4436 = 576，但其宏定义里设置的维度为128，会造成泄露，改下定义就好了贴两张Rob Hess代码得到的效果图（上为SIFT检测效果，下为match效果）：​"
  },
  
  {
    "title": "《图像局部不变性特征与描述》阅读笔记（3）-- 点与边缘检测",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E7%89%B9%E5%BE%81%E4%B8%8E%E6%8F%8F%E8%BF%B0-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-3-%E7%82%B9%E4%B8%8E%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B/",
    "categories": "图像处理, 《图像局部不变性特征与描述》阅读笔记",
    "tags": "《图像局部不变性特征与描述》阅读笔记",
    "date": "2015-12-04 00:00:00 +0000",
    





    
    "snippet": "角点角点是图像亮度发生剧烈变化或图像边缘曲线上曲率极大值的点角点的检测方法主要分为两类：基于图像边缘的方法、基于图像灰度的方法基于图像边缘的方法一般需要先对图像边缘进行编码，因此计算量较大，并且受图像分割和边缘提取的效果影响较大。此类方法前期有Rosenfeld和Freeman的成果，后期有CSS（角点检测相关资源）。但不是现在的主流基于图像灰度的方法该类方法计算点的曲率和梯度来检测角点，是...",
    "content": "角点角点是图像亮度发生剧烈变化或图像边缘曲线上曲率极大值的点角点的检测方法主要分为两类：基于图像边缘的方法、基于图像灰度的方法基于图像边缘的方法一般需要先对图像边缘进行编码，因此计算量较大，并且受图像分割和边缘提取的效果影响较大。此类方法前期有Rosenfeld和Freeman的成果，后期有CSS（角点检测相关资源）。但不是现在的主流基于图像灰度的方法该类方法计算点的曲率和梯度来检测角点，是目前研究的重点。      Moravec算子：最早的角点检测算子，通过计算当前点4（8）个方向的灰度变化来确定角点，计算简单，但不具备旋转不变性（相关步骤及分析见《图像局部不变性特征与描述》阅读笔记（1）– 背景）        Forstner算子：首先使用Robert算子计算各方向的梯度，构成一个协方差矩阵，通过两个阈值筛选角点。Forstner角点计算简单，定位准确，但由于需要指定阈值，受亮度、对比度影响（具体算法参考http://blog.csdn.net/carson2005/article/details/40370901）        Harris算子：将边缘响应构成的二维矩阵视为几何上的椭圆，通过考察特征值（椭圆的长轴短轴）确定角点。具有旋转不变性，对亮度、对比度变化也有很好的鲁棒性（见Harris角点检测（1））        Harris-Laplace算子：在不同尺度空间中检测Harris角点，然后对每个角点，看其在LOG空间中是不是局部极值，如果是，则定为角点（见Harris角点检测（2）– Harris-Laplace）。该算子增加了尺度不变性        Harris-Affine算子：进一步实现了Harris的仿射不变性。Harris-Laplace使用的二阶Laplace矩阵可以处理各向同性的尺度变化（缩放），但对于每个方向缩放比率不同的仿射变换无能为力。Harris-Affine使用非标准高斯核（几何上看是椭圆的）替代Harris-Laplace的标准高斯核（圆形）：$g(\\sum)=\\frac{1}{2\\pi\\sqrt{det\\sum}e^{-\\frac{x^T\\sum^{-1}x}{2}}}$  算法步骤（1）Harris-Laplace算法获得初始角点（2）对每个初始角点，采用如下步骤需要说明的是，Harris-Laplace的计算复杂度已经比较高了，Harris-Affine计算量更大。  SUSAN算子：不再沿用微分的思想，而是通过统计圆形模板内与中心点相似点的个数确定。计算简单，抗噪性好，具有旋转、平移不变性（具体算法见SUSAN角点检测）边缘边缘是像素亮度变化的不连续产生的，是前背景之间的边界。边缘检测的方法都是通过计算图像的微分来实现：利用一阶微分 – Sobel, Prewitt, Kirsch, Canny\\[\\begin{cases}\\frac{\\partial I}{\\partial x}=\\lim\\limits_{\\Delta x\\rightarrow\\infty}\\frac{I(x+\\Delta x,y)-I(x,y)}{\\Delta x}\\approx I(x+1,y)-I(x,y)\\\\\\frac{\\partial I}{\\partial y}=\\lim\\limits_{\\Delta y\\rightarrow\\infty}\\frac{I(x,y+\\Delta y)-I(x,y)}{\\Delta y}\\approx I(x,y+1)-I(x,y)\\end{cases} \\tag{1}\\]转换成矩阵形式：\\[m_x=\\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; -1\\\\ \\end{bmatrix}\\]\\[m_y=\\begin{bmatrix}0 &amp; -1 \\\\ 1 &amp; 0\\\\ \\end{bmatrix}\\]这个就是Roberts边缘检测卷积核对于3*3的模板\\[m_x=\\begin{bmatrix}a_0 &amp; a_1 &amp; a_2 \\\\ a_3 &amp; a_4 &amp; a_5\\\\ a_6 &amp; a_7 &amp; a_8 \\end{bmatrix}\\]有\\[\\begin{cases}\\frac{\\partial I}{\\partial x}=a_2+ca_5+a_8-(a_0+ca_3+a_6)\\\\\\frac{\\partial I}{\\partial y}=a_6+ca_7+a_8-(a_0+ca_1+a_2)\\end{cases}\\]  当c=1时，有\\[m_x=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\\\ -1 &amp; 0 &amp; 1\\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}\\]\\[m_y=\\begin{bmatrix}-1 &amp; -1 &amp; -1 \\\\ 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 \\end{bmatrix}\\]这个就是Prewitt边缘检测卷积核  当c=2时，有\\[m_x=\\begin{bmatrix}-1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2\\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}\\]\\[m_y=\\begin{bmatrix}-1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0\\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix}\\]这个就是Sobel边缘检测卷积核Canny边缘检测算法使用Sobel卷积核，是边缘检测算法中的代表性方法（具体分析见Canny边缘检测）利用二阶微分 – Marr, Lindeberg\\[\\begin{cases}\\frac{\\partial^2 I}{\\partial x^2}=I(x+1,y)-2I(x,y)+I(x-1,y)\\\\\\frac{\\partial^2 I}{\\partial y^2}=I(x,y+1)-2I(x,y)+I(x,y-1)\\end{cases} \\tag{2}\\]因此，有$\\nabla^2I=\\frac{\\partial^2 I}{\\partial x^2}+\\frac{\\partial^2 I}{\\partial y^2}=-4I(x,y)+I(x+1,y)+I(x-1,y)+I(x,y+1)+I(x,y-1)$二阶微分对噪声较为敏感LOG算法使用高斯分布来平滑和突出阶跃点"
  },
  
  {
    "title": "《图像局部不变性特征与描述》阅读笔记（2）-- 图像尺度空间",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E7%89%B9%E5%BE%81%E4%B8%8E%E6%8F%8F%E8%BF%B0-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-2-%E5%9B%BE%E5%83%8F%E5%B0%BA%E5%BA%A6%E7%A9%BA%E9%97%B4/",
    "categories": "图像处理, 《图像局部不变性特征与描述》阅读笔记",
    "tags": "《图像局部不变性特征与描述》阅读笔记",
    "date": "2015-11-27 00:00:00 +0000",
    





    
    "snippet": "对于未知场景，计算机没办法预知待检目标的尺度。因此需要对图像在不同尺度下进行描述总的来说，目前有两种尺度空间描述体系：金字塔多分辨率、高斯尺度空间金字塔多分辨率金字塔是早期图像处理领域多尺度描述的主要方法。将经过低通滤波器的图像按隔行、隔列采样，得到一组空间尺寸以1/4的比率逐步降低的集合虽然金字塔化的计算复杂度低，但缺乏坚实的理论依据，不能获得图像中目标的尺度（除非知道当前的层数，然后逆推...",
    "content": "对于未知场景，计算机没办法预知待检目标的尺度。因此需要对图像在不同尺度下进行描述总的来说，目前有两种尺度空间描述体系：金字塔多分辨率、高斯尺度空间金字塔多分辨率金字塔是早期图像处理领域多尺度描述的主要方法。将经过低通滤波器的图像按隔行、隔列采样，得到一组空间尺寸以1/4的比率逐步降低的集合虽然金字塔化的计算复杂度低，但缺乏坚实的理论依据，不能获得图像中目标的尺度（除非知道当前的层数，然后逆推，否则不知道当前目标的真实尺寸）高斯尺度空间高斯尺度空间使用一组宽度递增的参数（高斯函数的方差）生成的高斯滤波器对原始信号进行滤波得到。形成的信号集是原始信号不同精细度的表达（方差越大，模糊程度越大，丢失的细节越多），从空间尺度上看这些信号的尺寸保持一致。那么，这种尺度变换能不能用其他的函数呢？Koenderink、Lindeberg和Florack等人用数学形式证明了高斯核是实现尺度变换的唯一变换核，并且其满足平移不变性、半群结构、非增局部极值、尺度不变性和旋转不变性等。其中半群结构指的是：两个高斯核的卷积可以用另一个不同核参数的高斯核表示，也就是说不同的高斯核对信号的平滑是连续的。非增局部极值指的是：信号细节总是越来越少的基于高斯尺度空间的图像局部特征检测斑点检测斑点通常指与周围有灰度差异的区域，相较于仅表示一个像素的角点，斑点更稳定，抗噪能力更强，在图像配准和立体视觉中有广泛应用。斑点检测主要应用不同核参数的二维高斯函数的二阶形式做卷积，当核参数（方差）和某个斑点的尺寸（半径）匹配时，卷积取到极值。这里主要介绍了LOG和DOH两种算法做斑点检测，在文章 (LOG)Laplacian of Guassian &amp; (DOH)Determinant of Hessian 斑点检测中有详细介绍和具体实现书的第4章介绍了两个高效的斑点（特征）检测算法：SIFT和SURF（分别见文章SIFT特征检测和SURF特征检测）。其中SIFT是基于LOG的近似算法DOG来提取特征点，考察其邻域统计特征的方向，最后结合BBF算法实现快速匹配，是公认最有效最稳定的特征检测算法；SURF采用积分图像的方法近似卷积操作，对DOH进行简化和近似，是SIFT在效率上的改进算法（平均可提升3倍左右）。边缘检测和角点检测边缘从数学上指的是亮度梯度方向上达到最大值的点，也就是该点灰度变化梯度方向的一阶导为0，二阶导小于0角点有两个版本的定义：1.两个边缘的交点；2.邻域内具有两个主方向的特征点角点检测到后往往还需要对其坐标进行精确定位：对当前角点邻域内每个点，求其与梯度方向垂直的直线，理论上所有这些直线会相交于一点，那个点就是当前角点的精确位置。考虑到误差，可以用最小二乘法获得次优的位置。"
  },
  
  {
    "title": "《图像局部不变性特征与描述》阅读笔记（1）-- 背景",
    "url": "/posts/%E5%9B%BE%E5%83%8F%E5%B1%80%E9%83%A8%E4%B8%8D%E5%8F%98%E6%80%A7%E7%89%B9%E5%BE%81%E4%B8%8E%E6%8F%8F%E8%BF%B0-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0-1-%E8%83%8C%E6%99%AF/",
    "categories": "图像处理, 《图像局部不变性特征与描述》阅读笔记",
    "tags": "《图像局部不变性特征与描述》阅读笔记",
    "date": "2015-11-24 00:00:00 +0000",
    





    
    "snippet": "1. 局部特征概念及性质局部特征是从图像局部区域中抽取的特征，可以提供一种具有统计意义的图像内容表述；全局特征（比如颜色直方图）可以描述图像的内容，具有很好的检索性能，但受图像遮挡的影响较大。局部特征最重要是需要满足可重复性：同一场景在不同视角（几何变形）下，特征能保持自身的稳定（也就是不变性：令f为特征提取函数，t为某种几何变换，对于图像x，应有 f(t(x))=f(x)）。此外，定义的局...",
    "content": "1. 局部特征概念及性质局部特征是从图像局部区域中抽取的特征，可以提供一种具有统计意义的图像内容表述；全局特征（比如颜色直方图）可以描述图像的内容，具有很好的检索性能，但受图像遮挡的影响较大。局部特征最重要是需要满足可重复性：同一场景在不同视角（几何变形）下，特征能保持自身的稳定（也就是不变性：令f为特征提取函数，t为某种几何变换，对于图像x，应有 f(t(x))=f(x)）。此外，定义的局部特征应能在图像上检测到足够多的数量，然后通过阈值进行筛选。2. 局部特征的发展图2-1. 局部特征发展历程Moravec角点考察当前点与其4-邻域（8-邻域）像素的相似性，只有当4（8）个方向的相似度都小于给定阈值才认为当前点为角点\\[E(u,v)=\\sum_{x,y}w(x,y)[I(x+u,y+v)-I(x,y)]^2\\]算法步骤（参考这里）：  对每个点，计算E(u,v)。(u,v)取值为(1,0), (1,1), (0,1), (-1,1)，代表4个方向  选取E(u,v)最小的那个值（方向）作为备选（存在一个与原始图相同大小的矩阵M里）  对M中的每个点，计算w窗口内取最大的那个值，如果该值大于阈值，则认为原始图中该坐标的点为特征点；之后M中的这个点平移w窗口大小的位置（与上个窗口不重叠），重复3（3中之所以在窗口内选最大值其实是起到了一定的筛选作用）可以看到，因为只计算了4个方向（最多8个），因此当同一场景旋转一定角度后，之前能检测到的Moravec角点可能就检测不到了（因为不在这几个方向中）。也就是说，Moravec角点不具备旋转不变性，此外，其对噪声也很敏感。因此现在已经被淘汰。Harris角点是Moravec角点的改进算法。针对Moravec角点对方向依赖性强，使用微分算子替代了窗口的移动；使用高斯窗口（模拟圆形窗口）替代Moravec的方形窗口，降低对噪声的敏感度；对E（见Moravec公式）变形为二次型，以区分边缘和角点。相对Moravec角点，Harris角点具备旋转不变性，具备平移不变性，但不具备空间尺度不变性（缩放）Harris-Laplacian角点结合Harris和高斯尺度空间，使角点增加了空间尺度不变性。Harris-Affine角点具有仿射不变性（仿射变换就是将某向量进行旋转、放缩和平移：x-&gt;Ax+b）SIFT（Scale Invariant Feature Transform）特征利用金字塔和高斯滤波差分来快速求解高斯拉普拉斯空间中的极值点，能快速提取具有仿射不变性的特征点SURF（Speeded Up Robust Feature）特征是SIFT的改进，通过积分图像和Harr小波进一步提高了检测速度MSERs（Maximally Stable Extrernal Regions）特征借用分水岭的思路检测灰度最稳定的区域，然后对其进行旋转和尺度的归一化，具有严格的仿射不变性从局部特征点的发展历程可以看到，局部特征的关注点集中在可重复性（各种不变性）。"
  },
  
  {
    "title": "机器学习（2）——KMeans、DBSCAN、决策树",
    "url": "/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-2-KMeans-DBSCAN-%E5%86%B3%E7%AD%96%E6%A0%91/",
    "categories": "机器学习",
    "tags": "",
    "date": "2015-09-10 00:00:00 +0000",
    





    
    "snippet": "KMeans聚类KMeans是最基础的无监督聚类算法，计算思路也比较简单1）标准化待聚类数据2）随机选定K个聚类中心的位置3）计算每个点与K个中心的距离（欧式距离、余弦相似度），选择距离最小的那个中心为该点所属的簇4）计算每个簇中所有点的均值为新的聚类中心，重复3）5）当聚类中心的变化小于给定阈值时停止迭代图1. KMeans工作流程KMeans算法简单，快速，适合常规数据集，但缺点很突出 ...",
    "content": "KMeans聚类KMeans是最基础的无监督聚类算法，计算思路也比较简单1）标准化待聚类数据2）随机选定K个聚类中心的位置3）计算每个点与K个中心的距离（欧式距离、余弦相似度），选择距离最小的那个中心为该点所属的簇4）计算每个簇中所有点的均值为新的聚类中心，重复3）5）当聚类中心的变化小于给定阈值时停止迭代图1. KMeans工作流程KMeans算法简单，快速，适合常规数据集，但缺点很突出  K值难确定  初始位置的不同会造成不同的聚类结果  只适用于可线性分割的数据集图2. KMeans无法分隔任意形状的簇一般为了避免初始聚类中心带来的聚类不稳定性，可以选择重复多次运行KMeans，取其中总体误差（每个点与其聚类中心的距离之和）最小的那次为最终输出DBSCANDBSCAN是基于密度思想的无监督聚类算法，一般情况下比KMeans好用：1）指定搜索半径$r$，搜索圆内数据点最小数目$p$2）随机选择一个初始点，从以该点为圆心$r$为半径搜索落在该区域的其他点，如果总点数大于$p$则认为该区域的所有点是同一个类别，如果初始点的邻域点数小于$p$，则认为该点为离群点3）依次以该区域的所有点为圆心，重复2），直到区域内的点数小于$p$，停止扩张，此时该类别的所有点聚类完成4）从未被访问过的点里随机选择一个初始点，重复2）和3），直到所有点都访问到相比KMeans，DBSCAN有以下优势：  不需要指定聚类类别数K  可以聚类任意形状的簇  擅长找到离群点（可以用于异常数据检测）图3. DBSCAN支持分割任意形状的簇不过DBSCAN引入了另外2个参数，并且对最终结果影响较大，需要不断试验选择最佳的取值此外对于大数据和高维数据，DBSCAN的效率会比较低聚类算法的评估指标由于聚类算法是无监督的，因此没有gt做参照1）Inertia指标计算每个点与其聚类中心的距离，然后将所有距离加起来。好的聚类算法应该使得该累加值最小事实上sklearn的KMeans算法在多次迭代后会根据式1选择最佳的一次聚类结果\\[min\\sum_{i=1}^K\\sum_{x\\in C_i}(c_i,x)^2 \\tag{1}\\]2）轮廓系数\\[s_i=\\frac{b_i-a_i}{max(a_i,b_i)} \\tag{2}\\]其中$a_i$是第$i$个样本与同簇其他样本的平均距离，$b_{ij}$是第$i$个样本与簇$j$中所有样本的平均距离，$b_i$是所有$b_{ij}$的最小值  $s_i$接近1：样本$i$聚类合理  $s_i$接近-1：样本$i$聚类不合理  $s_i$接近0：样本$i$在两个簇的边界上决策树决策树是一种通过二叉树结构进行分类和回归的有监督机器学习算法图4. 决策树示例决策树算法（参考《决策树（decision tree）(一)——构造决策树方法》系列文章）最早是1966年E.B.Hunt等人提出，昆兰1979年提出ID3算法，1993年又提出C4.5改进算法，将决策树研究推向高潮"
  },
  
  {
    "title": "机器学习（1）——线性回归、逻辑回归",
    "url": "/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/",
    "categories": "机器学习",
    "tags": "",
    "date": "2015-07-24 00:00:00 +0000",
    





    
    "snippet": "本系列部分内容参考Andrew机器学习课程和Python机器学习训练营线性回归线性回归的超平面方程如式1\\[h_{\\theta}(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx \\tag{1}\\]对每个样本，有\\[y^{(i)}=\\theta^Tx^{(i)}+\\epsilon^{(i)} \\tag{2}\\]其中误差$\\epsilon^{(i)}$服从高斯分布，代入...",
    "content": "本系列部分内容参考Andrew机器学习课程和Python机器学习训练营线性回归线性回归的超平面方程如式1\\[h_{\\theta}(x)=\\sum_{i=0}^n\\theta_ix_i=\\theta^Tx \\tag{1}\\]对每个样本，有\\[y^{(i)}=\\theta^Tx^{(i)}+\\epsilon^{(i)} \\tag{2}\\]其中误差$\\epsilon^{(i)}$服从高斯分布，代入式2，有\\[p(y^{(i)}|x^{(i)};\\theta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}} \\tag{3}\\]一个好的模型误差对每个样本，应该满足$(y^{(i)}-\\theta^Tx^{(i)})^2$取极小值，也就是式4取极大值\\[L(\\theta)=\\prod_{i=1}^np(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}} \\tag{4}\\]对式4取对数似然，有\\[log(L(\\theta))=\\sum_{i=1}^nlog\\left(\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y^{(i)}-\\theta^Tx^{(i)})^2}{2\\sigma^2}}\\right)\\\\=nlog\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\theta^Tx^{(i)})^2 \\tag{5}\\]显然式5取极大值等价于式6(最小二乘目标函数)取极小值\\[J(\\theta)=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\theta^Tx^{(i)})^2 \\tag{6}\\]事实上对于式6代表的$Ax=b$问题，在满足一定条件（列向量线性无关）下是可以直接求近似解的（《线性代数(3)》）\\[\\hat{\\theta}=(X^TX)^{-1}X^Ty \\tag{7}\\]但实际应用中数据是不可控的，无法保证$X^TX$可逆，因此用迭代优化（梯度下降）的思路求近似值岭回归&amp;lasso回归岭回归和lasso回归都是为了解决线性回归中的过拟合问题容易出现过拟合的数据有2种情况：  数据集特征维度（列数）大于数据量（行数）  数据集特征之间存在较强的线性相关性，也就是说数据集中不同类别的数据分布不均匀lasso回归用于应对第一种情况，岭回归用于应对第二种情况岭回归岭回归通过在代价函数中加入L2正则项使所有维度的系数尽可能平均，其效果是直接求解方程（式7）中加了一个单位矩阵对岭回归的一个直观理解是，其倾向于更加均等的考察所有特征带来的影响，从而避免因为数据的偏向给某些特征更高的权重\\[\\hat{\\theta}=(X^TX+\\lambda I)^{-1}X^Ty \\tag{8}\\]其中$\\lambda$是岭系数证明：\\[\\begin{aligned}J(\\theta)=&amp;\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\theta^Tx^{(i)})^2+\\lambda\\sum_{i=1}^n\\theta_i^2\\\\=&amp;\\frac{1}{2}(\\theta^TX^TX\\theta-\\theta^TX^TY-Y^TX\\theta+Y^TY)+\\lambda\\theta^T\\theta\\end{aligned} \\tag{9}\\]式9对$\\theta$求偏导，有\\[\\frac{\\partial J(\\theta)}{\\partial\\theta}=X^TX\\theta-X^TY+\\lambda\\theta \\tag{10}\\]令式10等于0，得式8lasso回归lasso回归通过在代价函数中加入L1正则项使某些维度的系数变成0来实现特征选择\\[J(\\theta)=\\frac{1}{2}\\sum_{i=1}^n(y^{(i)}-\\theta^Tx^{(i)})^2+\\lambda\\sum_{i=1}^n|\\theta_i| \\tag{11}\\]逻辑回归逻辑回归其实不是回归算法而是二分类算法就是在线性回归之后加了$sigmoid$函数将输出从$(-\\infty,\\infty)$映射到$[0,1]$，从而实现概率输出\\[h_{\\theta}(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}} \\tag{12}\\]其中$\\theta^Tx=\\sum_{i=0}^n\\theta_ix_i$就是线性回归对每个样本，其误差函数为\\[P(y|x;\\theta)=(h_{\\theta}(x))^y(1-h_{\\theta}(x))^{1-y} \\tag{12}\\]其中$y$是该样本的$groundtruth$，取0或1对所有样本，其误差似然函数为\\[L(\\theta)=\\prod_{i=1}^n(h_{\\theta}(x_i))^y_i(1-h_{\\theta}(x_i))^{1-y_i} \\tag{13}\\]相应的可以得到其对数似然函数\\[l(\\theta)=logL(\\theta)=\\sum_{i=1}^n(y_ilogh_{\\theta}(x_i)+(1-y_i)log(1-h_{\\theta}(x_i))) \\tag{14}\\]显然对于式14，我们希望其越大越好，为了方便使用梯度下降，对其取平均后乘以$-1$得到最终的误差表达式\\[J(\\theta)=-\\frac{1}{n}l(\\theta) \\tag{15}\\]式15对$\\theta$求偏导，易得\\[\\frac{\\partial}{\\partial\\theta_j}J(\\theta)=-\\frac{1}{n}\\sum_{i=1}^n(y_i-g(\\theta^Tx_i))x_i^j \\tag{16}\\]多分类逻辑回归对$n$分类问题，可以训练$n$个二分类逻辑回归分类器，将其级联起来组成一个多类别分类器Tricks特征缩放特征归一化是机器学习中经常用到的一种技巧，我以前也经常用，但一直没理解为什么要这么做。假定现在有两个特征（X1、X2），情况1是两个特征的范围几乎一致（图左），情况2是X1的范围远大于X2（图右）图1. 特征缩放如果特征的范围相当，那么求得的梯度方向会更趋向极值方向；反之寻优路线就会复杂很多。因此特征的缩放可以使得GD更快的收敛。特征缩放并不一定要求他们范围一致（相当即可），一般缩放的方法是：\\[x'=\\frac{x-\\bar{x}}{range} \\tag{9}\\]迭代次数一般是通过限定误差下限和指定最大迭代次数来实现Andrew表示，比较靠谱的方法还是实时画出每次迭代的误差曲线，这样可以很直观的看到目前算法的趋势学习率没什么办法，大神也是从 0.001, 0.003, 0.006, … , 1 一个一个试"
  }
  
]

