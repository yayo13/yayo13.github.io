

<feed xmlns="http://www.w3.org/2005/Atom">
  <id>http://localhost:4000/</id>
  <title>zizi7's blog</title>
  <subtitle>A minimal, responsive and feature-rich Jekyll theme for technical writing.</subtitle>
  <updated>2024-06-15T13:45:25+00:00</updated>
  <author>
    <name>zizi7</name>
    <uri>http://localhost:4000/</uri>
  </author>
  <link rel="self" type="application/atom+xml" href="http://localhost:4000/feed.xml"/>
  <link rel="alternate" type="text/html" hreflang="en"
    href="http://localhost:4000/"/>
  <generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator>
  <rights> © 2024 zizi7 </rights>
  <icon>/assets/img/favicons/favicon.ico</icon>
  <logo>/assets/img/favicons/favicon-96x96.png</logo>


  
  <entry>
    <title>【PLM】（7）XLNet</title>
    <link href="http://localhost:4000/posts/PLM-7-XLNet/" rel="alternate" type="text/html" title="【PLM】（7）XLNet" />
    <published>2024-06-11T00:00:00+00:00</published>
  
    <updated>2024-06-15T13:40:01+00:00</updated>
  
    <id>http://localhost:4000/posts/PLM-7-XLNet/</id>
    <content src="http://localhost:4000/posts/PLM-7-XLNet/" />
    <author>
      <name>zizi7</name>
    </author>

  
    
    <category term="LLM" />
    
    <category term="PLM" />
    
  

  <summary>XLNet是对以Bert为代表的自编码模型的改进

如《【PLM】（3）自编码自回归 &amp;amp;amp; GPT-1、2、3模型》所述，无监督语言建模可以分为

  以Bert为代表的自编码（AE）方式
  以GPT为代表的自回归（AR）方式


其中自编码方式拥有双向注意力的优势，但其Masked LM的预训练方式存在两个问题

  预训练阶段使用[MASK]符号做掩码，但fine-tune阶段该掩码不存在，会导致一定偏差
  Bert每次会对句子的15%个token做掩码，每次训练仅考虑未被掩码的内容，会丢失被掩码内容间的相关性知识


自回归方式由于采用生成移位方式进行训练，规避了自编码上述两个问题，但单向注意力的方式以及去噪自编码（DAE）的舍弃（MLM本质上有DAE的效果，对数据做了破坏）限制了其在NLU任务上的表现

因此谷歌联合卡内基在2019年提出XLNet【1】，在自回归训...</summary>

  </entry>

  
  <entry>
    <title>【长文本】（2）Ernie-Doc</title>
    <link href="http://localhost:4000/posts/%E9%95%BF%E6%96%87%E6%9C%AC-2-Ernie-Doc/" rel="alternate" type="text/html" title="【长文本】（2）Ernie-Doc" />
    <published>2024-05-30T00:00:00+00:00</published>
  
    <updated>2024-06-15T13:40:01+00:00</updated>
  
    <id>http://localhost:4000/posts/%E9%95%BF%E6%96%87%E6%9C%AC-2-Ernie-Doc/</id>
    <content src="http://localhost:4000/posts/%E9%95%BF%E6%96%87%E6%9C%AC-2-Ernie-Doc/" />
    <author>
      <name>zizi7</name>
    </author>

  
    
    <category term="LLM" />
    
    <category term="长文本" />
    
  

  <summary>Ernie-Doc【1】是百度2020年提出的面向篇章级长文本建模的预训练-微调框架

Ernie-Doc基于Transformer-XL和XLNet【2】做改进，主要有以下3点创新

  回顾式建模机制（Retrospective Feed Mechanism）
  增强记忆机制（Enhanced Recurrence Mechanism）
  段落重排目标训练（Segment-reordering Objective）


Ernie-Doc在业界首次实现了全篇章无限长文本的双向建模，在阅读理解、信息抽取等13个权威中英文长文本语言理解任务上取得SOTA



图1. 左：WikiText-103数据集（长文本语言建模）；右：IMDB&amp;amp;amp;HYP数据集（长文本分类）



长文本模型技术现状



图2. 长文本技术现状

如图2所示，目前长文本模型可分为3个技术方向

  ...</summary>

  </entry>

  
  <entry>
    <title>【长文本】（1）Transformer-XL</title>
    <link href="http://localhost:4000/posts/%E9%95%BF%E6%96%87%E6%9C%AC-1-Transformer-XL/" rel="alternate" type="text/html" title="【长文本】（1）Transformer-XL" />
    <published>2024-05-22T00:00:00+00:00</published>
  
    <updated>2024-06-15T13:40:01+00:00</updated>
  
    <id>http://localhost:4000/posts/%E9%95%BF%E6%96%87%E6%9C%AC-1-Transformer-XL/</id>
    <content src="http://localhost:4000/posts/%E9%95%BF%E6%96%87%E6%9C%AC-1-Transformer-XL/" />
    <author>
      <name>zizi7</name>
    </author>

  
    
    <category term="LLM" />
    
    <category term="长文本" />
    
  

  <summary>推荐阅读：

  Transformer-XL官方源代码
  《【核心代码解读】Transformer-XL》
  《Transformer-XL解读（论文 + PyTorch源码）》


回顾注意力机制的核心计算公式1，可以看到，虽然理论上transformer结构可以接受无限长度的输入，但由此带来的计算量是无法接受的

[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V \tag{1}]

因此包括bert在内的模型会限定输入长度（512）

Vanilla Transformers【1】将长文本输入截断为多个片段，每个片段分别做训练，推理阶段则采用“滑动窗口”的思路，每次往右移动一个token确保不超过最大长度。但这样会带来几个问题

  上下文长度受限：无法编码长文本
  上下文碎片：片段间信息无法共享，影响模型性能
  推...</summary>

  </entry>

  
  <entry>
    <title>【PLM】（6）ERNIE系列</title>
    <link href="http://localhost:4000/posts/PLM-6-ERNIE%E7%B3%BB%E5%88%97/" rel="alternate" type="text/html" title="【PLM】（6）ERNIE系列" />
    <published>2024-05-15T00:00:00+00:00</published>
  
    <updated>2024-06-15T13:40:01+00:00</updated>
  
    <id>http://localhost:4000/posts/PLM-6-ERNIE%E7%B3%BB%E5%88%97/</id>
    <content src="http://localhost:4000/posts/PLM-6-ERNIE%E7%B3%BB%E5%88%97/" />
    <author>
      <name>zizi7</name>
    </author>

  
    
    <category term="LLM" />
    
    <category term="PLM" />
    
  

  <summary>Erne是百度公开的系列预模型，原本只是在预训练层面对Bert的改进，之后不断发展为一个满足各种需求的预训练模型系列



图0. Ernie系列模型里程碑


  
    
      时间
      模型名称
      特点
    
  
  
    
      2019.3
      ERNIE1.0
      改进预训练方法，超越bert（模型结构与bert一致）
    
    
      2019.7
      ERNIE2.0
      提出多任务持续学习框架，进一步提升模型迁移学习能力（模型结构与bert一致）
    
    
      2020.5
      ERNIE-Gen
      将ERNIE预训练技术扩展至文本生成领域
    
    
      2020.9
      ERNIE-ViL
      面向视觉-语言...</summary>

  </entry>

  
  <entry>
    <title>【RAG】检索优化 - FliCo基于模型蒸馏</title>
    <link href="http://localhost:4000/posts/RAG-%E6%A3%80%E7%B4%A2%E4%BC%98%E5%8C%96-FliCo%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/" rel="alternate" type="text/html" title="【RAG】检索优化 - FliCo基于模型蒸馏" />
    <published>2024-04-25T00:00:00+00:00</published>
  
    <updated>2024-06-15T13:40:01+00:00</updated>
  
    <id>http://localhost:4000/posts/RAG-%E6%A3%80%E7%B4%A2%E4%BC%98%E5%8C%96-FliCo%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/</id>
    <content src="http://localhost:4000/posts/RAG-%E6%A3%80%E7%B4%A2%E4%BC%98%E5%8C%96-FliCo%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E8%92%B8%E9%A6%8F/" />
    <author>
      <name>zizi7</name>
    </author>

  
    
    <category term="LLM" />
    
    <category term="RAG" />
    
  

  <summary>RAG在检索方面的优化一般有以下几种思路

  提高检索效率：比如使用多向量检索器
  提高检索质量：比如对知识库做优化、对检索的内容做进一步优化
  改善用户query：将用户query改写为更适合检索的表达


FliCo【1】通过训练一个模型$M_{ctx}$，对检索得到的上下文做蒸馏，从而提高检索质量



图1. FliCo通过蒸馏上下文改善RAG输出

如图1所示，FliCo认为检索的上下文可能从两个方面误导模型

  检索到的上下文虽然是相关的，但表述过于复杂，容易误导（生成）模型
  检索到的上下文是不相关的，导致（生成）模型产生错误认知


FliCo在github上公开了工程代码



问题描述

给定知识库段落集合$P={p_i},i\in K$，每个$p_i$包含$n_i$个spans（论文中一个span是一个句子），即$p_i=[t_i^1,…,t_i^{n...</summary>

  </entry>

</feed>


